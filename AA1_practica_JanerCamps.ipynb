{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X1a2rkiAgHQ"
   },
   "source": [
    "#Â **AA1 Project** \n",
    "\n",
    "## Modeling delays in the air\n",
    "\n",
    "In order to predict whether a flight is likely to be delayed or not, create a ML model that will make predictions with that aim. \n",
    "\n",
    "First of all. Let's observe our data set and some first insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9WFwwT0G--Vb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.ticker import ScalarFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4naEd9v_gWI",
    "outputId": "f9312b58-6257-40ee-aac4-dd514d576d82"
   },
   "outputs": [],
   "source": [
    "airports = pd.read_csv('airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6p/292cfjv12xd9n9pzpglsj3w00000gn/T/ipykernel_1437/2102207835.py:1: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  flights = pd.read_csv('flights.csv')\n"
     ]
    }
   ],
   "source": [
    "flights = pd.read_csv('flights.csv')\n",
    "flights = flights.sample(frac=0.0025, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsI8BfIVAdal"
   },
   "source": [
    "As we can se with the `describe` method, we can observe the range where each feature takes values at more or less. It is also interesting to observe the histograms of the different columns to see how each feature is distributed more or less, to get an intuition of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "WMan5A-f_l5x",
    "outputId": "5e0b5489-d212-4929-cf12-18f77639fec4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.981244</td>\n",
       "      <td>-98.378964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.616736</td>\n",
       "      <td>21.523492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.483450</td>\n",
       "      <td>-176.646030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.652040</td>\n",
       "      <td>-110.839385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.297610</td>\n",
       "      <td>-93.403070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.154675</td>\n",
       "      <td>-82.722995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.285450</td>\n",
       "      <td>-64.798560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LATITUDE   LONGITUDE\n",
       "count  319.000000  319.000000\n",
       "mean    38.981244  -98.378964\n",
       "std      8.616736   21.523492\n",
       "min     13.483450 -176.646030\n",
       "25%     33.652040 -110.839385\n",
       "50%     39.297610  -93.403070\n",
       "75%     43.154675  -82.722995\n",
       "max     71.285450  -64.798560"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()\n",
    "airports.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'YEAR'}>,\n",
       "        <AxesSubplot: title={'center': 'MONTH'}>,\n",
       "        <AxesSubplot: title={'center': 'DAY'}>,\n",
       "        <AxesSubplot: title={'center': 'DAY_OF_WEEK'}>,\n",
       "        <AxesSubplot: title={'center': 'FLIGHT_NUMBER'}>],\n",
       "       [<AxesSubplot: title={'center': 'SCHEDULED_DEPARTURE'}>,\n",
       "        <AxesSubplot: title={'center': 'DEPARTURE_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>,\n",
       "        <AxesSubplot: title={'center': 'WHEELS_OFF'}>],\n",
       "       [<AxesSubplot: title={'center': 'SCHEDULED_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'ELAPSED_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'AIR_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'DISTANCE'}>,\n",
       "        <AxesSubplot: title={'center': 'WHEELS_ON'}>],\n",
       "       [<AxesSubplot: title={'center': 'TAXI_IN'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_ARRIVAL'}>,\n",
       "        <AxesSubplot: title={'center': 'ARRIVAL_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'ARRIVAL_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'DIVERTED'}>],\n",
       "       [<AxesSubplot: title={'center': 'CANCELLED'}>,\n",
       "        <AxesSubplot: title={'center': 'AIR_SYSTEM_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'SECURITY_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'AIRLINE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'LATE_AIRCRAFT_DELAY'}>],\n",
       "       [<AxesSubplot: title={'center': 'WEATHER_DELAY'}>,\n",
       "        <AxesSubplot: >, <AxesSubplot: >, <AxesSubplot: >,\n",
       "        <AxesSubplot: >]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flights.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the histograms, we can appreciate that some of the variables in our dataset present variables where the data appears to be quite concentrated and this could be explained by the presence of large outliers. Some other variables seem to be more uniformally distributed. However, the nature of the distribution that describes the data in our variables shouldn't worry ourselve to much right now as many data processment will be done (including gaussianization of the continuos variables and hot encoding of the categorical ones) before trainging and testing a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owSaZFCZ3-V2"
   },
   "source": [
    "## 1. Data cleaning\n",
    "### Nextly we will portray the schedule that we are going to follow. \n",
    "1. Check for duplicates: Check for and remove any duplicate rows in our dataset.\n",
    "\n",
    "2. Handle missing data: Identify any missing data and decide how to handle it. We will either remove the rows or fill in the missing data.\n",
    "\n",
    "3. Check for inconsistent data: Check for any inconsistent or erroneous data, such as values that are out of range or inconsistent with other data in the same row.\n",
    "\n",
    "4. Handle categorical data: If you have categorical data, decide how to handle it. One common approach is to use one-hot encoding.\n",
    "\n",
    "5. Normalize data: Normalize the data so that the features have similar ranges. This will prevent features with large ranges from dominating the model.\n",
    "\n",
    "6. Feature selection: Select the most relevant features for your model. You can use various techniques such as correlation analysis or principal component analysis (PCA).\n",
    "\n",
    "7. Train-test split: Finally, split your data into training and testing sets to evaluate the performance of your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9qgdnb740x-"
   },
   "source": [
    "### 1.1 Check for duplicates\n",
    "We firstly see that there are no duplicate samples with the following piece of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSrzDv455MWf",
    "outputId": "ffa33860-4374-4db7-ea5a-98cca5564064"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates = flights.duplicated()\n",
    "\n",
    "# Print the duplicate rows\n",
    "len(flights[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeODOfOB5pNx"
   },
   "source": [
    "### 1.2 Handle missing data\n",
    "In this section we have a bunch of different possibilities in order to approach the problem of missing data. \n",
    "1. Remove missing data: if the quantity of missing data is not that significative, maybe it is a good option to consider removing all those samples that contain `NaN` values since the subset of samples that is going to be deleted may not be significant while training the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W0tqcTO6hYt",
    "outputId": "9afbcca0-8c45-4053-d09b-209047d794f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR                       0\n",
      "MONTH                      0\n",
      "DAY                        0\n",
      "DAY_OF_WEEK                0\n",
      "AIRLINE                    0\n",
      "FLIGHT_NUMBER              0\n",
      "TAIL_NUMBER               34\n",
      "ORIGIN_AIRPORT             0\n",
      "DESTINATION_AIRPORT        0\n",
      "SCHEDULED_DEPARTURE        0\n",
      "DEPARTURE_TIME           204\n",
      "DEPARTURE_DELAY          204\n",
      "TAXI_OUT                 217\n",
      "WHEELS_OFF               217\n",
      "SCHEDULED_TIME             0\n",
      "ELAPSED_TIME             253\n",
      "AIR_TIME                 253\n",
      "DISTANCE                   0\n",
      "WHEELS_ON                226\n",
      "TAXI_IN                  226\n",
      "SCHEDULED_ARRIVAL          0\n",
      "ARRIVAL_TIME             226\n",
      "ARRIVAL_DELAY            253\n",
      "DIVERTED                   0\n",
      "CANCELLED                  0\n",
      "CANCELLATION_REASON    14330\n",
      "AIR_SYSTEM_DELAY       11803\n",
      "SECURITY_DELAY         11803\n",
      "AIRLINE_DELAY          11803\n",
      "LATE_AIRCRAFT_DELAY    11803\n",
      "WEATHER_DELAY          11803\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in each column\n",
    "nan_counts = flights.isna().sum()\n",
    "# Print the results\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwRj6Uih68dv"
   },
   "source": [
    "From the previous output we see that the last 6 features are almost useless since the majority of the samples do not have any information of those, therefore we are going to `drop`. A part from that, we also need to delete columns that regard information that is not going to be available (a posteriori information from the flight) such as information of the time elapsed during time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g28GIbss7vHi"
   },
   "outputs": [],
   "source": [
    "cols_of_interest = ['ARRIVAL_DELAY','MONTH', 'DAY' ,'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_ARRIVAL', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME', 'DISTANCE']\n",
    "flights = flights[cols_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "jczIDNTZOJpP",
    "outputId": "2b732e2a-8915-470d-c24e-922daa387018"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14344.000000</td>\n",
       "      <td>14344.000000</td>\n",
       "      <td>14331.000000</td>\n",
       "      <td>14331.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.397761</td>\n",
       "      <td>6.505774</td>\n",
       "      <td>15.762373</td>\n",
       "      <td>3.890019</td>\n",
       "      <td>1495.689098</td>\n",
       "      <td>1336.571249</td>\n",
       "      <td>9.245259</td>\n",
       "      <td>16.187217</td>\n",
       "      <td>1360.064057</td>\n",
       "      <td>142.108675</td>\n",
       "      <td>823.882596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37.434848</td>\n",
       "      <td>3.415867</td>\n",
       "      <td>8.794217</td>\n",
       "      <td>1.977637</td>\n",
       "      <td>507.325283</td>\n",
       "      <td>497.630899</td>\n",
       "      <td>35.066599</td>\n",
       "      <td>8.955900</td>\n",
       "      <td>498.626400</td>\n",
       "      <td>75.483655</td>\n",
       "      <td>609.618878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>922.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1522.000000</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>647.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1919.000000</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1758.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>947.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>965.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>4983.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARRIVAL_DELAY         MONTH           DAY   DAY_OF_WEEK  \\\n",
       "count   14295.000000  14548.000000  14548.000000  14548.000000   \n",
       "mean        4.397761      6.505774     15.762373      3.890019   \n",
       "std        37.434848      3.415867      8.794217      1.977637   \n",
       "min       -62.000000      1.000000      1.000000      1.000000   \n",
       "25%       -13.000000      4.000000      8.000000      2.000000   \n",
       "50%        -5.000000      7.000000     16.000000      4.000000   \n",
       "75%         8.000000      9.000000     23.000000      6.000000   \n",
       "max       947.000000     12.000000     31.000000      7.000000   \n",
       "\n",
       "       SCHEDULED_ARRIVAL  DEPARTURE_TIME  DEPARTURE_DELAY      TAXI_OUT  \\\n",
       "count       14548.000000    14344.000000     14344.000000  14331.000000   \n",
       "mean         1495.689098     1336.571249         9.245259     16.187217   \n",
       "std           507.325283      497.630899        35.066599      8.955900   \n",
       "min             1.000000        1.000000       -36.000000      3.000000   \n",
       "25%          1110.000000      922.000000        -5.000000     11.000000   \n",
       "50%          1522.000000     1331.000000        -2.000000     14.000000   \n",
       "75%          1919.000000     1743.000000         8.000000     19.000000   \n",
       "max          2359.000000     2400.000000       965.000000    145.000000   \n",
       "\n",
       "         WHEELS_OFF  SCHEDULED_TIME      DISTANCE  \n",
       "count  14331.000000    14548.000000  14548.000000  \n",
       "mean    1360.064057      142.108675    823.882596  \n",
       "std      498.626400       75.483655    609.618878  \n",
       "min        1.000000       20.000000     31.000000  \n",
       "25%      936.000000       86.000000    373.000000  \n",
       "50%     1345.000000      123.000000    647.000000  \n",
       "75%     1758.000000      174.000000   1066.000000  \n",
       "max     2359.000000      680.000000   4983.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV9TeZHT83xS"
   },
   "source": [
    "Once we've deleted those columns we can say that **maybe** and only **maybe** taking into account that we have more than 5M samples, deleting the other samples that contain at least one `NaN` value, may be reasonable. \n",
    "\n",
    "**CHECK WHETHER THIS IS REASONABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e82megy49YCh",
    "outputId": "c945bf9d-9c1f-4cd4-ad6b-2a5638b92934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14548 14295\n"
     ]
    }
   ],
   "source": [
    "l_bef = len(flights)\n",
    "flights = flights.dropna(how='any')\n",
    "l_aft = len(flights)\n",
    "print(l_bef, l_aft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ0KSQzZ-D3g"
   },
   "source": [
    "We pass from $5819079$ to $5714008$ samples. In other words, we keep the $98.2\\%$ of the samples, so it may be a good option to work with these new subset of samples that still contain a vast quantity of information.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LO35Qxc-pB3"
   },
   "source": [
    "### 1.3 Check for inconsistent data\n",
    "In order to do so, we firstly observe an overview of our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "c3TuA6Cx_x5G",
    "outputId": "8b9139e8-da40-4805-efdf-01dec510440f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.397761</td>\n",
       "      <td>6.521791</td>\n",
       "      <td>15.751522</td>\n",
       "      <td>3.896747</td>\n",
       "      <td>1494.538370</td>\n",
       "      <td>1336.156208</td>\n",
       "      <td>9.159706</td>\n",
       "      <td>16.186079</td>\n",
       "      <td>1359.684715</td>\n",
       "      <td>142.320462</td>\n",
       "      <td>826.171319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37.434848</td>\n",
       "      <td>3.404917</td>\n",
       "      <td>8.782405</td>\n",
       "      <td>1.975532</td>\n",
       "      <td>507.725006</td>\n",
       "      <td>497.734898</td>\n",
       "      <td>34.840084</td>\n",
       "      <td>8.961210</td>\n",
       "      <td>498.663054</td>\n",
       "      <td>75.566948</td>\n",
       "      <td>610.236930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>922.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1917.000000</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1758.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>947.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>965.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>4983.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARRIVAL_DELAY         MONTH           DAY   DAY_OF_WEEK  \\\n",
       "count   14295.000000  14295.000000  14295.000000  14295.000000   \n",
       "mean        4.397761      6.521791     15.751522      3.896747   \n",
       "std        37.434848      3.404917      8.782405      1.975532   \n",
       "min       -62.000000      1.000000      1.000000      1.000000   \n",
       "25%       -13.000000      4.000000      8.000000      2.000000   \n",
       "50%        -5.000000      7.000000     16.000000      4.000000   \n",
       "75%         8.000000      9.000000     23.000000      6.000000   \n",
       "max       947.000000     12.000000     31.000000      7.000000   \n",
       "\n",
       "       SCHEDULED_ARRIVAL  DEPARTURE_TIME  DEPARTURE_DELAY      TAXI_OUT  \\\n",
       "count       14295.000000    14295.000000     14295.000000  14295.000000   \n",
       "mean         1494.538370     1336.156208         9.159706     16.186079   \n",
       "std           507.725006      497.734898        34.840084      8.961210   \n",
       "min             1.000000        1.000000       -36.000000      3.000000   \n",
       "25%          1110.000000      922.000000        -5.000000     11.000000   \n",
       "50%          1521.000000     1331.000000        -2.000000     14.000000   \n",
       "75%          1917.000000     1743.000000         8.000000     19.000000   \n",
       "max          2359.000000     2400.000000       965.000000    145.000000   \n",
       "\n",
       "         WHEELS_OFF  SCHEDULED_TIME      DISTANCE  \n",
       "count  14295.000000    14295.000000  14295.000000  \n",
       "mean    1359.684715      142.320462    826.171319  \n",
       "std      498.663054       75.566948    610.236930  \n",
       "min        1.000000       20.000000     31.000000  \n",
       "25%      936.000000       86.000000    373.000000  \n",
       "50%     1345.000000      123.000000    650.000000  \n",
       "75%     1758.000000      174.000000   1067.000000  \n",
       "max     2359.000000      680.000000   4983.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will plot a few histograms of the variables in our dataset to get a deeper understanding of their nature. First of all we are going to implement a function that is going to allow us to plot the histogram and personalize a few parameters such as the title, labels for the axis and the options to apply logarithms to any of the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_of_column(data, title, x_title, y_title, x_log, y_log):\n",
    "    # Assign default title if none is provided\n",
    "    if not title:\n",
    "        title = \"Histogram\"\n",
    "\n",
    "    # Assign default title to x axis if none is provided\n",
    "    if not x_title:\n",
    "        x_title = \"Value\"\n",
    "    \n",
    "    # Assing default title to y axis if none is provided\n",
    "    if not y_title:\n",
    "        y_title = \"Count\"\n",
    "    \n",
    "    # Apply logarithm to x axis if required\n",
    "    if x_log:\n",
    "        # Set up histogram with logarithmic x-axis\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data, bins=10**np.linspace(np.log10(0.1), np.log10(data.max()), 50))\n",
    "\n",
    "        # Set x-axis to logarithmic scale\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "        # Add \"log\" to x axis title\n",
    "        x_title = \"Log \" + x_title\n",
    "    else:\n",
    "        # Create plot with histogram of DAY column\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data)\n",
    "\n",
    "\n",
    "    # Apply logarithm to y-axis if required\n",
    "    if y_log:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        # Add \"log\" to y axis title\n",
    "        y_title = \"Log \" + y_title\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a function that allows us to easily plot any column from the dataset that we are interested in, we will start by studying the nature of the columns that refer to delays on the flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of the arrival_delay column\n",
    "plot_histogram_of_column(flights[\"ARRIVAL_DELAY\"], \"Distribution of arrival delay\", None, None, True, False)\n",
    "\n",
    "# Histogram of DEPARTURE_DELAY column\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_DELAY\"], \"Distribution of departure delay\", None, None, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that both histograms share a similar shape, which means that these two variables are strongly correlated. However, we expected to get this result since those flights that suffer departure with any form of delay are extremely likely to also arrive with a similar delay.\n",
    "\n",
    "Another interesting result extracted from the data is that there are quite a lot of flights that depart before their scheduled departure time. Moreover, if we compare closely both histograms, we will notice that the amount of flights that arrive before their arrival time is smaller than the number of flights that departure ahead of their departure time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to analyse how the variables containing data related with the date of the flight are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"MONTH\"], \"Distribution of month\", None, None, False, False)\n",
    "\n",
    "# Create plot with histogram of DAY column\n",
    "plot_histogram_of_column(flights[\"DAY\"], \"Distribution of day\", None, None, False, True)\n",
    "\n",
    "# Histogram of DAY_OF_WEEK colum\n",
    "plot_histogram_of_column(flights[\"DAY_OF_WEEK\"], \"Distribution of day of week\", None, None, False, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we'll plot some other columns that could be interesting to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA10AAAIhCAYAAAC17NBRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1JElEQVR4nO3de5xVdb34//cGZoDhMjIgN1Eky8sIDIWTipmayhERUfKRl1JMOUVnrNA8ZvlNsJvayUvpYGneOqJSHbFOmB1MBE2yQfCSYx5LCbxwEeIiKASzfn/0Y47jzOAMzofNzDyfj8c8Hu2116z93jOf9oOXe+01uSzLsgAAACCJDvkeAAAAoC0TXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAI244447IpfL1X516dIl+vfvH8ccc0xceeWVsXLlynrfM23atMjlcs16nE2bNsW0adPikUceadb3NfRY++67b5x00knNOs57ufvuu+P6669v8L5cLhfTpk1r0cdrab/73e/ikEMOiW7dukUul4v777+/Wd//yCOPRC6Xa/bvJx929LvKtwceeKDRtbLvvvvGueeeu0vnAdiVRBfAe7j99ttjwYIFMWfOnKisrIwRI0bE1VdfHQcddFA89NBDdfadNGlSLFiwoFnH37RpU1xxxRXN/kf9zjzWztjRP+QXLFgQkyZNSj7DzsqyLD71qU9FQUFB/OpXv4oFCxbEUUcdle+xktndo+uKK65o8L5Zs2bFN77xjV08EcCu0ynfAwDs7oYOHRqHHHJI7e1PfvKTceGFF8bHPvaxmDBhQrz44ovRr1+/iIgYNGhQDBo0KOk8mzZtiqKiol3yWO/lsMMOy+vjv5fXXnst1qxZE6eeemoce+yx+R4nme1rorUe/8Mf/nCyYwPsDrzTBbAT9tlnn7jmmmtiw4YN8eMf/7h2e0On/D388MNx9NFHR+/evaNr166xzz77xCc/+cnYtGlTLFmyJPbcc8+IiLjiiitqT2XcfqrV9uMtWrQoTjvttOjVq1fst99+jT7WdrNmzYrhw4dHly5d4gMf+ED88Ic/rHP/9lMnlyxZUmf7u0+lO/roo2P27Nnxt7/9rc6plts1dHrhn/70pxg/fnz06tUrunTpEiNGjIg777yzwce555574rLLLouBAwdGz54947jjjosXXnih8R/8Ozz22GNx7LHHRo8ePaKoqChGjRoVs2fPrr1/2rRptVH61a9+NXK5XOy77747POaf//znOOGEE6KoqCj69OkTkydPjg0bNjS470MPPRTHHnts9OzZM4qKiuKII46I3/3ud3X22f47Wrx4cUyYMCF69uwZxcXF8ZnPfCZWrVpVZ9+ZM2fG6NGjY8CAAdG1a9c46KCD4tJLL42NGzfW2e/cc8+N7t27x7PPPhujR4+OHj16xLHHHrvD31Vjp0guWbIkcrlc3HHHHe95/IiILVu2xLe//e048MADo3PnzrHnnnvGZz/72XrP5d3OPffcqKysjIioM9v29ffu0wu3z3v33XfHV7/61RgwYEB07949xo0bFytWrIgNGzbE5z73uejTp0/06dMnPvvZz8abb75Z5zGzLIvp06fHiBEjomvXrtGrV6847bTT4qWXXtrhrAApeKcLYCedeOKJ0bFjx5g/f36j+yxZsiTGjh0bRx55ZNx2222xxx57xKuvvhoPPvhgbNmyJQYMGBAPPvhgnHDCCXH++efXnqq3PcS2mzBhQpxxxhkxefLkev8If7ennnoqpkyZEtOmTYv+/fvHjBkz4stf/nJs2bIlLr744mY9x+nTp8fnPve5+Otf/xqzZs16z/1feOGFGDVqVPTt2zd++MMfRu/eveOuu+6Kc889N1asWBGXXHJJnf2//vWvxxFHHBE/+clPYv369fHVr341xo0bF88//3x07Nix0ceZN29eHH/88TF8+PC49dZbo3PnzjF9+vQYN25c3HPPPXH66afHpEmToqysLCZMmBBf/OIX46yzzorOnTs3eswVK1bEUUcdFQUFBTF9+vTo169fzJgxIy644IJ6+951111xzjnnxPjx4+POO++MgoKC+PGPfxz/8i//Er/97W/rvat26qmnxqc+9amYPHlyPPfcc/GNb3wjqqur44knnoiCgoKIiHjxxRfjxBNPjClTpkS3bt3iz3/+c1x99dXxxz/+MR5++OE6x9uyZUucfPLJ8fnPfz4uvfTS2Lp1awwaNKhZv6sdaej4NTU1MX78+Hj00UfjkksuiVGjRsXf/va3mDp1ahx99NGxcOHC6Nq1a4PH+8Y3vhEbN26MX/ziF3VOiR0wYMAO5/j6178exxxzTNxxxx2xZMmSuPjii+PMM8+MTp06RVlZWdxzzz2xePHi+PrXvx49evSo8x8XPv/5z8cdd9wRX/rSl+Lqq6+ONWvWxDe/+c0YNWpUPP3007XvTgPsEhkADbr99tuziMiqqqoa3adfv37ZQQcdVHt76tSp2TtfWn/xi19kEZE99dRTjR5j1apVWURkU6dOrXff9uNdfvnljd73ToMHD85yuVy9xzv++OOznj17Zhs3bqzz3F5++eU6+82dOzeLiGzu3Lm128aOHZsNHjy4wdnfPfcZZ5yRde7cOVu6dGmd/caMGZMVFRVla9eurfM4J554Yp39fvazn2URkS1YsKDBx9vusMMOy/r27Ztt2LChdtvWrVuzoUOHZoMGDcpqamqyLMuyl19+OYuI7D/+4z92eLwsy7KvfvWrjf7s3vkz2bhxY1ZSUpKNGzeuzn7btm3LysrKso9+9KO127b/ji688MI6+86YMSOLiOyuu+5qcJaamprsH//4RzZv3rwsIrKnn3669r6JEydmEZHddttt9b6vsd9VQ7/XLPu/n8/tt9/+nse/5557sojI/uu//qvO9qqqqiwisunTpzf4XLarqKiot163Gzx4cDZx4sR68777ZzxlypQsIrIvfelLdbafcsopWUlJSe3tBQsWZBGRXXPNNXX2W7ZsWda1a9fskksu2eGsAC3N6YUA70OWZTu8f8SIEVFYWBif+9zn4s4779zpU5s++clPNnnfgw8+OMrKyupsO+uss2L9+vWxaNGinXr8pnr44Yfj2GOPjb333rvO9nPPPTc2bdpU78IfJ598cp3bw4cPj4iIv/3tb40+xsaNG+OJJ56I0047Lbp37167vWPHjnH22WfHK6+80uRTFN9p7ty5jf7s3unxxx+PNWvWxMSJE2Pr1q21XzU1NXHCCSdEVVVVvXcjP/3pT9e5/alPfSo6deoUc+fOrd320ksvxVlnnRX9+/ePjh07RkFBQe1FP55//vl68zZnTeyMdx//17/+deyxxx4xbty4Os97xIgR0b9//yRXd3z3lTgPOuigiIgYO3Zsve1r1qypPcXw17/+deRyufjMZz5TZ9b+/ftHWVlZq7gSJdC2OL0QYCdt3LgxVq9eHcOGDWt0n/322y8eeuih+N73vhcVFRWxcePG+MAHPhBf+tKX4stf/nKTH+u9TsN6p/79+ze6bfXq1U0+zs5YvXp1g7MOHDiwwcfv3bt3ndvbT/976623Gn2Mv//975FlWbMepylWr14dQ4YMqbf93T/PFStWRETEaaed1uix1qxZE926dWv0GJ06dYrevXvXzvnmm2/GkUceGV26dIlvf/vbsf/++0dRUVEsW7YsJkyYUO/nUVRUFD179mzeE2yGho6/YsWKWLt2bRQWFjb4PW+88UaLz1FSUlLn9vbHbmz722+/Hd27d48VK1ZElmWNnkL4gQ98oMVnBdgR0QWwk2bPnh3btm2Lo48+eof7HXnkkXHkkUfGtm3bYuHChXHDDTfElClTol+/fnHGGWc06bGa87e/li9f3ui27ZHTpUuXiIjYvHlznf3e7z+ce/fuHa+//nq97a+99lpERPTp0+d9HT8iolevXtGhQ4cWf5zevXvv8Ge33fZj33DDDY1evfHd/9hfvnx57LXXXrW3t27dGqtXr679fTz88MPx2muvxSOPPFLnkvZr165t8PjN/Vtwzf19N3T8Pn36RO/evePBBx9s8Ht69OjRrJlS6tOnT+RyuXj00Ucb/Bzfjj7bB5CC0wsBdsLSpUvj4osvjuLi4vj85z/fpO/p2LFjHHroobVXcdt+ql9T3t1pjueeey6efvrpOtvuvvvu6NGjR3zkIx+JiKi9it8zzzxTZ79f/epX9Y7XuXPnJs927LHH1gbEO/30pz+NoqKiFrnEfLdu3eLQQw+N++67r85cNTU1cdddd8WgQYNi//33b/ZxjznmmEZ/du90xBFHxB577BHV1dVxyCGHNPj17neDZsyYUef2z372s9i6dWttsG+PnHfHwDuvjNkUjf2umvP7bsxJJ50Uq1evjm3btjX4nA844ID3nC2i5db5e82aZVm8+uqrDc66o3enAVLwThfAe/jTn/5U+5mQlStXxqOPPhq33357dOzYMWbNmlXvSoPv9KMf/SgefvjhGDt2bOyzzz7x9ttvx2233RYREccdd1xE/PMdgsGDB8cvf/nLOPbYY6OkpCT69Onznpc3b8zAgQPj5JNPjmnTpsWAAQPirrvuijlz5sTVV19d+7eWysvL44ADDoiLL744tm7dGr169YpZs2bFY489Vu94w4YNi/vuuy9uuummGDlyZHTo0KHO3y17p6lTp8avf/3rOOaYY+Lyyy+PkpKSmDFjRsyePTu+973vRXFx8U49p3e78sor4/jjj49jjjkmLr744igsLIzp06fHn/70p7jnnnua/U5QRMSUKVPitttui7Fjx8a3v/3t2qsX/vnPf66zX/fu3eOGG26IiRMnxpo1a+K0006Lvn37xqpVq+Lpp5+OVatWxU033VTne+67777o1KlTHH/88bVXLywrK4tPfepTERExatSo6NWrV0yePDmmTp0aBQUFMWPGjHoB+F4a+131798/jjvuuLjyyiujV69eMXjw4Pjd734X9913X5OPfcYZZ8SMGTPixBNPjC9/+cvx0Y9+NAoKCuKVV16JuXPnxvjx4+PUU0/d4WwREVdffXWMGTMmOnbsGMOHD2/0dMX344gjjojPfe5z8dnPfjYWLlwYH//4x6Nbt27x+uuvx2OPPRbDhg2LL3zhCy3+uACNyu91PAB2X9uv8Lf9q7CwMOvbt2921FFHZd/97nezlStX1vued19RcMGCBdmpp56aDR48OOvcuXPWu3fv7Kijjsp+9atf1fm+hx56KPvwhz+cde7cOYuI2iu5bT/eqlWr3vOxsuyfV4EbO3Zs9otf/CI7+OCDs8LCwmzffffNrr322nrf/7//+7/Z6NGjs549e2Z77rln9sUvfjGbPXt2vavcrVmzJjvttNOyPfbYI8vlcnUeMxq46uKzzz6bjRs3LisuLs4KCwuzsrKyOlfHy7L/uzrdz3/+8zrbG7qaXmMeffTR7BOf+ETWrVu3rGvXrtlhhx2W/fd//3eDx2vK1QuzLMuqq6uz448/PuvSpUtWUlKSnX/++dkvf/nLBq/8N2/evGzs2LFZSUlJVlBQkO21117Z2LFj6zyn7b+jJ598Mhs3blzWvXv3rEePHtmZZ56ZrVixos7xHn/88ezwww/PioqKsj333DObNGlStmjRogavLtitW7cG59/R7+r111/PTjvttKykpCQrLi7OPvOZz2QLFy5s1vH/8Y9/ZN///vezsrKyrEuXLln37t2zAw88MPv85z+fvfjiizv82W7evDmbNGlStueee9bOtv3qmY1dvfDd66OxK4o29v+T2267LTv00ENr18h+++2XnXPOOdnChQt3OCtAS8tl2XtcegsA2CnTpk2LK664IlatWtUin2cDoHXymS4AAICERBcAAEBCTi8EAABIyDtdAAAACYkuAACAhEQXAABAQu36jyPX1NTEa6+9Fj169NipP6QJAAC0DVmWxYYNG2LgwIHRoUPLvjfVrqPrtddei7333jvfYwAAALuJZcuWxaBBg1r0mO06unr06BER//zB9uzZM8/TAAAA+bJ+/frYe++9axuhJbXr6Np+SmHPnj1FFwAAkORjRy6kAQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEioU74HAEhh30tn53uE3daSq8bme4TdkjXTMOsF4P3zThcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQELtMroqKyujtLQ0ysvL8z0KAADQxnXK9wD5UFFRERUVFbF+/fooLi7O9zgAQCu076Wz8z3CbmnJVWPzPQLsdtpldAEATSMsAN6/dnl6IQAAwK4iugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASMgfR4ZWzh8uBQDYvYkugHZGqAPAriW6AABoMf7DTuOWXDU23yOQJz7TBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJBQq4+uDRs2RHl5eYwYMSKGDRsWt9xyS75HAgAAqNUp3wO8X0VFRTFv3rwoKiqKTZs2xdChQ2PChAnRu3fvfI8GAADQ+t/p6tixYxQVFUVExNtvvx3btm2LLMvyPBUAAMA/5T265s+fH+PGjYuBAwdGLpeL+++/v94+06dPjyFDhkSXLl1i5MiR8eijj9a5f+3atVFWVhaDBg2KSy65JPr06bOLpgcAANixvEfXxo0bo6ysLG688cYG7585c2ZMmTIlLrvssli8eHEceeSRMWbMmFi6dGntPnvssUc8/fTT8fLLL8fdd98dK1as2FXjAwAA7FDeo2vMmDHx7W9/OyZMmNDg/ddee22cf/75MWnSpDjooIPi+uuvj7333jtuuummevv269cvhg8fHvPnz2/wWJs3b47169fX+QIAAEgp79G1I1u2bIknn3wyRo8eXWf76NGj4/HHH4+IiBUrVtTG0/r162P+/PlxwAEHNHi8K6+8MoqLi2u/9t5777RPAAAAaPd26+h64403Ytu2bdGvX7862/v16xfLly+PiIhXXnklPv7xj0dZWVl87GMfiwsuuCCGDx/e4PG+9rWvxbp162q/li1blvw5AAAA7VuruGR8LperczvLstptI0eOjKeeeqpJx+ncuXN07ty5pccDAABo1G79TlefPn2iY8eOte9qbbdy5cp6734BAADsjnbr6CosLIyRI0fGnDlz6myfM2dOjBo1Kk9TAQAANF3eTy9888034y9/+Uvt7ZdffjmeeuqpKCkpiX322ScuuuiiOPvss+OQQw6Jww8/PG6++eZYunRpTJ48OY9TAwAANE3eo2vhwoVxzDHH1N6+6KKLIiJi4sSJcccdd8Tpp58eq1evjm9+85vx+uuvx9ChQ+OBBx6IwYMH52tkAACAJstlWZble4h8Wb9+fRQXF8e6deuiZ8+e+R4Hdsq+l87O9wgAQBMsuWpsvkdgB1K2wW79mS4AAIDWrl1GV2VlZZSWlkZ5eXm+RwEAANq4dhldFRUVUV1dHVVVVfkeBQAAaOPaZXQBAADsKqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICE2mV0VVZWRmlpaZSXl+d7FAAAoI1rl9FVUVER1dXVUVVVle9RAACANq5dRhcAAMCuIroAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABJql9FVWVkZpaWlUV5enu9RAACANq5dRldFRUVUV1dHVVVVvkcBAADauHYZXQAAALuK6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJNQuo6uysjJKS0ujvLw836MAAABtXLuMroqKiqiuro6qqqp8jwIAALRx7TK6AAAAdhXRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJtcvoqqysjNLS0igvL8/3KAAAQBvXLqOroqIiqquro6qqKt+jAAAAbVy7jC4AAIBdRXQBAAAkJLoAAAAS6pTvAQAAoD3Y99LZ+R5ht7TkqrH5HiE573QBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQELtMroqKyujtLQ0ysvL8z0KAADQxrXL6KqoqIjq6uqoqqrK9ygAAEAb1y6jCwAAYFcRXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEioXUZXZWVllJaWRnl5eb5HAQAA2rhmR9fSpUsjy7J627Msi6VLl7bIUKlVVFREdXV1VFVV5XsUAACgjWt2dA0ZMiRWrVpVb/uaNWtiyJAhLTIUAABAW9Hs6MqyLHK5XL3tb775ZnTp0qVFhgIAAGgrOjV1x4suuigiInK5XHzjG9+IoqKi2vu2bdsWTzzxRIwYMaLFBwQAAGjNmhxdixcvjoh/vtP17LPPRmFhYe19hYWFUVZWFhdffHHLTwgAANCKNTm65s6dGxERn/3sZ+MHP/hB9OzZM9lQAAAAbUWTo2u722+/PcUcAAAAbVKzo2vjxo1x1VVXxe9+97tYuXJl1NTU1Ln/pZdearHhAAAAWrtmR9ekSZNi3rx5cfbZZ8eAAQMavJIhAAAA/9Ts6PrNb34Ts2fPjiOOOCLFPAAAAG1Ks/9OV69evaKkpCTFLAAAAG1Os6PrW9/6Vlx++eWxadOmFPMAAAC0Kc0+vfCaa66Jv/71r9GvX7/Yd999o6CgoM79ixYtarHhAAAAWrtmR9cpp5ySYAwAAIC2qdnRNXXq1BRzAAAAtEnN/kwXAAAATdfsd7o6dOiww7/NtW3btvc1EAAAQFvS7OiaNWtWndv/+Mc/YvHixXHnnXfGFVdc0WKDAQAAtAXNjq7x48fX23baaafFwQcfHDNnzozzzz+/RQYDAABoC1rsM12HHnpoPPTQQy11OAAAgDahRaLrrbfeihtuuCEGDRrUEocDAABoM5p9emGvXr3qXEgjy7LYsGFDFBUVxV133dWiwwEAALR2zY6u66+/vs7tDh06xJ577hmHHnpo9OrVq6XmAgAAaBOaHV0TJ05MMQcAAECb1OzoiohYu3Zt3HrrrfH8889HLpeL0tLSOO+886K4uLil5wMAAGjVmn0hjYULF8Z+++0X1113XaxZsybeeOONuPbaa2O//faLRYsWpZgRAACg1Wr2O10XXnhhnHzyyXHLLbdEp07//PatW7fGpEmTYsqUKTF//vwWHxIAAKC1anZ0LVy4sE5wRUR06tQpLrnkkjjkkENadDgAAIDWrtmnF/bs2TOWLl1ab/uyZcuiR48eLTIUAABAW9Hs6Dr99NPj/PPPj5kzZ8ayZcvilVdeiXvvvTcmTZoUZ555ZooZAQAAWq1mn174/e9/P3K5XJxzzjmxdevWiIgoKCiIL3zhC3HVVVe1+IAAAACtWbOjq7CwMH7wgx/ElVdeGX/9618jy7L44Ac/GEVFRSnmAwAAaNWafHrhtm3b4plnnom33norIiKKiopi2LBhMXz48MjlcvHMM89ETU1NskEBAABaoyZH13/+53/GeeedF4WFhfXuKywsjPPOOy/uvvvuFh0OAACgtWtydN16661x8cUXR8eOHevd17Fjx7jkkkvi5ptvbtHhAAAAWrsmR9cLL7wQhx12WKP3l5eXx/PPP98iQ6VWWVkZpaWlUV5enu9RAACANq7J0bVx48ZYv359o/dv2LAhNm3a1CJDpVZRURHV1dVRVVWV71EAAIA2rsnR9aEPfSgef/zxRu9/7LHH4kMf+lCLDAUAANBWNDm6zjrrrPh//+//xTPPPFPvvqeffjouv/zyOOuss1p0OAAAgNauyX+n68ILL4zf/OY3MXLkyDjuuOPiwAMPjFwuF88//3w89NBDccQRR8SFF16YclYAAIBWp8nRVVBQEP/zP/8T1113Xdx9990xf/78yLIs9t9///jOd74TU6ZMiYKCgpSzAgAAtDpNjq6If4bXJZdcEpdcckmqeQAAANqUJn+mCwAAgOYTXQAAAAmJLgAAgIREFwAAQEKiCwAAIKFmXb0wIuKiiy5qcHsul4suXbrEBz/4wRg/fnyUlJS87+EAAABau2ZH1+LFi2PRokWxbdu2OOCAAyLLsnjxxRejY8eOceCBB8b06dPjK1/5Sjz22GNRWlqaYmYAAIBWo9mnF44fPz6OO+64eO211+LJJ5+MRYsWxauvvhrHH398nHnmmfHqq6/Gxz/+8bjwwgtTzAsAANCq5LIsy5rzDXvttVfMmTOn3rtYzz33XIwePTpeffXVWLRoUYwePTreeOONFh22pa1fvz6Ki4tj3bp10bNnz3yPAztl30tn53sEAICdtuSqsfkeISLStkGz3+lat25drFy5st72VatWxfr16yMiYo899ogtW7a8/+kAAABauZ06vfC8886LWbNmxSuvvBKvvvpqzJo1K84///w45ZRTIiLij3/8Y+y///4tPSsAAECr0+wLafz4xz+OCy+8MM4444zYunXrPw/SqVNMnDgxrrvuuoiIOPDAA+MnP/lJy04KAADQCjU7urp37x633HJLXHfddfHSSy9FlmWx3377Rffu3Wv3GTFiREvOCAAA0Go1O7q26969e5SUlEQul6sTXAAAAPyfZn+mq6amJr75zW9GcXFxDB48OPbZZ5/YY4894lvf+lbU1NSkmBEAAKDVavY7XZdddlnceuutcdVVV8URRxwRWZbF73//+5g2bVq8/fbb8Z3vfCfFnAAAAK1Ss6PrzjvvjJ/85Cdx8skn124rKyuLvfbaK/7t3/5NdAEAALxDs08vXLNmTRx44IH1th944IGxZs2aFhkKAACgrWh2dJWVlcWNN95Yb/uNN94YZWVlLTIUAABAW9Hs0wu/973vxdixY+Ohhx6Kww8/PHK5XDz++OOxbNmyeOCBB1LMCAAA0Go1+52uo446Kv73f/83Tj311Fi7dm2sWbMmJkyYEC+88EIceeSRKWYEAABotXbq73QNHDiw3gUzli1bFuedd17cdtttLTIYAABAW9Dsd7oas2bNmrjzzjtb6nAAAABtQotFFwAAAPWJLgAAgIREFwAAQEJNvpDGhAkTdnj/2rVr3+8sAAAAbU6To6u4uPg97z/nnHPe90AAAABtSZOj6/bbb085BwAAQJvkM10AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhFp9dC1btiyOPvroKC0tjeHDh8fPf/7zfI8EAABQq1O+B3i/OnXqFNdff32MGDEiVq5cGR/5yEfixBNPjG7duuV7NAAAgNYfXQMGDIgBAwZERETfvn2jpKQk1qxZI7oAAIDdQt5PL5w/f36MGzcuBg4cGLlcLu6///56+0yfPj2GDBkSXbp0iZEjR8ajjz7a4LEWLlwYNTU1sffeeyeeGgAAoGnyHl0bN26MsrKyuPHGGxu8f+bMmTFlypS47LLLYvHixXHkkUfGmDFjYunSpXX2W716dZxzzjlx880374qxAQAAmiTvpxeOGTMmxowZ0+j91157bZx//vkxadKkiIi4/vrr47e//W3cdNNNceWVV0ZExObNm+PUU0+Nr33tazFq1KhGj7V58+bYvHlz7e3169e30LMAAABoWN7f6dqRLVu2xJNPPhmjR4+us3306NHx+OOPR0RElmVx7rnnxic+8Yk4++yzd3i8K6+8MoqLi2u/nIYIAACktltH1xtvvBHbtm2Lfv361dner1+/WL58eURE/P73v4+ZM2fG/fffHyNGjIgRI0bEs88+2+Dxvva1r8W6detqv5YtW5b8OQAAAO1b3k8vbIpcLlfndpZltds+9rGPRU1NTZOO07lz5+jcuXOLzwcAANCY3fqdrj59+kTHjh1r39XabuXKlfXe/QIAANgd7dbRVVhYGCNHjow5c+bU2T5nzpwdXjADAABgd5H30wvffPPN+Mtf/lJ7++WXX46nnnoqSkpKYp999omLLroozj777DjkkEPi8MMPj5tvvjmWLl0akydPzuPUAAAATZP36Fq4cGEcc8wxtbcvuuiiiIiYOHFi3HHHHXH66afH6tWr45vf/Ga8/vrrMXTo0HjggQdi8ODB+RoZAACgyXJZlmX5HiJf1q9fH8XFxbFu3bro2bNnvseBnbLvpbPzPQIAwE5bctXYfI8QEWnbYLf+TBcAAEBr1y6jq7KyMkpLS6O8vDzfowAAAG1cu4yuioqKqK6ujqqqqnyPAgAAtHHtMroAAAB2FdEFAACQkOgCAABIKO9/p4u6XP67YbvLpUQBAKC5vNMFAACQkOgCAABISHQBAAAkJLoAAAASapfRVVlZGaWlpVFeXp7vUQAAgDauXUZXRUVFVFdXR1VVVb5HAQAA2rh2GV0AAAC7iugCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEioXUZXZWVllJaWRnl5eb5HAQAA2rh2GV0VFRVRXV0dVVVV+R4FAABo49pldAEAAOwqogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkFC7jK7KysooLS2N8vLyfI8CAAC0ce0yuioqKqK6ujqqqqryPQoAANDGtcvoAgAA2FVEFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAk1C6jq7KyMkpLS6O8vDzfowAAAG1cu4yuioqKqK6ujqqqqnyPAgAAtHHtMroAAAB2FdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAIKF2GV2VlZVRWloa5eXl+R4FAABo49pldFVUVER1dXVUVVXlexQAAKCNa5fRBQAAsKuILgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACTULqOrsrIySktLo7y8PN+jAAAAbVy7jK6Kioqorq6OqqqqfI8CAAC0ce0yugAAAHYV0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkFC7jK7KysooLS2N8vLyfI8CAAC0ce0yuioqKqK6ujqqqqryPQoAANDGtcvoAgAA2FVEFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEuqU7wGgKfa9dHa+RwAAgJ3inS4AAICERBcAAEBCogsAACAh0QUAAJCQ6AIAAEhIdAEAACQkugAAABISXQAAAAmJLgAAgIREFwAAQEKiCwAAICHRBQAAkJDoAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgoU75HiCfsiyLiIj169fneZL/U7N5U75HAACAXWZ3+bf49jm2N0JLatfRtWHDhoiI2HvvvfM8CQAAtE/F1+d7gro2bNgQxcXFLXrMXJYi5VqJmpqaeO2116JHjx6Ry+XyOsv69etj7733jmXLlkXPnj3zOgutm7VES7GWaCnWEi3FWqKlNLSWsiyLDRs2xMCBA6NDh5b9FFa7fqerQ4cOMWjQoHyPUUfPnj29iNAirCVairVES7GWaCnWEi3l3Wuppd/h2s6FNAAAABISXQAAAAmJrt1E586dY+rUqdG5c+d8j0IrZy3RUqwlWoq1REuxlmgpu3ottesLaQAAAKTmnS4AAICERBcAAEBCogsAACAh0QUAAJCQ6NpNTJ8+PYYMGRJdunSJkSNHxqOPPprvkdiNTJs2LXK5XJ2v/v37196fZVlMmzYtBg4cGF27do2jjz46nnvuuTrH2Lx5c3zxi1+MPn36RLdu3eLkk0+OV155ZVc/FXax+fPnx7hx42LgwIGRy+Xi/vvvr3N/S62dv//973H22WdHcXFxFBcXx9lnnx1r165N/OzYld5rLZ177rn1XqcOO+ywOvtYS1x55ZVRXl4ePXr0iL59+8Ypp5wSL7zwQp19vC7RFE1ZS7vT65Lo2g3MnDkzpkyZEpdddlksXrw4jjzyyBgzZkwsXbo036OxGzn44IPj9ddfr/169tlna+/73ve+F9dee23ceOONUVVVFf3794/jjz8+NmzYULvPlClTYtasWXHvvffGY489Fm+++WacdNJJsW3btnw8HXaRjRs3RllZWdx4440N3t9Sa+ess86Kp556Kh588MF48MEH46mnnoqzzz47+fNj13mvtRQRccIJJ9R5nXrggQfq3G8tMW/evKioqIg//OEPMWfOnNi6dWuMHj06Nm7cWLuP1yWaoilrKWI3el3KyLuPfvSj2eTJk+tsO/DAA7NLL700TxOxu5k6dWpWVlbW4H01NTVZ//79s6uuuqp229tvv50VFxdnP/rRj7Isy7K1a9dmBQUF2b333lu7z6uvvpp16NAhe/DBB5POzu4jIrJZs2bV3m6ptVNdXZ1FRPaHP/yhdp8FCxZkEZH9+c9/TvysyId3r6Usy7KJEydm48ePb/R7rCUasnLlyiwisnnz5mVZ5nWJnffutZRlu9frkne68mzLli3x5JNPxujRo+tsHz16dDz++ON5mord0YsvvhgDBw6MIUOGxBlnnBEvvfRSRES8/PLLsXz58jprqHPnznHUUUfVrqEnn3wy/vGPf9TZZ+DAgTF06FDrrB1rqbWzYMGCKC4ujkMPPbR2n8MOOyyKi4utr3bmkUceib59+8b+++8f//qv/xorV66svc9aoiHr1q2LiIiSkpKI8LrEznv3Wtpud3ldEl159sYbb8S2bduiX79+dbb369cvli9fnqep2N0ceuih8dOf/jR++9vfxi233BLLly+PUaNGxerVq2vXyY7W0PLly6OwsDB69erV6D60Py21dpYvXx59+/atd/y+fftaX+3ImDFjYsaMGfHwww/HNddcE1VVVfGJT3wiNm/eHBHWEvVlWRYXXXRRfOxjH4uhQ4dGhNcldk5Dayli93pd6rQzT4yWl8vl6tzOsqzeNtqvMWPG1P7vYcOGxeGHHx777bdf3HnnnbUfCN2ZNWSdEdEya6eh/a2v9uX000+v/d9Dhw6NQw45JAYPHhyzZ8+OCRMmNPp91lL7dcEFF8QzzzwTjz32WL37vC7RHI2tpd3pdck7XXnWp0+f6NixY71SXrlyZb3/ygPbdevWLYYNGxYvvvhi7VUMd7SG+vfvH1u2bIm///3vje5D+9NSa6d///6xYsWKesdftWqV9dWODRgwIAYPHhwvvvhiRFhL1PXFL34xfvWrX8XcuXNj0KBBtdu9LtFcja2lhuTzdUl05VlhYWGMHDky5syZU2f7nDlzYtSoUXmait3d5s2b4/nnn48BAwbEkCFDon///nXW0JYtW2LevHm1a2jkyJFRUFBQZ5/XX389/vSnP1ln7VhLrZ3DDz881q1bF3/84x9r93niiSdi3bp11lc7tnr16li2bFkMGDAgIqwl/inLsrjgggvivvvui4cffjiGDBlS536vSzTVe62lhuT1danJl9wgmXvvvTcrKCjIbr311qy6ujqbMmVK1q1bt2zJkiX5Ho3dxFe+8pXskUceyV566aXsD3/4Q3bSSSdlPXr0qF0jV111VVZcXJzdd9992bPPPpudeeaZ2YABA7L169fXHmPy5MnZoEGDsoceeihbtGhR9olPfCIrKyvLtm7dmq+nxS6wYcOGbPHixdnixYuziMiuvfbabPHixdnf/va3LMtabu2ccMIJ2fDhw7MFCxZkCxYsyIYNG5addNJJu/z5ks6O1tKGDRuyr3zlK9njjz+evfzyy9ncuXOzww8/PNtrr72sJer4whe+kBUXF2ePPPJI9vrrr9d+bdq0qXYfr0s0xXutpd3tdUl07SYqKyuzwYMHZ4WFhdlHPvKROpe7hNNPPz0bMGBAVlBQkA0cODCbMGFC9txzz9XeX1NTk02dOjXr379/1rlz5+zjH/949uyzz9Y5xltvvZVdcMEFWUlJSda1a9fspJNOypYuXbqrnwq72Ny5c7OIqPc1ceLELMtabu2sXr06+/SnP5316NEj69GjR/bpT386+/vf/76LniW7wo7W0qZNm7LRo0dne+65Z1ZQUJDts88+2cSJE+utE2uJhtZQRGS333577T5el2iK91pLu9vrUu7/HxoAAIAEfKYLAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAASEl0AAAAJiS4AAICERBcAAEBCoguAdmHatGkxYsSIfI8BQDuUy7Isy/cQAPB+5HK5Hd4/ceLEuPHGG2Pz5s3Ru3fvXTQVAPyT6AKg1Vu+fHnt/545c2Zcfvnl8cILL9Ru69q1axQXF+djNABweiEArV///v1rv4qLiyOXy9Xb9u7TC88999w45ZRT4rvf/W7069cv9thjj7jiiiti69at8e///u9RUlISgwYNittuu63OY7366qtx+umnR69evaJ3794xfvz4WLJkya59wgC0KqILgHbr4Ycfjtdeey3mz58f1157bUybNi1OOumk6NWrVzzxxBMxefLkmDx5cixbtiwiIjZt2hTHHHNMdO/ePebPnx+PPfZYdO/ePU444YTYsmVLnp8NALsr0QVAu1VSUhI//OEP44ADDojzzjsvDjjggNi0aVN8/etfjw996EPxta99LQoLC+P3v/99RETce++90aFDh/jJT34Sw4YNi4MOOihuv/32WLp0aTzyyCP5fTIA7LY65XsAAMiXgw8+ODp0+L///tivX78YOnRo7e2OHTtG7969Y+XKlRER8eSTT8Zf/vKX6NGjR53jvP322/HXv/511wwNQKsjugBotwoKCurczuVyDW6rqamJiIiampoYOXJkzJgxo96x9txzz3SDAtCqiS4AaKKPfOQjMXPmzOjbt2/07Nkz3+MA0Er4TBcANNGnP/3p6NOnT4wfPz4effTRePnll2PevHnx5S9/OV555ZV8jwfAbkp0AUATFRUVxfz582OfffaJCRMmxEEHHRTnnXdevPXWW975AqBR/jgyAABAQt7pAgAASEh0AQAAJCS6AAAAEhJdAAAACYkuAACAhEQXAABAQqILAAAgIdEFAACQkOgCAABISHQBAAAkJLoAAAAS+v8Ap6drrLVGR6sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"DISTANCE\"], \"Distribution of distance\", \"Distance\", None, False, True)\n",
    "\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_TIME\"], \"Distribution of departure time\", \"Time\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distance plot, we observe that the distance of the vast majority of flights range between a few hundred and a three thousand miles. The longest flights in the dataset are around 5000 miles. \n",
    "\n",
    "On the other hand, looking at the distribution of the departure time variabel, we see that fligh departure are almos uniformly distributed from 6 am to 8 pm. From 9 pm to 5 am, a much smaller number of departure take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWIhprxNkiCK",
    "outputId": "43978d06-cd15-4dda-b194-33eb6e73d502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DTW' 'SEA' 'DAL' 'HNL' 'ATL' 'TPA' 'PHX' 'LAX' 11292 'BWI' 'SJU' 'SMF'\n",
      " 'BNA' 'CLT' 'CAE' 'MSP' 'DFW' 'SAN' 'OGG' 'OAK' 14107 'DCA' 'BOS' 'HRL'\n",
      " 'MCI' 'SNA' 'ORD' 'MIA' 'RNO' 12451 'SFO' 'SAT' 'IAH' 'SLC' 'LAS' 'DEN'\n",
      " 'PIT' 'ACV' 'DRO' 'GSP' 'RSW' 'ITO' 12266 'LSE' 'OKC' 'DAY' 'EWR' 10299\n",
      " 'PHL' 'HOU' 11298 'MFE' 'FLL' 'MSY' 'IND' 'BZN' 'PSP' 'ASE' 'MEM' 'RDU'\n",
      " 'SBA' 15016 'SJC' 11066 'JAC' 'MCO' 'ROC' 11697 'BOI' 'CLE' 13830 10257\n",
      " 14908 'SDF' 12278 'MKE' 'JFK' 12889 10397 'RDM' 13851 'BRW' 'ALB' 'OMA'\n",
      " 'CRP' 'RIC' 'DHN' 11618 'BUR' 'CVG' 'MDW' 14893 'JAX' 13485 'ABQ' 12402\n",
      " 'CMH' 12264 10721 'PVD' 11433 'KTN' 10154 'LGA' 'PDX' 'LRD' 'SYR' 'PBI'\n",
      " 'ANC' 'BUF' 'RST' 'TUL' 'TUS' 'STL' 'TLH' 'BDL' 'AUS' 'MSN' 11278 13204\n",
      " 14771 'CHS' '13232' 'DSM' '12478' 14869 'AMA' 'LGB' 'GRR' 'IAD' 10874\n",
      " 'ONT' 'LIH' 'PIB' 15370 10980 13303 'GSO' 'MYR' 12478 11057 'BIS' 13930\n",
      " 'CDV' 14679 10713 'MOB' 14576 'KOA' 'TRI' 'CHO' 14843 'LEX' 11447 'MGM'\n",
      " 15919 14057 'ELP' 'IDA' 'MVY' 'EWN' 13487 10140 'COS' 11259 'ISP' 13495\n",
      " 13232 'CLL' '12391' 'LAN' 'SIT' 'SGF' 'EYW' 'ECP' 14524 15304 10792 'MHT'\n",
      " 'HPN' 12892 'LNK' 'MAF' 'FAR' 'ACT' 'PHF' '12953' 'AVL' 'ATW' 14635 'PNS'\n",
      " 13342 'CIU' 'TYR' 12953 'FCA' 13158 'DLH' 15024 'MLU' 14100 'CMI' 'EUG'\n",
      " 'CRW' 10423 12945 'GFK' 'CID' 'AZO' 'MHK' 'GRK' 10693 'LBB' 11630 'LIT'\n",
      " 'VPS' 'FAI' '11298' 12191 12173 14122 'COU' 'SRQ' 'SHV' 'BFL' '13891'\n",
      " 'BTR' 'SPS' 'MDT' 10821 'ORF' 'FNT' 'JAN' 'JMS' 'AEX' 'JNU' 16218 'BGM'\n",
      " 'MLI' '14986' 'FSD' 11042 'HDN' 'AVP' 'ICT' 11274 14831 'FAY' 'SBN' 'STT'\n",
      " 'EKO' 'ABR' 'SAV' '11042' 'BHM' 14027 14633 11884 'CAK' 'EGE' 'SUX' 13871\n",
      " 'GPT' 'ACK' 'PIA' 'YUM' '14107' 'AGS' 'GGG' 'GTF' 12896 'ILM' 'LAW' 14783\n",
      " 'BTV' 'LWS' 'MRY' 14747 '13851' 10529 'AKN' '14674' 'XNA' 12758 'ELM'\n",
      " 11140 14193 'FLG' 14683 14828 'VLD' 'GRB' 11775 'BIL' 'TTN' 'TWF' '12266'\n",
      " 'OTH' 'OAJ' 'FAT' 'GTR' 'GEG' 'ROA' 11537 10781 '12889' 'TYS' 11193\n",
      " '12892' 'EVV' 10994 13198 'SWF' 11481 'HSV' 12884 'PWM' 10158 'FWA' 'DIK'\n",
      " 'SGU' 12898 '10397' 'BQN' '10721' 'JLN' 'SBP' 'PLN' 11995 11097 'ABI'\n",
      " '13930' 'BTM' 11540 12217 12339 'MBS' 11637 '14831' 15376 '13244' 'BPT'\n",
      " 'BLI' 'GJT' 14492 'BRD' 'CHA' 14321 11267 'BMI' 12389 'HIB' 12992 'HLN'\n",
      " '14783' 13377 'MLB' 'BET' 'ISN' 'DBQ' 'GUC' 'TOL' 11638 '13487' '10821'\n",
      " 12448 'STC' '14747' 'ABE' 10561 11823 'VEL' 'SAF' 14730 'RAP' 11977 'LAR'\n",
      " 13296 'PBG' 'LFT' 10208 10800 'TVC' 14570 '10785' '13830' 11778 '11865'\n",
      " '13495' 14689 11315 13577 'CWA' 'APN' 13931 13244 'ACY' 15041 'FSM' 'INL'\n",
      " '11292' 12343 '15304' 'BRO' 'HYS' 'MOT' 14487 'ROW' 'GRI' 'SJT' 10685\n",
      " 12951 'OME' 'BGR' 'WYS' 12954 'CEC' 'PSC' 'PIH' 11986 15412 10747 '15016'\n",
      " 15249 'SPI' 'CPR' 10185 'MFR' 'COD' 'SUN' 14307 'DAB' 11982 13476 13891\n",
      " 'IAG' 'BJI' 'WRG' 'MEI' 'GUM' 11111 'ABY' 13433 13796 11973 '10792' 'PUB'\n",
      " 'LCH' '14771' 'GNV' 'MTJ' 11624 'TXK' 'GCK' 'GCC' 'PAH' 'CSG' '15376'\n",
      " 'RHI' 'IMT' 14489 10434 'PSE' '14307' 'HOB' '10279' 'MSO' '12191' 13029\n",
      " 'YAK' '10713' 11413 12197 '11433' 15411 'SCE' 10157 'BQK' '11618' 'CDC'\n",
      " 15096 '10431' 11109 15323 13061]\n"
     ]
    }
   ],
   "source": [
    "print(flights[\"DESTINATION_AIRPORT\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.397761</td>\n",
       "      <td>6.521791</td>\n",
       "      <td>15.751522</td>\n",
       "      <td>3.896747</td>\n",
       "      <td>1494.538370</td>\n",
       "      <td>1336.153340</td>\n",
       "      <td>9.159706</td>\n",
       "      <td>16.186079</td>\n",
       "      <td>1359.684715</td>\n",
       "      <td>142.320462</td>\n",
       "      <td>826.171319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37.434848</td>\n",
       "      <td>3.404917</td>\n",
       "      <td>8.782405</td>\n",
       "      <td>1.975532</td>\n",
       "      <td>507.725006</td>\n",
       "      <td>497.728885</td>\n",
       "      <td>34.840084</td>\n",
       "      <td>8.961210</td>\n",
       "      <td>498.663054</td>\n",
       "      <td>75.566948</td>\n",
       "      <td>610.236930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>922.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1917.000000</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1758.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>947.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>965.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>4983.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARRIVAL_DELAY         MONTH           DAY   DAY_OF_WEEK  \\\n",
       "count   14295.000000  14295.000000  14295.000000  14295.000000   \n",
       "mean        4.397761      6.521791     15.751522      3.896747   \n",
       "std        37.434848      3.404917      8.782405      1.975532   \n",
       "min       -62.000000      1.000000      1.000000      1.000000   \n",
       "25%       -13.000000      4.000000      8.000000      2.000000   \n",
       "50%        -5.000000      7.000000     16.000000      4.000000   \n",
       "75%         8.000000      9.000000     23.000000      6.000000   \n",
       "max       947.000000     12.000000     31.000000      7.000000   \n",
       "\n",
       "       SCHEDULED_ARRIVAL  DEPARTURE_TIME  DEPARTURE_DELAY      TAXI_OUT  \\\n",
       "count       14295.000000    14295.000000     14295.000000  14295.000000   \n",
       "mean         1494.538370     1336.153340         9.159706     16.186079   \n",
       "std           507.725006      497.728885        34.840084      8.961210   \n",
       "min             1.000000        1.000000       -36.000000      3.000000   \n",
       "25%          1110.000000      922.000000        -5.000000     11.000000   \n",
       "50%          1521.000000     1331.000000        -2.000000     14.000000   \n",
       "75%          1917.000000     1743.000000         8.000000     19.000000   \n",
       "max          2359.000000     2359.000000       965.000000    145.000000   \n",
       "\n",
       "         WHEELS_OFF  SCHEDULED_TIME      DISTANCE  \n",
       "count  14295.000000    14295.000000  14295.000000  \n",
       "mean    1359.684715      142.320462    826.171319  \n",
       "std      498.663054       75.566948    610.236930  \n",
       "min        1.000000       20.000000     31.000000  \n",
       "25%      936.000000       86.000000    373.000000  \n",
       "50%     1345.000000      123.000000    650.000000  \n",
       "75%     1758.000000      174.000000   1067.000000  \n",
       "max     2359.000000      680.000000   4983.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.loc[flights['WHEELS_OFF'] == 2400, 'WHEELS_OFF'] = 2359\n",
    "flights.loc[flights['DEPARTURE_TIME'] == 2400, 'DEPARTURE_TIME'] = 2359\n",
    "\n",
    "\n",
    "\n",
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZraBMwF8Miy"
   },
   "source": [
    "As we can infer from the output of the `decribe()` method, there are no values that are out of range or are non-sense in comparison with other values of the same column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the rows that contain integer values\n",
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: isinstance(x, int))]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: isinstance(x, int))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: str(x).isnumeric())]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: str(x).isnumeric())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZwbFfyp9k-l"
   },
   "source": [
    "### 1.4 Handle categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by looking at our columns to determine what type of data each one of them contains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4Mmph7r97oo",
    "outputId": "06cdb8d8-d82b-4c83-d198-60705beff9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARRIVAL_DELAY float64\n",
      "MONTH int64\n",
      "DAY int64\n",
      "DAY_OF_WEEK int64\n",
      "AIRLINE object\n",
      "ORIGIN_AIRPORT object\n",
      "DESTINATION_AIRPORT object\n",
      "SCHEDULED_ARRIVAL int64\n",
      "DEPARTURE_TIME float64\n",
      "DEPARTURE_DELAY float64\n",
      "TAXI_OUT float64\n",
      "WHEELS_OFF float64\n",
      "SCHEDULED_TIME float64\n",
      "DISTANCE int64\n"
     ]
    }
   ],
   "source": [
    "#Check nature of the columns \n",
    "for col in flights.columns:\n",
    "  print(col, flights[col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of the nature of each column we must determine which columns we are going to modify. Those columns are going to be the ones containing categorical variables. We are going to modify:\n",
    "- Month\n",
    "- Arrival_delay\n",
    "- Day\n",
    "- Day_of_week\n",
    "- Origin_airport\n",
    "- Destination_airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiHh3pHHfwVV"
   },
   "source": [
    "The next thing we want to do after observing the types of every feature is to transform some variables to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation we have chosen for the MONTH column is grouping the month by quarters. This will allow us to reduce the number of categories from 12 to 3. \n",
    "\n",
    "Additionally, when dividing the year in quarters we get a partition that closely matches the different travel seasons. Summer season matches almost perfectly with the second quarter while the first and third quarter match with winter season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "7qiBl0YzhBmX",
    "outputId": "946e44b0-5f07-4969-a137-28ebc8d1e52b"
   },
   "outputs": [],
   "source": [
    "# MONTH treatment: group months in quarters\n",
    "flights['Q_YEAR'] = flights['MONTH'].apply(lambda x: (x-1)//4 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrival_delay column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column which's data we need to process is the ARRIVAL_DELAY column. This column is the one we want to predict and the aim of our model is to be able to predict if a flight will arrive to its destination with some type of delay. Therefore, we want to convert this column into a column with binary values.\n",
    "\n",
    "We will create a new column in our dataset (i.e. DELAYED) which will have a 1 if the flight is delayed and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELAY TREATMENT\n",
    "flights['DELAYED'] = flights['ARRIVAL_DELAY'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAY column has values that range from 1 to 31, which it's unfeasible to work with. We have decided to divide the month in two fortnights and classify each day with the value of the fortnight they belong to. \n",
    "- From days 1 - 15 they will belong to the first fortnight. They will have a value of 1.\n",
    "- From days 15 - 31 they will belong to the second fortnight. The will have a value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uONZFls9ueiW",
    "outputId": "cadec565-7aa7-405b-8a98-f24f4f4e9988"
   },
   "outputs": [],
   "source": [
    "# DAY treatment \n",
    "\n",
    "flights['FORTNIGHT'] = pd.cut(flights['DAY'], bins=[0, 15, 31], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that this is an appropiate transformation because, usually, people earn they salaries at the end of the month. Therefore, people will have more money available during the first fortnight and this could mean that the number of passengers increase causing to be more flights during the first fortnight. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day_of_week column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same arguments we used for the DAY column apply to DAY_OF_WEEK. The values in this column range from 1 (i.e.: Monday) to 7 (i.e.: Sunday). Instead of working with each of this values, we are going to separate them into working days and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_OF_WEEK 1 is in-week days, 2 is weekend\n",
    "\n",
    "flights['WEEK_INFO'] = pd.cut(flights['DAY_OF_WEEK'], bins=[1, 5, 7], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airline column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to treat the AIRLINES column. Originally, our dataframe had around 15 different airlines and we considered that it was going to be unfeasible to work with all of them. Therefore, we have decided to divide them intro three major groups:\n",
    "- Major airlines\n",
    "- Low cost airlines\n",
    "- Regional airlines\n",
    "\n",
    "We believe that this is a valid division because the nature of the airline could possibly affect the amount of delayed flights they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRLINE\n",
    "\n",
    "# Define the airlines categories\n",
    "major_airlines = ['DL', 'AA', 'UA', 'US', 'AS']\n",
    "low_cost_airlines = ['WN', 'NK', 'F9', 'B6', 'VX']\n",
    "regional_airlines = ['EV', 'OO', 'MQ', 'HA']\n",
    "\n",
    "# create a new column with the airline category\n",
    "flights['AC'] = 'Other'\n",
    "flights.loc[flights['AIRLINE'].isin(major_airlines), 'AC'] = 'Major'\n",
    "flights.loc[flights['AIRLINE'].isin(low_cost_airlines), 'AC'] = 'Low-Cost'\n",
    "flights.loc[flights['AIRLINE'].isin(regional_airlines), 'AC'] = 'Regional'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Origin_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original dataset there are over 300 different airports. Although it would be great to be able to keep all of them as individual categories, we have also considered it unfeasible. So, in order to reduce the number of categories, we are going to divide the USA territory in four quadrants:\n",
    "- Upper left\n",
    "- Upper right\n",
    "- Bottom right\n",
    "- Bottom left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before being able to classify the airports as we just mentioned, we first need to get its coordinates. This can be achieved by extractinc the necessary data (i.e. longitude and latitude) from the *airports* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index of the dataset to the IATA_CODE (code that identifies each airport)\n",
    "airports = airports.set_index('IATA_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LONGITUDE_O column to each row in flights with the longitude of the origin airport\n",
    "flights['LONGITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "\n",
    "# Add LATITUDE_O column to each row in flights with the latitude of the origin airport\n",
    "flights['LATITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have detected that some rows have a *NaN* value in either their longitude or latitude. Since this happens rarely, we can delete those columns without loosing to many rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999086410354016\n"
     ]
    }
   ],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_O\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false/len(flights[\"LONGITUDE_O\"]))\n",
    "flights = flights.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have removed al conflictive rows, we can proceed with the classification of the airports by its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty column with airport location (i.e.: GR_O)\n",
    "flights['GR_O'] = ''\n",
    "\n",
    "# Classify the airports\n",
    "# Upper right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_RIGHT'\n",
    "\n",
    "# Upper left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_LEFT'\n",
    "\n",
    "# Bottom right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_RIGHT'\n",
    "\n",
    "# Bottom left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_LEFT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check that all airports have been given a location and that there are no empty quadrants, which would indicate that the classification is not correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UPPER_RIGHT', 'BOTTOM_LEFT', 'UPPER_LEFT', 'BOTTOM_RIGHT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights[\"GR_O\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy8g04lNBpmY"
   },
   "source": [
    "We observe a huge amount of features which may overfit the model in case of using them all. Therefore we will try to choose an optimal subset of explanatory variables that are going to be able to give our model the enough information in order to do good predictions. \n",
    "\n",
    "**Observation: We cannot follow like this since we need to consider some categorical variables. REMEMBER to one_shot_encode them before going on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Destination_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the same transformations we just applied for the destination airport column. We first add the coordinates of the airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['LONGITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "flights['LATITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13113\n",
      "13123\n"
     ]
    }
   ],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_D\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false)\n",
    "print(len(flights[\"LONGITUDE_D\"]))\n",
    "\n",
    "flights = flights.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And classify the destination airports based on their coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['GR_D'] = ''\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_LEFT'\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_LEFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UPPER_RIGHT', 'UPPER_LEFT', 'BOTTOM_RIGHT', 'BOTTOM_LEFT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights[\"GR_D\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of the airports classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our airports classification, we will plot them over a USA map and assing a color to each quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gmplot\n",
    "\n",
    "# # Initialize the map with the first airport as the center point\n",
    "# gmap = gmplot.GoogleMapPlotter(flights.iloc[0]['LATITUDE_O'], flights.iloc[0]['LONGITUDE_O'], zoom=3)\n",
    "\n",
    "# # Define the color mapping for different values of GR_D\n",
    "# color_map = {\n",
    "#     'UPPER_RIGHT': 'red',\n",
    "#     'UPPER_LEFT': 'green',\n",
    "#     'BOTTOM_RIGHT': 'blue',\n",
    "#     'BOTTOM_LEFT': 'purple'\n",
    "# }\n",
    "\n",
    "# # Plot each airport with its corresponding color\n",
    "# for i, row in flights.iterrows():\n",
    "#     color = color_map.get(row['GR_D'], 'gray')\n",
    "#     gmap.marker(row['LATITUDE_O'], row['LONGITUDE_O'], color=color)\n",
    "\n",
    "# # Draw the map and save it to an HTML file\n",
    "# gmap.draw('airports_map.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify scheduled_time and departure_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset has a few columns with values ranging from 0000 to 2359. This values are found in columns scheduled_time and departure_time and they represent an hour and minutes. The format is hhmm (i.e.: 1915 is 19:15).\n",
    "\n",
    "Once again, having 2359 categories is unfeasible. The solution we have come up with is dividing the day between daytime and nightime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to classify the time segment based on the input time\n",
    "def classify_time(time):\n",
    "    sunrise = 600.0   # Define the time of sunrise as 6:00 am (in decimal format)\n",
    "    sunset = 1800.0   # Define the time of sunset as 6:00 pm (in decimal format)\n",
    "    \n",
    "    if time >= sunrise and time < sunset:\n",
    "        return 'Daytime'\n",
    "    else:\n",
    "        return 'Nighttime'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined the function that will allow us to apply the classification, we apply it to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change formatting of SCHEDULED_ARRIVAL and DEPARTURE_TIME\n",
    "flights['ArrivalDayNight'] = flights['SCHEDULED_ARRIVAL'].apply(classify_time)\n",
    "flights['DepartureDayNight'] = flights['DEPARTURE_TIME'].apply(classify_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing and processing with all categorical data, we are left with a few unwanted columns, those ones we have used to create new columns. We will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ARRIVAL_DELAY', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE',\n",
       "       'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_ARRIVAL',\n",
       "       'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF',\n",
       "       'SCHEDULED_TIME', 'DISTANCE', 'Q_YEAR', 'DELAYED', 'FORTNIGHT',\n",
       "       'WEEK_INFO', 'AC', 'LONGITUDE_O', 'LATITUDE_O', 'GR_O', 'LONGITUDE_D',\n",
       "       'LATITUDE_D', 'GR_D', 'ArrivalDayNight', 'DepartureDayNight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()\n",
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[['DELAYED','DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE', 'Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O',\n",
    "       'GR_D', 'ArrivalDayNight', 'DepartureDayNight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DELAYED', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF',\n",
       "       'SCHEDULED_TIME', 'DISTANCE', 'Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC',\n",
       "       'GR_O', 'GR_D', 'ArrivalDayNight', 'DepartureDayNight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "In order to do the process correctly, normalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n",
    "\n",
    "This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance. In our case, That is what we are going to observe right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X as the features (input variables)\n",
    "X = flights.drop(\"DELAYED\", axis=1)  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Define y as the target variable\n",
    "y = flights[\"DELAYED\"]  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>],\n",
       "       [<AxesSubplot: title={'center': 'WHEELS_OFF'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_TIME'}>],\n",
       "       [<AxesSubplot: title={'center': 'DISTANCE'}>,\n",
       "        <AxesSubplot: title={'center': 'Q_YEAR'}>]], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we observe that the distribution of the features are the same than that of the whole dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have processed all our data, it migh be interesting to, once again, visualize the distribution from some of the new columns we have added to the dataset and study if we can extract any knowledge from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"Q_YEAR\"], \"Distribution of flights by quarter of the year\", \"Quarter\", None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the difference between the three quarters it is not huge, it is clear that, as we expected, the second quarter (i.e.: May to August) is the one with the greater amount of flights as it matches with the summer season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, False)\n",
    "plot_histogram_of_column(flights[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the difference is minimal. However, if we apply the logarithm to the y axis we can see that the second forntingh accumulates more flights than the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"WEEK_INFO\"], \"Flights distribution by working days and weekends\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it could be expected, weekdays accumulate almost three times more flights than weekends. However, we must keep in mind that the weekday category groups five days while the weekend only groups two days. Probably, the difference wouldn't be this big if we were plotting the average number of flights per day. However, this escapes from the scope of this project as it doesn't provide any additional information that could be helpful to predict flight delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"AC\"], \"Flights distribution by airline category\", \"Airline category\", None, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"GR_O\"], \"Flights distribution by quadrant of origin airport\", \"Quadrant\", None, False,False)\n",
    "\n",
    "plot_histogram_of_column(flights[\"GR_D\"], \"Flights distribution by quadrant of destination airport\", \"Quadrant\", None, False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the quadrants with the greater amount of flights are *UPPER_RIGHT* and *BOTTOM_RIGT*. This represents the east cost of the USA, where many of the biggest cities are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"DepartureDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)\n",
    "\n",
    "plot_histogram_of_column(flights[\"ArrivalDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logically, most of the flights departure during daytime and this trend seems to hold for the arrival time. However, if we compare both histograms we will see that the amount of flights departing during daytime is considerably greater than the flights arriving during daytime. This mean that a considerable amount of flights arrive at their destination at night time even though day departed at daytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauzanization of continuos variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models we will apply assume that our data is distributed following a gaussian distribution, we need to make sure that our data is gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we are going to determine which columns of our dataset are the ones containing continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>],\n",
       "       [<AxesSubplot: title={'center': 'WHEELS_OFF'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_TIME'}>],\n",
       "       [<AxesSubplot: title={'center': 'DISTANCE'}>, <AxesSubplot: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Declare which variables are categorical and which ones are continuous. \n",
    "\n",
    "continuous = ['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE']\n",
    "\n",
    "flights[continuous].hist()               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data doesn't follow a gaussian distribution. Before gaussianizing it, we will first remove any outliers that can be identified in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_outliers_zscore(data_series, threshold=3):\n",
    "    \"\"\"\n",
    "    Remove outliers from a pandas Series using the Z-score method.\n",
    "    :param data_series: a pandas Series containing the data\n",
    "    :param threshold: the number of standard deviations from the mean at which to consider a data point an outlier\n",
    "    :return: a new pandas Series with the outliers removed\n",
    "    \"\"\"\n",
    "    z_scores = np.abs((data_series - data_series.mean()) / data_series.std())\n",
    "    return data_series[z_scores <= threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13113"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove outliers (no outliers in the whole dataset, no need to modify X_train and X_test)\n",
    "flights_cont = remove_outliers_zscore(flights[continuous])\n",
    "len(flights_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that no outliers are removed. We can now proceed with the normalization of the continuous data defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we normalize continuous variables\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Initialize transformer with number of quantiles and output distribution\n",
    "transformer = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "# Apply transformation to continuous columns\n",
    "for col in continuous:\n",
    "    flights[col] = transformer.fit_transform(flights[col].values.reshape(-1, 1))\n",
    "    X_train[col] = transformer.fit_transform(X_train[col].values.reshape(-1, 1))\n",
    "    X_test[col] = transformer.fit_transform(X_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot the normalized data, which now clearly follow a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>],\n",
       "       [<AxesSubplot: title={'center': 'WHEELS_OFF'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_TIME'}>],\n",
       "       [<AxesSubplot: title={'center': 'DISTANCE'}>, <AxesSubplot: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train[continuous].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encodign of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing with the continuous variables, we will apply hot encoding to our categorical variables, which are:\n",
    "- Q_year\n",
    "- Fortnight\n",
    "- Week_info\n",
    "- Ac\n",
    "- Gr_o\n",
    "- Gr_d\n",
    "- ArrivalDayNight\n",
    "- DepartureDayNight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for categorical variables\n",
    "categorical = ['Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O','GR_D', 'ArrivalDayNight', 'DepartureDayNight']\n",
    "\n",
    "flights= pd.get_dummies(flights, columns=categorical)\n",
    "X_train = pd.get_dummies(X_train, columns = categorical)\n",
    "X_test = pd.get_dummies(X_test, columns = categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will start testing some initial models and check what accuracy are we getting. First of all we need to split our variables into features (i.e.: X) and target variable (i.e.: y). However recall that we already split before in order to infer from X_train which transformations may be necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data, we can initialize the model we will be using to predict flight delays. Then, using the fraction of the data we have selected for this purpouse, we will fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "GkrBH5jr6jDJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained, we are abla to make predictions based on the testing data. This will produce a one-dimentional vector containing 1's or 0's depending on if the model believes that flight is going to be delayed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.83698761 0.83984747 0.83460439 0.82268827 0.81696854]\n",
      "Mean Accuracy: 0.8302192564346997\n",
      "Standard Deviation: 0.008832209587721969\n",
      "Cross-Validation F1 Scores: [0.8225727  0.82526929 0.82127256 0.80499228 0.80238153]\n",
      "Mean F1 Score: 0.8152976721552498\n",
      "Standard Deviation: 0.009602981341004659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Assuming you have already initialized and fitted your logistic regression model\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_accuracy = cross_val_score(logreg, X_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_scores_f1 = cross_val_score(logreg, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores_accuracy)\n",
    "print(\"Mean Accuracy:\", cv_scores_accuracy.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_accuracy.std())\n",
    "\n",
    "print(\"Cross-Validation F1 Scores:\", cv_scores_f1)\n",
    "print(\"Mean F1 Score:\", cv_scores_f1.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_f1.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an 83% of accuracy of mean, which is not bad at all specially when taking into account that only a small fraction of the available data has been used troughout the training and testing of the model.\n",
    "\n",
    "Nextly we will check if getting a smaller subset of variables we can get better results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
      "       'DISTANCE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the feature selection model\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(random_state=42), threshold='mean')\n",
    "\n",
    "# Fit the feature selector on the training data\n",
    "feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training and test sets to include only the selected features\n",
    "X_train_selected = feature_selector.transform(X_train)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "\n",
    "# Get the original feature names for the selected features\n",
    "selected_feature_names = X.columns[selected_feature_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf = X_train[selected_feature_names]\n",
    "X_test_sf = X_test[selected_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train_sf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.83317445 0.83651096 0.8250715  0.82173499 0.81172545]\n",
      "Mean Accuracy: 0.8256434699714014\n",
      "Standard Deviation: 0.008765074939159272\n",
      "Cross-Validation F1 Scores: [0.81831471 0.82114188 0.81054187 0.80394385 0.79711529]\n",
      "Mean F1 Score: 0.810211518807203\n",
      "Standard Deviation: 0.008899871777217218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Assuming you have already initialized and fitted your logistic regression model\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_accuracy = cross_val_score(logreg, X_train_sf, y_train, cv=5, scoring='accuracy')\n",
    "cv_scores_f1 = cross_val_score(logreg, X_train_sf, y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores_accuracy)\n",
    "print(\"Mean Accuracy:\", cv_scores_accuracy.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_accuracy.std())\n",
    "\n",
    "print(\"Cross-Validation F1 Scores:\", cv_scores_f1)\n",
    "print(\"Mean F1 Score:\", cv_scores_f1.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_f1.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model with all the features gives better predictions, but they are kind of similar, so we will keep both subset of variables since they both work good and this new subset is much smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric for choosing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we are going to decide which of the metrics to use in order to compare our models and decide which of them is the best. We firstly observe the balance between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Balance:\n",
      "0    62.411348\n",
      "1    37.588652\n",
      "Name: DELAYED, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your target variable is stored in a DataFrame or Series named \"y\"\n",
    "class_counts = y.value_counts()\n",
    "class_balance = class_counts / len(y) * 100\n",
    "\n",
    "print(\"Class Balance:\")\n",
    "print(class_balance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With class 0 at 62.41% and class 1 at 37.59%, there is a noticeable difference in class frequencies, but it does not indicate severe class imbalance. However, it is still worth considering the implications of the imbalance and how it might affect our modeling approach.\n",
    "\n",
    "When dealing with imbalanced data, it is important to be aware that standard evaluation metrics like accuracy may not provide an accurate representation of model performance. Instead, we need to focus on metrics that are more robust to class imbalance, such as precision, recall, F1 score, or area under the ROC curve (AUC-ROC).\n",
    "\n",
    "Nevertheless, since the imbalance is not that big, we will also consider the accuracy score in order to choose our models. We mainly will take into account the F1-score (taking into account then Recall and precision that will take into account the slightly unbalanced classes that we have). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "To start with: We remember some of the options we have to optimize the learning of our models\n",
    "\n",
    "**1. Hyperparameter Tuning**: Fine-tune the hyperparameters of your model to find the optimal combination for your specific problem. Use techniques like grid search, random search, or Bayesian optimization to systematically explore the hyperparameter space and identify the best configuration.\n",
    "\n",
    "\n",
    "**2. Cross-Validation**: Utilize cross-validation techniques, such as k-fold cross-validation, to evaluate your model's performance more reliably. This helps to assess how well your model generalizes to unseen data and reduces the risk of overfitting.\n",
    "\n",
    "**3. Regularization**: Apply regularization techniques like L1 or L2 regularization (e.g., LASSO, Ridge) to prevent overfitting. Regularization helps in reducing the complexity of the model by adding a penalty term to the loss function, promoting simpler models that generalize better.\n",
    "\n",
    "\n",
    "**4. Ensemble Methods**: Explore ensemble methods like bagging, boosting, or stacking to combine multiple models and improve overall performance. Ensemble techniques can help capture different patterns in the data and reduce model variance.\n",
    "\n",
    "\n",
    "**5. Data Augmentation**: If you have limited data, consider data augmentation techniques to artificially increase the size of your training set. This can involve techniques like rotation, flipping, zooming, or adding noise to your existing data.\n",
    "\n",
    "\n",
    "**6. Early Stopping**: Implement early stopping during model training to prevent overfitting and find the optimal number of training epochs. Early stopping stops training when the model's performance on a validation set starts to deteriorate.\n",
    "\n",
    "\n",
    "**7. Batch Normalization**: Apply batch normalization techniques to normalize the activations of the previous layer in \n",
    "\n",
    "**8. Model Architecture**: Experiment with different model architectures or network architectures, such as adding or removing layers, adjusting layer sizes, or trying different activation functions. It can help to explore pre-trained models or architectures specifically designed for your problem domain.\n",
    "\n",
    "\n",
    "**9. Monitoring and Debugging**: Monitor your model during training, track performance metrics, and analyze learning curves to identify issues like underfitting, overfitting, or convergence problems. Debug any potential errors, explore misclassified samples, and consider adjusting your model or data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data-frame to store results: \n",
    "results_df = pd.DataFrame(index=[], columns= ['Accuracy', 'F1 Macro', 'Precision Macro', 'Recall Macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVC)\n",
    "First of all we recall the hyperparameters we are going to tune:\n",
    "\n",
    "1.C: Regularization parameter. It controls the trade-off between allowing training errors and maximizing the margin. Larger values of C penalize errors more, leading to a potentially narrower margin.\n",
    "\n",
    "2.kernel: Specifies the type of kernel function used for mapping the input data to a higher-dimensional feature space. Common options include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "3.gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels. It defines the influence of training samples on the decision boundary.\n",
    "\n",
    "We fit the SVC using the whole subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT  0.833937  0.817714        0.829882     0.810271"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only using the best features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT     0.833937  0.817714        0.829882     0.810271\n",
       "SVC subset features Before HT  0.832793  0.816053        0.829476     0.808078"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have very similar results, let us compute the hyperparameter tuning with the SVC using only the small part of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.8430095899394434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize the SVC model\n",
    "clf = SVC()\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit SVC using the hyperparameters that we found and using both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT     0.833937  0.817714        0.829882     0.810271\n",
       "SVC subset features Before HT  0.832793  0.816053        0.829476     0.808078\n",
       "SVC all features After HT      0.858055  0.842509        0.862018     0.831795"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='rbf', gamma = 'auto', C=10)\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT     0.833937  0.817714        0.829882     0.810271\n",
       "SVC subset features Before HT  0.832793  0.816053        0.829476     0.808078\n",
       "SVC all features After HT      0.858055  0.842509        0.862018     0.831795\n",
       "SVC subset features after HT   0.860153   0.84301        0.870671     0.829687"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='rbf', gamma = 'auto', C=10)\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features after HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now observing that the subset of features is now doing better job than that of all the features surprisingly. The best combination is the last one we fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "The parameters GaussianNB are the following: Priors: Prior probabilities of the classes. If specified, the priors are not adjusted according to the data. Var_smoothing:Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "\n",
    "However we are only going to tune var_smoothing:\n",
    "\n",
    "To proceed, we are going to fit the Naive Bayes model for both subsets of the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                   0.833937  0.817714   \n",
       "SVC subset features Before HT                0.832793  0.816053   \n",
       "SVC all features After HT                    0.858055  0.842509   \n",
       "SVC subset features after HT                 0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT               0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT  0.767016  0.747318   \n",
       "\n",
       "                                            Precision Macro Recall Macro  \n",
       "SVC all features Before HT                         0.829882     0.810271  \n",
       "SVC subset features Before HT                      0.829476     0.808078  \n",
       "SVC all features After HT                          0.862018     0.831795  \n",
       "SVC subset features after HT                       0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                     0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT        0.752331      0.74385  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB()\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB()\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute hyperparameter tuning using the smallest subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__var_smoothing': 0.1}\n",
      "Best Score:  0.8328884652049571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.25 ,0.3 , 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] \n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We now fit the model using the best parameter we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB(var_smoothing = 0.1)\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB(var_smoothing = 0.1)\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the best model is by far the one using the subset of features (the other one is probably overfitting) and in this case the hyperparamter tuning hasn't been helpful since the F1 score has decreased and the accuracy has almost remained the same. Therefore we belief that the best model is that of Gaussian Naive Bayes subset features After HT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "n_components: This hyperparameter specifies the number of components (dimensions) to retain after performing dimensionality reduction with LDA. By default, n_components is set to None, which means it will retain (n_classes - 1) components, where n_classes is the number of distinct classes in the data. In the param_grid, values of [None, 1, 2, 3] are provided to try different numbers of retained components. By setting n_components to a specific value, you can explicitly control the dimensionality of the reduced feature space.\n",
    "\n",
    "shrinkage: Shrinkage is a regularization technique used to improve the estimation of the covariance matrix in LDA, especially when the number of samples is small or the covariance matrix is ill-conditioned. The shrinkage hyperparameter controls the degree of shrinkage applied. \n",
    "In the param_grid, the values [None, 'auto', 0.1, 0.5] are provided to try different levels of shrinkage:\n",
    "None: No shrinkage is applied.\n",
    "'auto': Shrinkage is estimated using the Ledoit-Wolf lemma, which automatically determines the amount of shrinkage based on the data.\n",
    "0.1, 0.5: Specific values between 0 and 1 can be provided to manually set the shrinkage intensity.\n",
    "\n",
    "solver: This hyperparameter specifies the solver used for LDA computation. LDA can be solved using different algorithms, and the solver parameter determines the specific algorithm to use. \n",
    "In the param_grid, the values ['svd', 'lsqr', 'eigen'] are provided to try different solver algorithms:\n",
    "'svd': Singular Value Decomposition (SVD) solver, which computes the exact solution but can be slower for large datasets.\n",
    "'lsqr': Least Squares solver, which can handle both shrinkage and regularized covariance matrix.\n",
    "'eigen': Eigenvalue Decomposition solver, which computes the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "We now fit the Linear Discriminant Analysis classifier for both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.831459  0.816218   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.825102     0.810283  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.831459  0.816218   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.825102     0.810283  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do hyperparameter tuning with the small subset of features dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'n_components': None, 'shrinkage': None, 'solver': 'svd', 'tol': 1e-06}\n",
      "Best Score:  0.8314585319351764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "800 fits failed out of a total of 1200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 8 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 10 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 15 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 615, in fit\n",
      "    raise NotImplementedError(\"shrinkage not supported with 'svd' solver.\")\n",
      "NotImplementedError: shrinkage not supported with 'svd' solver.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 608, in fit\n",
      "    raise ValueError(\n",
      "ValueError: n_components cannot be larger than min(n_features, n_classes - 1).\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.83145853 0.83145853 0.83145853 0.83145853 0.83145853 0.83145853\n",
      " 0.83145853 0.83145853 0.83145853 0.83145853        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82945663 0.82945663 0.82945663 0.82945663\n",
      " 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663\n",
      "        nan        nan        nan        nan        nan 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676        nan        nan        nan\n",
      "        nan        nan 0.81982841 0.81982841 0.81982841 0.81982841\n",
      " 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841\n",
      " 0.83145853 0.83145853 0.83145853 0.83145853 0.83145853 0.83145853\n",
      " 0.83145853 0.83145853 0.83145853 0.83145853        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82945663 0.82945663 0.82945663 0.82945663\n",
      " 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663\n",
      "        nan        nan        nan        nan        nan 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676        nan        nan        nan\n",
      "        nan        nan 0.81982841 0.81982841 0.81982841 0.81982841\n",
      " 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create an LDA classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_components': [None, 1, 2, 3],    # Values to try for n_components\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5],    # Values to try for shrinkage\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],    # Values to try for solver\n",
    "    'tol': [1e-6,1e-5,1e-4, 1e-3, 1e-2]    # Values to try for tol\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=lda, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning we fit the LDA with the hyperparameters we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis(n_components = None,shrinkage = None,  solver = 'svd', tol = 1e-06)\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "\n",
    "We now fit the normal model for the perceptron:\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "penalty: Specifies the penalty term used in the update rule to handle misclassifications. It can be set to 'l1', 'l2', or 'elasticnet'. The default value is 'l2'.\n",
    "\n",
    "alpha: The constant that multiplies the penalty term if regularization is applied. It controls the strength of the regularization. The default value is 0.0001.\n",
    "\n",
    "fit_intercept: Indicates whether an intercept term should be included in the model. If set to True, the perceptron learns an intercept term. The default value is True.\n",
    "\n",
    "max_iter: The maximum number of passes over the training data (epochs) for training the perceptron. The default value is 1000.\n",
    "\n",
    "tol: The tolerance for the stopping criterion. It specifies the minimum change in the average loss for training to continue. The default value is 1e-3.\n",
    "\n",
    "shuffle: Determines whether to shuffle the training data before each epoch during training. The default value is True.\n",
    "\n",
    "eta0: The initial learning rate. It controls the step size at each update during training. The default value is 1.0.\n",
    "\n",
    "early_stopping: If set to True, training will stop when validation loss does not improve anymore. The default value is False.\n",
    "\n",
    "validation_fraction: The proportion of training data to use for early stopping validation. The default value is 0.1.\n",
    "\n",
    "n_iter_no_change: The maximum number of epochs to wait for the validation loss to improve when early_stopping is enabled. The default value is 5.\n",
    "\n",
    "We now fit the models for both subsets of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the hyper parameters using the smallest subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'alpha': 0.001, 'eta0': 0.1, 'max_iter': 1000, 'penalty': 'elasticnet', 'tol': 10}\n",
      "Best Score:  0.8057197330791229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization penalty type\n",
    "    'alpha': [0.0001, 0.001, 0.01],         # Regularization parameter\n",
    "    'max_iter': [1000, 2000, 3000],         # Maximum number of iterations\n",
    "    'eta0': [0.1, 0.01, 0.001],             # Initial learning rate\n",
    "    'tol': [10,1,1e-1,1e-2,1e-3]               # Tolerance for stopping criterion\n",
    "}\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=perceptron, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit with the found hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron(alpha = 0.001, eta0=0.1, max_iter=1000, penalty='elasticnet', tol=10)\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron(alpha = 0.001, eta0=0.1, max_iter=1000, penalty='elasticnet', tol=10)\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we observe that the best model is by far the last we fitted: Perceptron subset features After HT, since it has the best accuracy and f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boost \n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "n_estimators: The number of boosting stages to perform.\n",
    "\n",
    "learning_rate: The learning rate or shrinkage parameter, which controls the contribution of each tree.\n",
    "\n",
    "max_depth: The maximum depth of individual trees in the ensemble.\n",
    "\n",
    "subsample: The subsample ratio of the training instances.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node.\n",
    "\n",
    "We fit the model for both subset of features: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train_sf, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 4, 'min_samples_split': 4, 'subsample': 0.8}\n",
      "Best F1 Score: 0.8394708055874597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    #'n_estimators': [50, 100, 200],  # Number of boosting stages to perform\n",
    "    #'learning_rate': [0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [3, 4, 5],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [2, 3, 4]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the left hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best F1 Score: 0.8397310793851449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],  # Number of boosting stages to perform\n",
    "    'learning_rate': [1,0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [4],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [3]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=100, subsample=0.8)\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=100, subsample=0.8)\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain slightly better results with the Gradient Boost subset features After HT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Hyperparameters:\n",
    "\n",
    "n_estimators: The number of decision trees in the random forest. Increasing the number of estimators typically improves performance but increases computational complexity. It represents the ensemble size.\n",
    "\n",
    "max_depth: The maximum depth of each decision tree in the forest. Higher values increase model complexity and can lead to overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. Larger values prevent overfitting by requiring a certain number of samples in each split.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. Similar to min_samples_split, larger values help control overfitting by requiring a minimum number of samples in each leaf.\n",
    "\n",
    "max_features: The number of features to consider when looking for the best split. Reducing this number can help control overfitting. Values such as 'sqrt' or 'log2' can be used to consider a square root or logarithm of the total features, respectively.\n",
    "\n",
    "bootstrap: Determines whether bootstrap samples are used when building trees. Setting it to True enables bootstrap sampling, while False disables it. Bootstrap sampling introduces randomness into the training process and helps improve model diversity.\n",
    "\n",
    "criterion: The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We have an additional parameter (not to tune) : criterion: The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We first fit the normal model with both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to do hyperparameter tuning with the small subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    #'n_estimators': [100, 200, 500],\n",
    "    #'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [42],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    #'n_estimators': [100, 200, 500],\n",
    "    #'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['log2'],\n",
    "    'bootstrap': [True],\n",
    "    'random_state': [42],\n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': ['log2'],\n",
    "    'bootstrap': [True],\n",
    "    'random_state': [42],\n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.849285</td>\n",
       "      <td>0.83157</td>\n",
       "      <td>0.855133</td>\n",
       "      <td>0.819829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "Random Forest all features After HT             0.849285   0.83157   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  \n",
       "Random Forest all features After HT                   0.855133     0.819829  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='gini', max_depth=None, max_features='log2', min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.849285</td>\n",
       "      <td>0.83157</td>\n",
       "      <td>0.855133</td>\n",
       "      <td>0.819829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.84776</td>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.850959</td>\n",
       "      <td>0.820193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "Random Forest all features After HT             0.849285   0.83157   \n",
       "Random Forest subset features After HT           0.84776  0.830782   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  \n",
       "Random Forest all features After HT                   0.855133     0.819829  \n",
       "Random Forest subset features After HT                0.850959     0.820193  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='gini', max_depth=None, max_features='log2', min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Nets\n",
    "Hyperparameters: \n",
    "\n",
    "Learning rate: The learning rate determines how much the weights are updated in response to the estimated error each time the model weights are updated. Choosing the right learning rate can be crucial as a value too small may result in a long training process that could get stuck, while a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "Optimizer: what type of optimizer you want to use\n",
    "\n",
    "Batch size: This is the number of samples to work through before updating the internal model parameters. A smaller batch size can lead to more updates and potentially faster convergence, but it also introduces more variance, which can lead to instability in the learning process.\n",
    "\n",
    "Activation function: Different activation functions can result in significant differences in the performance of a neural network. The Rectified Linear Unit (ReLU) and its variants (like Leaky ReLU, Parametric ReLU) are often a good starting point for many problems.\n",
    "\n",
    "We now do some neural nets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in /Users/marcamps/env/lib/python3.8/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: packaging>=0.21 in /Users/marcamps/env/lib/python3.8/site-packages (from scikeras) (23.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/marcamps/env/lib/python3.8/site-packages (from scikeras) (1.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/marcamps/.local/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.24.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/marcamps/env/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/marcamps/env/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/marcamps/.local/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\r\n"
     ]
    }
   ],
   "source": [
    "# install needed packages for following modeling\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network I \n",
    "Fit neural network with all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:33:55.609002: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4325 - accuracy: 0.8092\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3591 - accuracy: 0.8481\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8557\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8580\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8618\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8625\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3230 - accuracy: 0.8642\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8677\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8688\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3112 - accuracy: 0.8695\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:09.138132: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:09.421337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4357 - accuracy: 0.8076\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3599 - accuracy: 0.8477\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8550\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8584\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8618\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8612\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8621\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8648\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8673\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3139 - accuracy: 0.8684\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:22.784284: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:23.054705: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4374 - accuracy: 0.7961\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3621 - accuracy: 0.8464\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3471 - accuracy: 0.8546\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3398 - accuracy: 0.8565\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8606\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8620\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8646\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8668\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8663\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3137 - accuracy: 0.8669\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:36.534273: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:36.826091: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4358 - accuracy: 0.8023\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3540 - accuracy: 0.8497\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3399 - accuracy: 0.8615\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8608\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8637\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8699\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8699\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3118 - accuracy: 0.8695\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3090 - accuracy: 0.8687\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8749\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:50.862893: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:51.153262: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4334 - accuracy: 0.8043\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3599 - accuracy: 0.8494\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3446 - accuracy: 0.8563\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8589\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8622\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8643\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8668\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8668\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3130 - accuracy: 0.8677\n",
      "66/66 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:35:04.985248: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with the subset of variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:38:42.304313: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4341 - accuracy: 0.8130\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3600 - accuracy: 0.8502\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3481 - accuracy: 0.8565\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3438 - accuracy: 0.8561\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3403 - accuracy: 0.8594\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3388 - accuracy: 0.8583\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3379 - accuracy: 0.8582\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8602\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8602\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3357 - accuracy: 0.8609\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:38:56.124400: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:38:56.421478: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4257 - accuracy: 0.8159\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3595 - accuracy: 0.8483\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3495 - accuracy: 0.8555\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3438 - accuracy: 0.8566\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3435 - accuracy: 0.8577\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3415 - accuracy: 0.8576\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8597\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3400 - accuracy: 0.8584\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3400 - accuracy: 0.8588\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8614\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:10.068816: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:39:10.362915: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4403 - accuracy: 0.8051\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3623 - accuracy: 0.8468\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3504 - accuracy: 0.8519\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3471 - accuracy: 0.8556\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3431 - accuracy: 0.8575\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8550\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3399 - accuracy: 0.8574\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8571\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8564\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3378 - accuracy: 0.8557\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:24.293557: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/263 [..............................] - ETA: 1:19 - loss: 0.6959 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:24.574849: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4150 - accuracy: 0.8199\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3498 - accuracy: 0.8572\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8606\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8601\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8605\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8646\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8622\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8622\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8628\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:38.337303: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:39:38.976980: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4232 - accuracy: 0.8160\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3547 - accuracy: 0.8521\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8577\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8572\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3375 - accuracy: 0.8580\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8599\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8601\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8614\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8608\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8606\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:52.696844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.849285</td>\n",
       "      <td>0.83157</td>\n",
       "      <td>0.855133</td>\n",
       "      <td>0.819829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.856339</td>\n",
       "      <td>0.841872</td>\n",
       "      <td>0.856631</td>\n",
       "      <td>0.833087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network all features before HT</th>\n",
       "      <td>0.856339</td>\n",
       "      <td>0.840602</td>\n",
       "      <td>0.860203</td>\n",
       "      <td>0.829915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset features before HT</th>\n",
       "      <td>0.860248</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.833858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "Random Forest all features After HT             0.849285   0.83157   \n",
       "Random Forest subset features After HT          0.856339  0.841872   \n",
       "Neural Network all features before HT           0.856339  0.840602   \n",
       "Neural Network I subset features before HT      0.860248  0.844776   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  \n",
       "Random Forest all features After HT                   0.855133     0.819829  \n",
       "Random Forest subset features After HT                0.856631     0.833087  \n",
       "Neural Network all features before HT                 0.860203     0.829915  \n",
       "Neural Network I subset features before HT            0.865241     0.833858  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we observe that the subset of features works better, we will do hyperparameter tuning using a the subset of the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit the model that has the best hyperparameters we found: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network II\n",
    "We firstly fit the neural network with both subsests of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning with all features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:06:17.974835: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.4614 - accuracy: 0.7908\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3635 - accuracy: 0.8470\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3473 - accuracy: 0.8568\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3398 - accuracy: 0.8620\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8644\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8651\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8665\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8659\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8683\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3148 - accuracy: 0.8714\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3119 - accuracy: 0.8727\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3117 - accuracy: 0.8733\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3068 - accuracy: 0.8732\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8725\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8749\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3010 - accuracy: 0.8754\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8763\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2986 - accuracy: 0.8754\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2962 - accuracy: 0.8794\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8782\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2931 - accuracy: 0.8787\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2908 - accuracy: 0.8775\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2895 - accuracy: 0.8782\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8795\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8814\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8818\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8816\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2797 - accuracy: 0.8843\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2797 - accuracy: 0.8825\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8850\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.8855\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8858\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8829\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8864\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2701 - accuracy: 0.8881\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2668 - accuracy: 0.8904\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2670 - accuracy: 0.8888\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8901\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8878\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8914\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8932\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8939\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8918\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8912\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2539 - accuracy: 0.8943\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8931\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.8947\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2517 - accuracy: 0.8945\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2490 - accuracy: 0.8945\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:07:22.170838: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:07:22.452516: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3930 - accuracy: 0.8295\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3492 - accuracy: 0.8509\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3395 - accuracy: 0.8576\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8596\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8614\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8646\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8639\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3180 - accuracy: 0.8659\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3138 - accuracy: 0.8696\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3133 - accuracy: 0.8702\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3090 - accuracy: 0.8699\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8708\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3055 - accuracy: 0.8709\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3015 - accuracy: 0.8723\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8749\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8754\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8767\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8791\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2888 - accuracy: 0.8774\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8813\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2846 - accuracy: 0.8806\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8808\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2801 - accuracy: 0.8817\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.2782 - accuracy: 0.8824\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8818\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.2732 - accuracy: 0.8852\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2723 - accuracy: 0.8838\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2718 - accuracy: 0.8830\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8899\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2657 - accuracy: 0.8873\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8893\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8870\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8913\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8910\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2593 - accuracy: 0.8913\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8949\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2520 - accuracy: 0.8939\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8944\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2497 - accuracy: 0.8963\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2483 - accuracy: 0.8962\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2465 - accuracy: 0.8972\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2431 - accuracy: 0.8992\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2418 - accuracy: 0.9000\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9007\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2374 - accuracy: 0.9028\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2398 - accuracy: 0.9007\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2350 - accuracy: 0.9044\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2343 - accuracy: 0.9040\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2306 - accuracy: 0.9035\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2299 - accuracy: 0.9037\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:08:28.510396: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:08:28.828648: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3891 - accuracy: 0.8314\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8520\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3388 - accuracy: 0.8565\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8588\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8636\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8656\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8656\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3189 - accuracy: 0.8687\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3159 - accuracy: 0.8679\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3133 - accuracy: 0.8680\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8725\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3097 - accuracy: 0.8686\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3050 - accuracy: 0.8744\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3030 - accuracy: 0.8755\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3025 - accuracy: 0.8738\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2980 - accuracy: 0.8776\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2973 - accuracy: 0.8774\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2945 - accuracy: 0.8800\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2928 - accuracy: 0.8787\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2897 - accuracy: 0.8810\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8813\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8808\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8827\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2824 - accuracy: 0.8851\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2813 - accuracy: 0.8839\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8832\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8894\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8874\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8869\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8849\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2701 - accuracy: 0.8880\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2675 - accuracy: 0.8881\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8906\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8904\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8912\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8919\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8944\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8916\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8955\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.8966\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8948\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2515 - accuracy: 0.8945\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2488 - accuracy: 0.8970\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2471 - accuracy: 0.8966\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2465 - accuracy: 0.8985\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2438 - accuracy: 0.8992\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2431 - accuracy: 0.8972\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2388 - accuracy: 0.9024\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2378 - accuracy: 0.9018\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.9046\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:09:35.331520: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:09:35.615800: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3796 - accuracy: 0.8344\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8595\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8632\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8669\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3201 - accuracy: 0.8659\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3150 - accuracy: 0.8692\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3126 - accuracy: 0.8713\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3091 - accuracy: 0.8733\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8746\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8735\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3003 - accuracy: 0.8760\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8757\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2945 - accuracy: 0.8804\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2939 - accuracy: 0.8802\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2915 - accuracy: 0.8791\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8810\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.8801\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8841\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8842\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2815 - accuracy: 0.8825\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8861\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8848\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2751 - accuracy: 0.8882\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8880\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2727 - accuracy: 0.8869\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8901\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8906\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8914\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8913\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8907\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2619 - accuracy: 0.8920\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.8928\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8920\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8963\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8963\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8963\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.8963\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.8962\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2483 - accuracy: 0.8992\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2451 - accuracy: 0.9005\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.8987\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.8994\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2409 - accuracy: 0.9020\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2389 - accuracy: 0.9018\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2353 - accuracy: 0.9037\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2353 - accuracy: 0.9032\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2338 - accuracy: 0.9043\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2332 - accuracy: 0.9032\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2324 - accuracy: 0.9032\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2286 - accuracy: 0.9067\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:10:43.200337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:10:43.522209: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3853 - accuracy: 0.8350\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3460 - accuracy: 0.8539\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8615\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8633\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8652\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8676\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3189 - accuracy: 0.8688\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3157 - accuracy: 0.8694\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3150 - accuracy: 0.8700\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3102 - accuracy: 0.8729\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3074 - accuracy: 0.8720\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8755\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3027 - accuracy: 0.8752\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3005 - accuracy: 0.8733\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2979 - accuracy: 0.8768\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8767\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2952 - accuracy: 0.8771\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.8806\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8799\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2884 - accuracy: 0.8808\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2856 - accuracy: 0.8795\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8820\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8837\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2803 - accuracy: 0.8845\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.2801 - accuracy: 0.8839\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8873\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.8852\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2745 - accuracy: 0.8861\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.8888\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8893\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8891\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8907\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8929\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8925\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8947\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2604 - accuracy: 0.8937\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.8957\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8976\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8950\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2532 - accuracy: 0.8976\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2525 - accuracy: 0.8962\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2511 - accuracy: 0.8991\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2503 - accuracy: 0.9013\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2455 - accuracy: 0.8997\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2473 - accuracy: 0.8998\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.9000\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9038\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9022\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2416 - accuracy: 0.9023\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2397 - accuracy: 0.9057\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:11:52.404724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:11:52.734718: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3971 - accuracy: 0.8282\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8551\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8571\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8601\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8601\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8634\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8679\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8679\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3157 - accuracy: 0.8677\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3120 - accuracy: 0.8699\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3112 - accuracy: 0.8707\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3078 - accuracy: 0.8718\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8729\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3035 - accuracy: 0.8736\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3014 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2995 - accuracy: 0.8774\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2978 - accuracy: 0.8755\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8788\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2923 - accuracy: 0.8787\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2905 - accuracy: 0.8789\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2881 - accuracy: 0.8796\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.8823\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8844\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2836 - accuracy: 0.8819\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8856\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8863\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2775 - accuracy: 0.8823\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8868\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8886\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2705 - accuracy: 0.8879\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8880\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.8895\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8924\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8912\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8917\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8926\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2590 - accuracy: 0.8938\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8951\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2538 - accuracy: 0.8976\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2522 - accuracy: 0.8980\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8964\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2478 - accuracy: 0.8987\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2496 - accuracy: 0.9011\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.9025\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.9011\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9015\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.9007\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2400 - accuracy: 0.9035\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2386 - accuracy: 0.9054\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2346 - accuracy: 0.9049\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:13:01.224672: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:13:01.554819: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3954 - accuracy: 0.8254\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3493 - accuracy: 0.8516\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3410 - accuracy: 0.8555\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8612\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8679\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8674\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8683\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3130 - accuracy: 0.8699\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8714\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3067 - accuracy: 0.8725\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3040 - accuracy: 0.8735\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8770\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2969 - accuracy: 0.8761\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2938 - accuracy: 0.8801\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8805\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8776\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.8804\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2833 - accuracy: 0.8804\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8831\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2792 - accuracy: 0.8832\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2763 - accuracy: 0.8838\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8842\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2720 - accuracy: 0.8864\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8881\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2670 - accuracy: 0.8882\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8911\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.8900\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8931\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8932\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8944\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8925\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8966\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2513 - accuracy: 0.8962\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.8956\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2471 - accuracy: 0.8979\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2478 - accuracy: 0.8975\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2440 - accuracy: 0.9011\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2415 - accuracy: 0.9007\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2391 - accuracy: 0.9009\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2382 - accuracy: 0.9016\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2366 - accuracy: 0.9024\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2333 - accuracy: 0.9046\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2346 - accuracy: 0.9042\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2318 - accuracy: 0.9041\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2279 - accuracy: 0.9080\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2282 - accuracy: 0.9074\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2252 - accuracy: 0.9069\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2231 - accuracy: 0.9085\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2251 - accuracy: 0.9075\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:14:08.547124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:14:08.842268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3973 - accuracy: 0.8211\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3523 - accuracy: 0.8506\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3427 - accuracy: 0.8543\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3368 - accuracy: 0.8570\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8602\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8633\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8648\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8676\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8680\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3156 - accuracy: 0.8687\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3123 - accuracy: 0.8708\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3120 - accuracy: 0.8695\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3073 - accuracy: 0.8726\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3049 - accuracy: 0.8738\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8751\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8756\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8751\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2960 - accuracy: 0.8769\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8793\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8789\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2916 - accuracy: 0.8780\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8818\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8806\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8816\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2833 - accuracy: 0.8810\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8830\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8844\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8839\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8863\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8856\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8860\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8885\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8882\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8895\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8920\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8910\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8936\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2603 - accuracy: 0.8920\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8948\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.8943\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2559 - accuracy: 0.8964\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8968\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2517 - accuracy: 0.8957\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.8970\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8969\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.8982\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.8959\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2451 - accuracy: 0.8998\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.8973\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2454 - accuracy: 0.8988\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:15:16.055401: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:15:16.344243: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3827 - accuracy: 0.8346\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3392 - accuracy: 0.8570\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8631\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8669\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8680\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8679\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8692\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8699\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3055 - accuracy: 0.8726\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3031 - accuracy: 0.8755\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3003 - accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2988 - accuracy: 0.8768\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2965 - accuracy: 0.8793\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2933 - accuracy: 0.8795\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2917 - accuracy: 0.8789\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8812\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8814\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8819\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8832\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8836\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8830\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8848\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2769 - accuracy: 0.8849\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8862\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8857\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2723 - accuracy: 0.8864\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8870\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2705 - accuracy: 0.8869\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8879\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8892\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8880\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8900\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8895\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8924\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2572 - accuracy: 0.8922\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8941\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8938\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2510 - accuracy: 0.8966\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2503 - accuracy: 0.8942\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2490 - accuracy: 0.8978\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.8968\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8973\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.8972\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2437 - accuracy: 0.8976\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.8978\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9005\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2387 - accuracy: 0.9010\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2384 - accuracy: 0.9009\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2363 - accuracy: 0.9026\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:16:23.337701: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:16:23.653265: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3931 - accuracy: 0.8246\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3437 - accuracy: 0.8568\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8597\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8611\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3239 - accuracy: 0.8650\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8670\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8661\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3130 - accuracy: 0.8695\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8705\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3082 - accuracy: 0.8732\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3049 - accuracy: 0.8746\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3043 - accuracy: 0.8750\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8781\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2975 - accuracy: 0.8786\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2964 - accuracy: 0.8771\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8786\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2922 - accuracy: 0.8791\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2902 - accuracy: 0.8831\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8802\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8811\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8818\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2807 - accuracy: 0.8835\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8850\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2765 - accuracy: 0.8857\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.8875\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2729 - accuracy: 0.8862\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2717 - accuracy: 0.8883\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8882\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8924\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8923\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8891\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8919\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8932\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.8919\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2573 - accuracy: 0.8944\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8947\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8950\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2489 - accuracy: 0.8974\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2492 - accuracy: 0.8962\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8981\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2443 - accuracy: 0.9015\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2445 - accuracy: 0.8990\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2423 - accuracy: 0.8992\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2389 - accuracy: 0.9000\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2385 - accuracy: 0.9023\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2366 - accuracy: 0.9028\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2345 - accuracy: 0.9040\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2359 - accuracy: 0.9015\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2314 - accuracy: 0.9055\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2297 - accuracy: 0.9074\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:17:30.393477: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:17:30.701149: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.4041 - accuracy: 0.8234\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3520 - accuracy: 0.8520\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8592\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8630\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8656\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8664\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3174 - accuracy: 0.8692\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8694\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3134 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3095 - accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8725\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3063 - accuracy: 0.8743\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3044 - accuracy: 0.8735\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3013 - accuracy: 0.8738\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3022 - accuracy: 0.8761\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8783\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2969 - accuracy: 0.8785\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2954 - accuracy: 0.8799\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2941 - accuracy: 0.8794\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2926 - accuracy: 0.8783\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2901 - accuracy: 0.8783\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2894 - accuracy: 0.8826\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8789\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2865 - accuracy: 0.8813\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2836 - accuracy: 0.8808\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8806\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2810 - accuracy: 0.8843\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2805 - accuracy: 0.8826\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8867\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.8848\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8874\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2756 - accuracy: 0.8847\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2727 - accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8897\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8895\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8867\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2677 - accuracy: 0.8891\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8910\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2638 - accuracy: 0.8901\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8913\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8907\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8905\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2601 - accuracy: 0.8910\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8928\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.8945\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2563 - accuracy: 0.8955\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8939\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2551 - accuracy: 0.8959\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2542 - accuracy: 0.8956\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:18:06.251641: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:18:06.505568: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4085 - accuracy: 0.8217\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3534 - accuracy: 0.8494\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3425 - accuracy: 0.8534\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8583\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8617\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8611\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8626\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3168 - accuracy: 0.8639\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3144 - accuracy: 0.8668\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3094 - accuracy: 0.8683\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8687\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3060 - accuracy: 0.8692\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3024 - accuracy: 0.8726\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3010 - accuracy: 0.8727\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3006 - accuracy: 0.8724\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2972 - accuracy: 0.8731\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2955 - accuracy: 0.8738\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2935 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2961 - accuracy: 0.8739\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8769\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8751\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.8752\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2832 - accuracy: 0.8799\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8780\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8826\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8817\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8806\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8798\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2761 - accuracy: 0.8812\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2753 - accuracy: 0.8829\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8826\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8814\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8848\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8857\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2685 - accuracy: 0.8860\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8858\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8861\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8874\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8897\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8892\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2588 - accuracy: 0.8895\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2586 - accuracy: 0.8901\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2579 - accuracy: 0.8908\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8885\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8918\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8919\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2515 - accuracy: 0.8931\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2501 - accuracy: 0.8947\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8939\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.8956\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:18:41.367949: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:18:41.622265: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.3907 - accuracy: 0.8310\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3500 - accuracy: 0.8478\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3386 - accuracy: 0.8571\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8606\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3286 - accuracy: 0.8593\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3260 - accuracy: 0.8627\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3215 - accuracy: 0.8656\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3176 - accuracy: 0.8656\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3145 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3123 - accuracy: 0.8677\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3093 - accuracy: 0.8689\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3080 - accuracy: 0.8700\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3053 - accuracy: 0.8698\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3032 - accuracy: 0.8737\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3013 - accuracy: 0.8743\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2999 - accuracy: 0.8754\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2979 - accuracy: 0.8765\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8774\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2963 - accuracy: 0.8768\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2914 - accuracy: 0.8780\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8807\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8799\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8819\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2862 - accuracy: 0.8826\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2823 - accuracy: 0.8854\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2835 - accuracy: 0.8831\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2808 - accuracy: 0.8829\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8841\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2772 - accuracy: 0.8864\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8860\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8881\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2740 - accuracy: 0.8863\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.8873\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8882\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8891\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8913\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.8908\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8906\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2657 - accuracy: 0.8912\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8931\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8934\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8954\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8918\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8941\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8925\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8968\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8967\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8964\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8939\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2515 - accuracy: 0.8969\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:19:16.173297: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:19:16.443386: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.4035 - accuracy: 0.8183\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3443 - accuracy: 0.8571\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8618\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8645\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8667\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3134 - accuracy: 0.8701\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3105 - accuracy: 0.8724\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8713\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3049 - accuracy: 0.8735\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3002 - accuracy: 0.8760\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8777\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2955 - accuracy: 0.8774\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2928 - accuracy: 0.8787\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8796\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8789\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2880 - accuracy: 0.8791\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8789\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2847 - accuracy: 0.8830\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8811\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8831\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2808 - accuracy: 0.8827\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8844\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8851\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8845\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8837\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2724 - accuracy: 0.8856\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2706 - accuracy: 0.8860\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.8848\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2680 - accuracy: 0.8874\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2674 - accuracy: 0.8873\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2641 - accuracy: 0.8895\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8887\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8899\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2609 - accuracy: 0.8903\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2594 - accuracy: 0.8917\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2572 - accuracy: 0.8929\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2578 - accuracy: 0.8951\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2559 - accuracy: 0.8941\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8944\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2543 - accuracy: 0.8953\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2535 - accuracy: 0.8938\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2510 - accuracy: 0.8938\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2489 - accuracy: 0.8969\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2468 - accuracy: 0.9000\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2460 - accuracy: 0.8976\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2446 - accuracy: 0.8973\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2441 - accuracy: 0.8988\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2417 - accuracy: 0.8994\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:19:51.547268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:19:52.478572: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 3s 11ms/step - loss: 0.4097 - accuracy: 0.8189\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3498 - accuracy: 0.8522\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3381 - accuracy: 0.8614\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8652\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8658\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8649\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8700\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8699\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3141 - accuracy: 0.8683\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8711\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3095 - accuracy: 0.8731\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3069 - accuracy: 0.8760\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3048 - accuracy: 0.8740\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3028 - accuracy: 0.8762\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3038 - accuracy: 0.8723\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2992 - accuracy: 0.8770\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2973 - accuracy: 0.8791\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2966 - accuracy: 0.8773\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2947 - accuracy: 0.8782\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8791\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2915 - accuracy: 0.8787\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8792\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8783\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8793\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8823\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2844 - accuracy: 0.8849\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2835 - accuracy: 0.8827\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8831\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2813 - accuracy: 0.8844\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8836\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8830\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8837\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8854\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8850\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2750 - accuracy: 0.8873\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2725 - accuracy: 0.8863\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2708 - accuracy: 0.8880\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8885\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8900\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8868\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2663 - accuracy: 0.8894\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2636 - accuracy: 0.8891\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2636 - accuracy: 0.8914\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8934\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8910\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.8938\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.8943\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8953\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2565 - accuracy: 0.8930\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8941\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:20:28.188323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:20:28.473468: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4022 - accuracy: 0.8253\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3491 - accuracy: 0.8556\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8564\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8603\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8637\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8659\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3199 - accuracy: 0.8656\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3143 - accuracy: 0.8680\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3129 - accuracy: 0.8683\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8718\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3091 - accuracy: 0.8702\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8717\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3050 - accuracy: 0.8724\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3016 - accuracy: 0.8751\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8745\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2976 - accuracy: 0.8770\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2958 - accuracy: 0.8780\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8782\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8810\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8801\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2893 - accuracy: 0.8785\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8782\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8818\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2853 - accuracy: 0.8821\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8817\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2813 - accuracy: 0.8860\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8820\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8832\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2768 - accuracy: 0.8860\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8825\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2747 - accuracy: 0.8873\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8855\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8869\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2725 - accuracy: 0.8874\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8885\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8913\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8906\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8879\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8911\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8925\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8918\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8922\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8925\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8934\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8934\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8955\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2562 - accuracy: 0.8955\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8945\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8972\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:21:03.519908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:21:03.799901: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4151 - accuracy: 0.8082\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3557 - accuracy: 0.8495\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3432 - accuracy: 0.8549\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8566\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8596\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8595\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8648\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8644\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8682\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3144 - accuracy: 0.8682\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3117 - accuracy: 0.8696\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3102 - accuracy: 0.8705\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3067 - accuracy: 0.8725\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8723\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3028 - accuracy: 0.8749\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3010 - accuracy: 0.8742\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2963 - accuracy: 0.8776\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2940 - accuracy: 0.8792\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2922 - accuracy: 0.8758\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2922 - accuracy: 0.8792\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8818\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8796\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2853 - accuracy: 0.8820\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8817\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2812 - accuracy: 0.8826\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2805 - accuracy: 0.8824\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2779 - accuracy: 0.8868\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8850\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8839\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2724 - accuracy: 0.8869\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8852\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2719 - accuracy: 0.8855\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8863\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8867\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8880\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8888\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8882\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8911\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8912\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2598 - accuracy: 0.8905\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8923\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8918\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8937\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8949\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8962\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8935\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2505 - accuracy: 0.8948\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2497 - accuracy: 0.8956\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8976\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2483 - accuracy: 0.8993\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:21:38.951531: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:21:39.236395: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4105 - accuracy: 0.8209\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3552 - accuracy: 0.8488\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3488 - accuracy: 0.8519\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3376 - accuracy: 0.8600\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3331 - accuracy: 0.8594\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3315 - accuracy: 0.8607\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3269 - accuracy: 0.8645\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8652\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3221 - accuracy: 0.8643\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8663\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8676\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3148 - accuracy: 0.8704\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3110 - accuracy: 0.8695\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3116 - accuracy: 0.8702\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8708\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8711\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8737\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3040 - accuracy: 0.8743\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3008 - accuracy: 0.8730\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3006 - accuracy: 0.8762\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2997 - accuracy: 0.8745\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2974 - accuracy: 0.8754\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8765\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2946 - accuracy: 0.8763\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2935 - accuracy: 0.8762\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2920 - accuracy: 0.8782\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2907 - accuracy: 0.8776\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2892 - accuracy: 0.8794\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8783\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8792\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8795\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8802\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8825\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2788 - accuracy: 0.8835\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8819\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8837\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8841\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8862\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8848\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8839\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8845\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.8843\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.8875\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2676 - accuracy: 0.8875\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8885\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2655 - accuracy: 0.8893\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8898\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2654 - accuracy: 0.8897\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8905\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8900\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:22:14.691329: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:22:14.973475: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3918 - accuracy: 0.8286\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3420 - accuracy: 0.8565\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3313 - accuracy: 0.8633\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3237 - accuracy: 0.8652\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3199 - accuracy: 0.8665\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3148 - accuracy: 0.8683\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3106 - accuracy: 0.8676\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3098 - accuracy: 0.8717\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3073 - accuracy: 0.8719\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3029 - accuracy: 0.8738\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8756\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2976 - accuracy: 0.8769\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8757\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2926 - accuracy: 0.8771\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2918 - accuracy: 0.8795\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2894 - accuracy: 0.8791\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8817\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8789\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8811\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8830\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8848\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2798 - accuracy: 0.8847\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8844\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8812\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8864\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8863\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2715 - accuracy: 0.8875\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8878\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2696 - accuracy: 0.8905\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8900\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2662 - accuracy: 0.8903\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8900\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2630 - accuracy: 0.8942\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2616 - accuracy: 0.8910\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8939\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2579 - accuracy: 0.8939\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8959\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2543 - accuracy: 0.8945\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8969\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8954\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2511 - accuracy: 0.8961\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2512 - accuracy: 0.8976\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2495 - accuracy: 0.8975\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.8990\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2476 - accuracy: 0.8993\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2449 - accuracy: 0.8987\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2453 - accuracy: 0.9013\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2419 - accuracy: 0.9000\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.9013\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2426 - accuracy: 0.9020\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:22:50.674443: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:22:50.944579: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.3910 - accuracy: 0.8316\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3403 - accuracy: 0.8555\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3314 - accuracy: 0.8607\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3231 - accuracy: 0.8655\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3196 - accuracy: 0.8702\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3187 - accuracy: 0.8695\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8705\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3123 - accuracy: 0.8696\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3107 - accuracy: 0.8686\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3088 - accuracy: 0.8711\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8748\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3046 - accuracy: 0.8760\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3022 - accuracy: 0.8730\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3019 - accuracy: 0.8727\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8748\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2989 - accuracy: 0.8760\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8763\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8755\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2940 - accuracy: 0.8767\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8764\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8767\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8780\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2911 - accuracy: 0.8796\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8785\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8807\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8796\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2820 - accuracy: 0.8810\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8792\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2795 - accuracy: 0.8823\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8813\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2772 - accuracy: 0.8833\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8817\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.8833\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8838\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2734 - accuracy: 0.8842\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2727 - accuracy: 0.8849\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8824\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8841\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8857\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8870\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2652 - accuracy: 0.8868\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8882\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2642 - accuracy: 0.8886\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8893\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8888\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2598 - accuracy: 0.8892\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8905\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.8920\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2566 - accuracy: 0.8894\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:23:27.129832: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:23:27.406273: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4087 - accuracy: 0.8194\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3514 - accuracy: 0.8533\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.8587\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3340 - accuracy: 0.8595\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8633\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8643\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8649\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.8680\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8695\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8689\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3106 - accuracy: 0.8720\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3075 - accuracy: 0.8745\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3074 - accuracy: 0.8738\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8740\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3036 - accuracy: 0.8730\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2998 - accuracy: 0.8765\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2995 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2983 - accuracy: 0.8755\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2961 - accuracy: 0.8782\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8779\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2946 - accuracy: 0.8781\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2912 - accuracy: 0.8762\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2888 - accuracy: 0.8796\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2896 - accuracy: 0.8795\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2867 - accuracy: 0.8800\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2854 - accuracy: 0.8837\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2847 - accuracy: 0.8794\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2822 - accuracy: 0.8804\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2809 - accuracy: 0.8825\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.8833\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2797 - accuracy: 0.8806\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2774 - accuracy: 0.8844\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2752 - accuracy: 0.8847\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2760 - accuracy: 0.8830\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2725 - accuracy: 0.8864\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2732 - accuracy: 0.8847\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2699 - accuracy: 0.8849\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2707 - accuracy: 0.8866\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8874\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.8881\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.8900\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.8905\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2620 - accuracy: 0.8908\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.8917\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2589 - accuracy: 0.8916\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2580 - accuracy: 0.8931\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2563 - accuracy: 0.8925\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2557 - accuracy: 0.8924\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2559 - accuracy: 0.8926\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:23:51.140217: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:23:51.397566: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4234 - accuracy: 0.8061\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3554 - accuracy: 0.8471\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3415 - accuracy: 0.8552\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3384 - accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3301 - accuracy: 0.8638\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8639\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8655\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.8663\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8670\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8708\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3115 - accuracy: 0.8687\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.8730\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3058 - accuracy: 0.8731\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3051 - accuracy: 0.8711\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3024 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3005 - accuracy: 0.8761\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2984 - accuracy: 0.8755\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2969 - accuracy: 0.8761\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2943 - accuracy: 0.8777\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.8769\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2913 - accuracy: 0.8793\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.8785\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2878 - accuracy: 0.8807\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2872 - accuracy: 0.8800\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.8805\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2828 - accuracy: 0.8829\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2813 - accuracy: 0.8844\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2817 - accuracy: 0.8829\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2785 - accuracy: 0.8835\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2768 - accuracy: 0.8836\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.8855\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.8862\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2721 - accuracy: 0.8863\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2728 - accuracy: 0.8845\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2692 - accuracy: 0.8875\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2680 - accuracy: 0.8885\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8892\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.8883\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.8882\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2663 - accuracy: 0.8885\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8903\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.8922\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2609 - accuracy: 0.8903\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8917\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2588 - accuracy: 0.8917\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2558 - accuracy: 0.8943\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2556 - accuracy: 0.8948\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2536 - accuracy: 0.8937\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2525 - accuracy: 0.8954\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2535 - accuracy: 0.8919\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:24:15.019385: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:24:15.259587: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4227 - accuracy: 0.8105\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3544 - accuracy: 0.8474\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3466 - accuracy: 0.8528\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3404 - accuracy: 0.8564\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8595\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3329 - accuracy: 0.8597\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8602\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.8646\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8606\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3216 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8670\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8650\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8686\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8700\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8717\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3122 - accuracy: 0.8698\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3117 - accuracy: 0.8714\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3084 - accuracy: 0.8735\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3067 - accuracy: 0.8718\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3062 - accuracy: 0.8750\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3053 - accuracy: 0.8742\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3029 - accuracy: 0.8758\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3015 - accuracy: 0.8742\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2990 - accuracy: 0.8791\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2997 - accuracy: 0.8748\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2970 - accuracy: 0.8757\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8779\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2957 - accuracy: 0.8774\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.8782\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2932 - accuracy: 0.8786\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2922 - accuracy: 0.8783\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2908 - accuracy: 0.8789\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2883 - accuracy: 0.8820\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.8791\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2862 - accuracy: 0.8825\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2858 - accuracy: 0.8825\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2839 - accuracy: 0.8833\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2832 - accuracy: 0.8833\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2824 - accuracy: 0.8848\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2808 - accuracy: 0.8847\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2800 - accuracy: 0.8841\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.8844\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2766 - accuracy: 0.8861\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.8857\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2736 - accuracy: 0.8878\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2747 - accuracy: 0.8855\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2710 - accuracy: 0.8898\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2721 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2697 - accuracy: 0.8903\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.8893\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:24:39.127950: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:24:39.389986: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4062 - accuracy: 0.8234\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3468 - accuracy: 0.8505\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3349 - accuracy: 0.8607\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.8662\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8665\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.8689\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8713\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8698\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3083 - accuracy: 0.8740\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.8730\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3033 - accuracy: 0.8731\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3009 - accuracy: 0.8758\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3000 - accuracy: 0.8743\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2993 - accuracy: 0.8756\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2960 - accuracy: 0.8776\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2948 - accuracy: 0.8795\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2930 - accuracy: 0.8799\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2915 - accuracy: 0.8808\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2888 - accuracy: 0.8819\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2868 - accuracy: 0.8841\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2883 - accuracy: 0.8832\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.8836\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.8850\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2822 - accuracy: 0.8854\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2815 - accuracy: 0.8841\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2811 - accuracy: 0.8844\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2772 - accuracy: 0.8869\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.8894\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2743 - accuracy: 0.8878\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.8854\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.8882\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2720 - accuracy: 0.8864\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2713 - accuracy: 0.8898\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2701 - accuracy: 0.8899\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2691 - accuracy: 0.8913\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2669 - accuracy: 0.8919\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2668 - accuracy: 0.8917\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.8917\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.8929\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2629 - accuracy: 0.8926\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.8924\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2622 - accuracy: 0.8930\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2594 - accuracy: 0.8948\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2589 - accuracy: 0.8961\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2598 - accuracy: 0.8937\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2570 - accuracy: 0.8961\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2559 - accuracy: 0.8966\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2583 - accuracy: 0.8941\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2554 - accuracy: 0.8975\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2531 - accuracy: 0.8960\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:25:03.354151: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:25:03.610167: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4157 - accuracy: 0.8108\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 0.3517 - accuracy: 0.8516\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3379 - accuracy: 0.8599\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3287 - accuracy: 0.8617\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.8673\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8673\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8677\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8713\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3119 - accuracy: 0.8727\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3101 - accuracy: 0.8717\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.8740\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3047 - accuracy: 0.8737\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3028 - accuracy: 0.8736\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3015 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2989 - accuracy: 0.8756\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2983 - accuracy: 0.8764\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2956 - accuracy: 0.8763\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2955 - accuracy: 0.8775\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8791\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2919 - accuracy: 0.8795\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2893 - accuracy: 0.8783\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.8787\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2871 - accuracy: 0.8792\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.8800\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.8814\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2821 - accuracy: 0.8812\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8841\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2809 - accuracy: 0.8796\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.8817\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.8826\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2750 - accuracy: 0.8825\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.8851\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.8852\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2699 - accuracy: 0.8868\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2706 - accuracy: 0.8866\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2690 - accuracy: 0.8878\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8889\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2675 - accuracy: 0.8867\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8910\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2629 - accuracy: 0.8892\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2631 - accuracy: 0.8895\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8912\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2623 - accuracy: 0.8894\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2600 - accuracy: 0.8924\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2585 - accuracy: 0.8922\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2578 - accuracy: 0.8938\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2558 - accuracy: 0.8949\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2542 - accuracy: 0.8944\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2561 - accuracy: 0.8917\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:25:28.342231: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:25:28.626464: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4193 - accuracy: 0.8133\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3489 - accuracy: 0.8537\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3411 - accuracy: 0.8561\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8608\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8624\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3260 - accuracy: 0.8633\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.8673\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8661\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8690\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3123 - accuracy: 0.8727\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3101 - accuracy: 0.8717\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3102 - accuracy: 0.8724\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3083 - accuracy: 0.8735\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8744\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8751\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.8750\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3005 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2987 - accuracy: 0.8771\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2976 - accuracy: 0.8765\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2961 - accuracy: 0.8812\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2950 - accuracy: 0.8783\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.8792\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2925 - accuracy: 0.8780\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2910 - accuracy: 0.8802\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2886 - accuracy: 0.8796\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2877 - accuracy: 0.8817\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2859 - accuracy: 0.8819\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.8829\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2840 - accuracy: 0.8813\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2827 - accuracy: 0.8832\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2821 - accuracy: 0.8844\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2802 - accuracy: 0.8841\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.8858\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.8827\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.8831\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.8863\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2735 - accuracy: 0.8850\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2727 - accuracy: 0.8869\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2707 - accuracy: 0.8882\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.8880\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2701 - accuracy: 0.8880\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2653 - accuracy: 0.8906\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2675 - accuracy: 0.8897\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8900\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2632 - accuracy: 0.8905\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.8901\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2614 - accuracy: 0.8892\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8920\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2582 - accuracy: 0.8941\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:25:52.527268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:25:52.781885: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4107 - accuracy: 0.8233\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3542 - accuracy: 0.8493\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3430 - accuracy: 0.8580\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3378 - accuracy: 0.8595\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3332 - accuracy: 0.8592\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8631\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.8662\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3214 - accuracy: 0.8651\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.8658\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3160 - accuracy: 0.8681\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8663\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3119 - accuracy: 0.8724\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.8696\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3082 - accuracy: 0.8709\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3080 - accuracy: 0.8706\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3039 - accuracy: 0.8718\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.8735\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3017 - accuracy: 0.8750\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2996 - accuracy: 0.8752\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2986 - accuracy: 0.8757\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2955 - accuracy: 0.8757\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2938 - accuracy: 0.8755\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2963 - accuracy: 0.8733\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.8754\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2894 - accuracy: 0.8793\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.8777\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2875 - accuracy: 0.8800\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2867 - accuracy: 0.8807\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.8798\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2845 - accuracy: 0.8806\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2814 - accuracy: 0.8808\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2816 - accuracy: 0.8776\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2797 - accuracy: 0.8826\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.8820\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.8825\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2780 - accuracy: 0.8844\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.8826\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2747 - accuracy: 0.8825\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2749 - accuracy: 0.8830\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2738 - accuracy: 0.8851\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2713 - accuracy: 0.8858\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.8847\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.8851\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8855\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.8861\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2656 - accuracy: 0.8854\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2664 - accuracy: 0.8879\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2635 - accuracy: 0.8878\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8883\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8878\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:26:16.843081: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:26:17.098243: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4164 - accuracy: 0.8142\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3531 - accuracy: 0.8509\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3428 - accuracy: 0.8553\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3363 - accuracy: 0.8584\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8602\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3277 - accuracy: 0.8628\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.8639\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8640\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3165 - accuracy: 0.8688\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8676\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3120 - accuracy: 0.8700\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3098 - accuracy: 0.8690\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3091 - accuracy: 0.8705\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8727\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8732\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.8740\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3001 - accuracy: 0.8754\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2994 - accuracy: 0.8754\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2964 - accuracy: 0.8780\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8768\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8783\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.8770\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2924 - accuracy: 0.8783\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.8791\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2895 - accuracy: 0.8819\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.8792\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2864 - accuracy: 0.8798\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2837 - accuracy: 0.8829\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8850\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2807 - accuracy: 0.8832\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2798 - accuracy: 0.8825\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2772 - accuracy: 0.8857\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.8845\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2762 - accuracy: 0.8854\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2743 - accuracy: 0.8869\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2732 - accuracy: 0.8882\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2715 - accuracy: 0.8892\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2692 - accuracy: 0.8895\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2691 - accuracy: 0.8897\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2674 - accuracy: 0.8900\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8903\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8898\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8924\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2635 - accuracy: 0.8912\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2617 - accuracy: 0.8929\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2600 - accuracy: 0.8932\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8901\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.8922\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2579 - accuracy: 0.8948\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2564 - accuracy: 0.8943\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:26:40.507529: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:26:40.776018: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4098 - accuracy: 0.8223\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3516 - accuracy: 0.8518\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8570\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8618\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3268 - accuracy: 0.8633\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3235 - accuracy: 0.8636\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8648\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3146 - accuracy: 0.8692\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3122 - accuracy: 0.8698\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3094 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3073 - accuracy: 0.8735\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3067 - accuracy: 0.8736\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3035 - accuracy: 0.8740\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3009 - accuracy: 0.8752\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2997 - accuracy: 0.8750\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2989 - accuracy: 0.8762\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2952 - accuracy: 0.8748\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2943 - accuracy: 0.8775\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2923 - accuracy: 0.8789\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2906 - accuracy: 0.8781\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2913 - accuracy: 0.8781\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2874 - accuracy: 0.8805\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2871 - accuracy: 0.8798\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2850 - accuracy: 0.8821\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2841 - accuracy: 0.8824\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2818 - accuracy: 0.8824\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.8842\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2799 - accuracy: 0.8839\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2787 - accuracy: 0.8854\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2755 - accuracy: 0.8843\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.8844\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2744 - accuracy: 0.8847\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2726 - accuracy: 0.8863\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2734 - accuracy: 0.8866\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2703 - accuracy: 0.8880\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2710 - accuracy: 0.8866\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2682 - accuracy: 0.8895\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2667 - accuracy: 0.8883\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8872\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2666 - accuracy: 0.8900\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.8918\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.8901\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8919\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2603 - accuracy: 0.8919\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2595 - accuracy: 0.8935\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2583 - accuracy: 0.8924\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2567 - accuracy: 0.8929\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2565 - accuracy: 0.8937\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2566 - accuracy: 0.8938\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2536 - accuracy: 0.8959\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:27:04.663345: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:27:04.929428: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4079 - accuracy: 0.8158\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8544\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.8597\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3345 - accuracy: 0.8620\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8646\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.8653\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8676\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8705\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3172 - accuracy: 0.8681\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8698\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3109 - accuracy: 0.8709\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.8719\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3047 - accuracy: 0.8745\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3046 - accuracy: 0.8743\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3040 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3011 - accuracy: 0.8763\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2990 - accuracy: 0.8760\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2979 - accuracy: 0.8744\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2967 - accuracy: 0.8762\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2940 - accuracy: 0.8773\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2923 - accuracy: 0.8786\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2909 - accuracy: 0.8807\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2897 - accuracy: 0.8795\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2900 - accuracy: 0.8787\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2854 - accuracy: 0.8794\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.8806\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.8816\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2823 - accuracy: 0.8817\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8819\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2785 - accuracy: 0.8841\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.8829\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.8849\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.8856\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2740 - accuracy: 0.8847\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2735 - accuracy: 0.8847\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2696 - accuracy: 0.8881\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2690 - accuracy: 0.8898\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2683 - accuracy: 0.8869\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2679 - accuracy: 0.8867\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2654 - accuracy: 0.8888\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2634 - accuracy: 0.8916\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2627 - accuracy: 0.8906\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2611 - accuracy: 0.8913\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2619 - accuracy: 0.8916\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2596 - accuracy: 0.8928\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2588 - accuracy: 0.8926\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2587 - accuracy: 0.8925\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2569 - accuracy: 0.8948\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2567 - accuracy: 0.8926\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2556 - accuracy: 0.8924\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:27:29.007584: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:27:29.244683: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 9ms/step - loss: 0.4012 - accuracy: 0.8255\n",
      "Epoch 2/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3488 - accuracy: 0.8548\n",
      "Epoch 3/50\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.3365 - accuracy: 0.8601\n",
      "Epoch 4/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8601\n",
      "Epoch 5/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8636\n",
      "Epoch 6/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8653\n",
      "Epoch 8/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8655\n",
      "Epoch 9/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8687\n",
      "Epoch 10/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8684\n",
      "Epoch 11/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8684\n",
      "Epoch 12/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3118 - accuracy: 0.8708\n",
      "Epoch 13/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3099 - accuracy: 0.8693\n",
      "Epoch 14/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3080 - accuracy: 0.8717\n",
      "Epoch 15/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8730\n",
      "Epoch 16/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8718\n",
      "Epoch 17/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3018 - accuracy: 0.8753\n",
      "Epoch 18/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3015 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8749\n",
      "Epoch 20/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2985 - accuracy: 0.8758\n",
      "Epoch 21/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2960 - accuracy: 0.8764\n",
      "Epoch 22/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2936 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8780\n",
      "Epoch 24/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8788\n",
      "Epoch 25/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8804\n",
      "Epoch 26/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2888 - accuracy: 0.8794\n",
      "Epoch 27/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8790\n",
      "Epoch 28/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2859 - accuracy: 0.8806\n",
      "Epoch 29/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8817\n",
      "Epoch 30/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8831\n",
      "Epoch 31/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8805\n",
      "Epoch 32/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2807 - accuracy: 0.8845\n",
      "Epoch 33/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8845\n",
      "Epoch 34/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8848\n",
      "Epoch 35/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8848\n",
      "Epoch 36/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8832\n",
      "Epoch 37/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2745 - accuracy: 0.8869\n",
      "Epoch 38/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8846\n",
      "Epoch 39/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2716 - accuracy: 0.8864\n",
      "Epoch 40/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8890\n",
      "Epoch 41/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.8869\n",
      "Epoch 42/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8887\n",
      "Epoch 43/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2680 - accuracy: 0.8888\n",
      "Epoch 44/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8885\n",
      "Epoch 45/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8894\n",
      "Epoch 46/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8914\n",
      "Epoch 47/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.8882\n",
      "Epoch 48/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8906\n",
      "Epoch 49/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8931\n",
      "Epoch 50/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8925\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x42c558670&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3c820&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x42c558670&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3c820&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x42c558670&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x42c558670&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=<function create_model at 0x42c558670>, epochs=30),\n",
       "             param_grid={'batch_size': [32, 64, 96], 'epochs': [50],\n",
       "                         'optimizer': [<keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30>,\n",
       "                                       <keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3c820>]},\n",
       "             refit='f1_macro',\n",
       "             scoring=['accuracy', 'f1_macro', 'precision_macro',\n",
       "                      'recall_macro'])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 64, 'epochs': 50, 'optimizer': <keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30>}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with small subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:32:43.021296: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.4758 - accuracy: 0.7996\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3813 - accuracy: 0.8363\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3608 - accuracy: 0.8493\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3516 - accuracy: 0.8547\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8541\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3439 - accuracy: 0.8587\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3411 - accuracy: 0.8603\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3404 - accuracy: 0.8588\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8602\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3387 - accuracy: 0.8583\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8606\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8620\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8584\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8611\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3358 - accuracy: 0.8630\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8618\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8640\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8611\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8621\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8622\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8607\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8624\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8634\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8620\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8620\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8599\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8609\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8622\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8617\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8628\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8618\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8638\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8618\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8611\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8620\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8642\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8638\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8625\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8640\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8636\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8626\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8626\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8640\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8633\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8636\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8637\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8627\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8638\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:33:16.635692: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:33:16.925359: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4652 - accuracy: 0.7946\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3791 - accuracy: 0.8394\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3623 - accuracy: 0.8500\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3528 - accuracy: 0.8528\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3495 - accuracy: 0.8531\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8543\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3451 - accuracy: 0.8568\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8541\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3418 - accuracy: 0.8571\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3410 - accuracy: 0.8569\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8586\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3392 - accuracy: 0.8568\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3384 - accuracy: 0.8587\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8605\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8594\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8589\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3376 - accuracy: 0.8582\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3358 - accuracy: 0.8613\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8595\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8599\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8608\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8600\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8603\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8601\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3348 - accuracy: 0.8605\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8605\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8622\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8590\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8612\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8617\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8612\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8612\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8599\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8619\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8597\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8605\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8601\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8630\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8628\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8611\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8620\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8619\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8620\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8621\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8630\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8620\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8622\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8630\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:33:50.856342: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:33:51.102486: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4806 - accuracy: 0.7854\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3813 - accuracy: 0.8363\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3618 - accuracy: 0.8477\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3537 - accuracy: 0.8494\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3491 - accuracy: 0.8540\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3448 - accuracy: 0.8563\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8552\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8543\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8586\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3402 - accuracy: 0.8557\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8576\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8566\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8569\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8568\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3375 - accuracy: 0.8580\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8587\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8589\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3358 - accuracy: 0.8599\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8582\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8606\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8612\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8582\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8602\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8589\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8611\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8608\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8588\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8584\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8582\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8622\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8592\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8613\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8615\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8621\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8620\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8606\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8628\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8615\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8622\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8613\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8640\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8627\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8625\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8608\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8648\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8637\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8613\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8612\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8628\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:34:25.384690: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:34:25.664220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4875 - accuracy: 0.7890\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3720 - accuracy: 0.8425\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3505 - accuracy: 0.8566\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3406 - accuracy: 0.8617\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3361 - accuracy: 0.8620\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8601\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8624\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8640\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8630\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8621\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8628\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8651\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8659\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8655\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8658\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8649\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8656\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8657\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8642\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3223 - accuracy: 0.8656\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8675\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8662\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8661\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8663\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8650\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8668\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8675\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8645\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8668\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8663\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8673\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3193 - accuracy: 0.8661\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8674\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8680\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3201 - accuracy: 0.8674\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8695\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8676\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8675\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8681\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3191 - accuracy: 0.8670\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8649\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8682\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3180 - accuracy: 0.8665\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8683\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8670\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8671\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8702\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8683\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8688\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:35:00.629499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:35:00.886216: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4779 - accuracy: 0.7815\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3812 - accuracy: 0.8335\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3605 - accuracy: 0.8501\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3504 - accuracy: 0.8528\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8583\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3414 - accuracy: 0.8569\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8593\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8600\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8592\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8606\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8614\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8597\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8609\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8595\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8608\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8627\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8624\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8627\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8626\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8637\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8622\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8643\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8648\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8652\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8643\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8643\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8648\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8631\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8633\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8636\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8645\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8631\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8662\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8649\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8637\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8649\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8644\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8642\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8640\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8651\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8640\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8664\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8649\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8652\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8682\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8662\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8655\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3231 - accuracy: 0.8645\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Mean Accuracy: 0.8591992373689228\n",
      "Mean F1 Macro: 0.8424138259306495\n",
      "Mean Precision Macro: 0.8677992723881209\n",
      "Mean Recall Macro: 0.8298988923464122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:35:35.647436: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning with subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:41:13.853214: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.4457 - accuracy: 0.8009\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3622 - accuracy: 0.8497\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3485 - accuracy: 0.8564\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8613\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8599\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3392 - accuracy: 0.8601\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3374 - accuracy: 0.8597\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3357 - accuracy: 0.8607\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8608\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8612\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8601\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8608\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8624\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8619\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8622\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8643\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8621\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3320 - accuracy: 0.8603\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8648\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8633\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8606\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8630\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8638\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8633\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8639\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8618\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8619\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8637\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8612\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8625\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8624\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8648\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8636\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8637\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8646\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8645\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8636\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8621\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8637\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8630\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8655\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8643\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8651\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8650\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8643\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8632\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8634\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8645\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3231 - accuracy: 0.8643\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:42:18.874961: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:42:19.193528: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3838 - accuracy: 0.8346\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3479 - accuracy: 0.8558\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3445 - accuracy: 0.8572\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3413 - accuracy: 0.8570\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8578\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3383 - accuracy: 0.8596\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3375 - accuracy: 0.8587\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3374 - accuracy: 0.8578\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8601\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8582\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8602\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8595\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8613\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8603\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8615\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3347 - accuracy: 0.8603\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8593\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8597\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8601\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3330 - accuracy: 0.8620\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8593\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8613\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8618\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8606\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8621\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8611\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8631\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8597\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8634\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8621\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8634\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8607\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8638\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8648\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8628\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8640\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3277 - accuracy: 0.8628\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8630\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8633\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8631\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8640\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8640\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8657\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8639\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8643\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8651\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8659\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:43:25.396228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:43:25.710255: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3821 - accuracy: 0.8354\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3504 - accuracy: 0.8510\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3453 - accuracy: 0.8559\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3417 - accuracy: 0.8545\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3406 - accuracy: 0.8556\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8578\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3389 - accuracy: 0.8592\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8599\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8589\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8588\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8593\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8617\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8590\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8596\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8584\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3347 - accuracy: 0.8600\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8597\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8584\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8615\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8605\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8606\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8611\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8639\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8612\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8625\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8625\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8615\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8619\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8614\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8607\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8611\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8619\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8618\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8630\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8646\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8634\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8644\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8631\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8621\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8633\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8631\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8624\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8637\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8627\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8639\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8664\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:44:33.090715: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:44:33.410237: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3767 - accuracy: 0.8384\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8606\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8607\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8625\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8640\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8646\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8645\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8639\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8679\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8653\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8652\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8651\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8636\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8648\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8657\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3230 - accuracy: 0.8680\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8682\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8673\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3217 - accuracy: 0.8674\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8668\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3210 - accuracy: 0.8677\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8663\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3204 - accuracy: 0.8686\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8683\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8658\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3203 - accuracy: 0.8668\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8662\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8681\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8668\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8671\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8681\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8687\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8687\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8690\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8692\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8676\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8683\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8679\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8687\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8687\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8693\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8675\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8679\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8680\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8675\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8701\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8680\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8684\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3160 - accuracy: 0.8706\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:45:41.431133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:45:42.419404: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3775 - accuracy: 0.8401\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3452 - accuracy: 0.8555\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8577\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8589\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3356 - accuracy: 0.8615\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8636\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8632\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8632\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8608\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8625\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8653\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8649\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3294 - accuracy: 0.8638\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8642\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8648\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8650\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8643\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8664\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8645\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8652\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8657\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8659\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8663\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8656\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8652\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8652\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8670\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8643\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8657\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8652\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8657\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8658\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8668\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8655\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8670\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8665\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8655\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8674\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8674\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8673\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8665\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8682\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8682\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3222 - accuracy: 0.8668\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8676\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8669\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8681\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8673\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8680\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:46:51.277610: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:46:51.595972: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3865 - accuracy: 0.8358\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3461 - accuracy: 0.8565\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3419 - accuracy: 0.8575\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8586\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3372 - accuracy: 0.8599\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8614\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8614\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8595\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8608\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8605\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8632\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8608\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8611\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8608\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8621\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8640\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8637\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8607\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8627\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8619\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8618\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8640\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8617\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8632\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8625\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8628\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8631\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8643\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8634\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8653\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8640\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8663\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8634\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8642\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8648\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8646\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8649\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8646\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8625\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8645\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8648\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8646\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8659\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8658\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8650\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8656\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3246 - accuracy: 0.8651\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:48:00.552107: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:48:00.884416: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3877 - accuracy: 0.8354\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3515 - accuracy: 0.8510\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3454 - accuracy: 0.8561\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3437 - accuracy: 0.8553\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3409 - accuracy: 0.8572\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8581\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8595\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8576\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8605\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8594\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8574\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8594\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8583\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8599\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8628\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8595\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8605\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8586\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8622\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8589\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8627\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8628\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8615\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8608\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8633\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8617\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8622\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8619\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8621\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8618\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8619\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8627\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8636\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8653\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8640\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8639\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8645\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8642\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8655\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8639\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8645\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8637\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8643\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3246 - accuracy: 0.8646\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8640\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8659\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8659\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8649\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:49:08.458991: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:49:08.768094: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3823 - accuracy: 0.8389\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3483 - accuracy: 0.8521\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3448 - accuracy: 0.8547\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3414 - accuracy: 0.8569\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8552\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8576\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8583\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3379 - accuracy: 0.8578\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8575\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3359 - accuracy: 0.8577\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8593\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8596\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8588\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8577\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8603\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8603\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8599\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8617\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8619\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8615\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8601\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8612\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8592\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8620\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8624\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8595\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8621\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8599\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8619\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8611\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8620\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8628\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8637\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3277 - accuracy: 0.8628\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8637\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8655\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8633\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8626\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8628\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8633\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8650\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8630\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8658\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8644\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8644\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8640\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8631\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:50:16.863695: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:50:17.187618: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3809 - accuracy: 0.8359\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3448 - accuracy: 0.8570\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8601\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8619\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8607\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8619\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8667\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8645\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8633\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8639\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8651\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8673\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8652\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8648\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8636\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8665\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8659\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8667\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8675\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8671\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8650\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8684\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3199 - accuracy: 0.8680\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8674\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3190 - accuracy: 0.8676\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8674\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8680\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8663\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8679\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8671\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8671\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8684\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3179 - accuracy: 0.8673\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8680\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8688\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3179 - accuracy: 0.8688\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8683\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8694\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8676\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3152 - accuracy: 0.8694\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8702\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8694\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3159 - accuracy: 0.8695\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3152 - accuracy: 0.8704\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3153 - accuracy: 0.8693\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3155 - accuracy: 0.8689\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8695\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3162 - accuracy: 0.8674\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3151 - accuracy: 0.8657\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3144 - accuracy: 0.8704\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:51:24.991091: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:51:25.308484: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3818 - accuracy: 0.8364\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3450 - accuracy: 0.8576\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3398 - accuracy: 0.8586\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8628\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8597\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8642\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8615\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8632\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8646\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8618\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3307 - accuracy: 0.8633\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8640\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8642\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3290 - accuracy: 0.8643\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8622\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3286 - accuracy: 0.8634\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8651\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8627\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8646\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8657\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8636\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8630\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8643\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8634\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8650\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8642\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8639\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8650\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8674\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8656\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3239 - accuracy: 0.8655\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8667\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8661\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8664\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8674\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8651\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8653\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8682\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8683\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3223 - accuracy: 0.8657\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8679\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8688\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8668\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8673\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8675\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:52:34.752047: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:52:35.062575: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3990 - accuracy: 0.8258\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3497 - accuracy: 0.8561\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8576\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8589\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3386 - accuracy: 0.8575\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8609\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8603\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8614\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8606\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8617\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8621\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8625\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8633\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8617\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8619\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8614\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8632\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8620\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8630\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8625\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8626\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8638\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8620\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8653\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8631\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8631\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8626\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8622\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8636\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8639\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8642\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8648\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8646\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8633\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8643\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8648\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8636\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8642\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8653\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8643\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8669\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8648\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8644\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8650\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8648\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8648\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8631\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8668\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:53:10.294860: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:53:10.568545: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3978 - accuracy: 0.8290\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3503 - accuracy: 0.8531\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3438 - accuracy: 0.8545\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3418 - accuracy: 0.8559\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3405 - accuracy: 0.8553\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3389 - accuracy: 0.8580\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8599\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8593\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8583\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8581\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8615\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8586\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8618\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8589\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8602\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8603\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3348 - accuracy: 0.8613\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8624\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8603\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8618\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8602\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8618\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8617\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8600\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8614\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8621\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8613\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8617\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8603\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8613\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8614\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8606\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8611\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8618\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8605\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8624\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8612\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8625\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8622\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8611\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8615\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8621\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8639\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8605\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8622\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8628\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3266 - accuracy: 0.8622\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8620\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:53:45.754807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:53:46.024670: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.3967 - accuracy: 0.8296\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3537 - accuracy: 0.8546\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3460 - accuracy: 0.8557\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3444 - accuracy: 0.8564\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3411 - accuracy: 0.8566\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8578\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8564\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3368 - accuracy: 0.8596\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8588\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8569\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8584\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8570\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8594\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8578\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8580\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8574\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8603\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3330 - accuracy: 0.8601\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8600\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8605\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8603\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3333 - accuracy: 0.8595\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8632\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8597\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8596\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8620\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8617\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8608\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8626\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8600\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8613\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8618\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8618\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8609\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8620\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8619\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8617\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8636\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8636\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8627\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8626\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8632\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8609\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8645\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8605\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8642\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8639\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8607\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8637\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:54:21.264850: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:54:21.537763: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.3959 - accuracy: 0.8275\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3426 - accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8618\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8631\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8652\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8657\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8649\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8659\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8649\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3246 - accuracy: 0.8642\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8693\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8676\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8646\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8651\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8664\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8680\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8659\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8659\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3222 - accuracy: 0.8690\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8677\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8661\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8667\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3210 - accuracy: 0.8675\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8677\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3203 - accuracy: 0.8693\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8682\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8667\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3190 - accuracy: 0.8680\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8684\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8694\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8671\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8686\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8679\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8687\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8688\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3175 - accuracy: 0.8681\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3180 - accuracy: 0.8670\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8681\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3175 - accuracy: 0.8674\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3168 - accuracy: 0.8674\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8693\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8689\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8694\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3161 - accuracy: 0.8693\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8675\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8700\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8680\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8698\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:54:56.672810: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:54:56.946130: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3966 - accuracy: 0.8285\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3456 - accuracy: 0.8570\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3396 - accuracy: 0.8571\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3359 - accuracy: 0.8580\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8615\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3334 - accuracy: 0.8607\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8622\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8631\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8621\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3306 - accuracy: 0.8640\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8627\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8619\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8628\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8632\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8628\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8624\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8631\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8643\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8631\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8639\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8650\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8642\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8662\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8643\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8651\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8651\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8664\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8665\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8646\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8640\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8658\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8640\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8653\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8667\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8653\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8663\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3239 - accuracy: 0.8657\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8653\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8638\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8669\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8669\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8675\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8671\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8656\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8643\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8650\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8653\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8677\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:55:32.381509: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:55:32.667291: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3967 - accuracy: 0.8279\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3497 - accuracy: 0.8557\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3453 - accuracy: 0.8575\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8564\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3389 - accuracy: 0.8608\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8590\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8607\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3359 - accuracy: 0.8603\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8611\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3342 - accuracy: 0.8603\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3347 - accuracy: 0.8615\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8605\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8619\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8622\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8619\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8607\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8603\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8619\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8612\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8617\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8633\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8612\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8639\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8628\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8634\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8628\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8633\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8617\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8630\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8638\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8643\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8634\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8646\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8632\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8624\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8628\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8657\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8642\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8636\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8638\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8632\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8626\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8657\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8636\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8628\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8640\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8640\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8657\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8640\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8646\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:56:07.859423: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:56:08.141355: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4064 - accuracy: 0.8246\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3547 - accuracy: 0.8508\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3469 - accuracy: 0.8546\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3437 - accuracy: 0.8572\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3411 - accuracy: 0.8597\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8593\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3384 - accuracy: 0.8587\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8602\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3365 - accuracy: 0.8608\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8605\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8596\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8589\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3361 - accuracy: 0.8607\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3352 - accuracy: 0.8607\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8596\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8601\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8614\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8611\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8612\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8637\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8630\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8609\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8614\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8588\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8631\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8619\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8602\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8619\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8621\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8617\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8608\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8617\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8621\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8613\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8608\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8621\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8613\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8613\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8621\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8613\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8617\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8632\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8640\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8614\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8622\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8620\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8638\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:56:43.407428: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:56:43.691042: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4036 - accuracy: 0.8233\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3526 - accuracy: 0.8568\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3471 - accuracy: 0.8547\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3434 - accuracy: 0.8552\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3414 - accuracy: 0.8566\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3409 - accuracy: 0.8572\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3391 - accuracy: 0.8559\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3388 - accuracy: 0.8601\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8593\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8601\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8577\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3356 - accuracy: 0.8592\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8578\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8602\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8586\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8602\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8600\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8592\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8602\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8596\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8606\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8606\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8619\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8607\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8599\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8626\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8625\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8599\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8611\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8599\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8612\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8614\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8614\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8614\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8634\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8617\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8609\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8626\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8620\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8625\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8628\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8640\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8636\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3277 - accuracy: 0.8626\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8613\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8645\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8637\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8638\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8645\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8625\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:57:19.158074: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:57:19.439552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3905 - accuracy: 0.8320\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3407 - accuracy: 0.8587\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8596\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8644\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3310 - accuracy: 0.8638\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8645\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8651\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8661\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8668\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8653\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8664\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8675\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8651\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8671\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3231 - accuracy: 0.8665\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8669\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8652\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8673\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3217 - accuracy: 0.8665\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8673\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8674\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3203 - accuracy: 0.8674\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8662\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3210 - accuracy: 0.8670\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8687\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8690\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8677\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8679\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3199 - accuracy: 0.8682\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8684\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8681\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3189 - accuracy: 0.8684\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3191 - accuracy: 0.8677\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8662\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8675\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8695\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8695\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3193 - accuracy: 0.8665\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8676\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3176 - accuracy: 0.8692\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8686\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8671\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8677\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8702\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8690\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8689\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8702\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8698\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8682\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:57:54.324747: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:57:54.606082: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3945 - accuracy: 0.8348\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3481 - accuracy: 0.8538\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3422 - accuracy: 0.8583\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3384 - accuracy: 0.8605\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8602\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8626\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8607\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8624\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8621\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8619\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8638\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8608\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8637\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8626\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8636\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8644\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8627\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8640\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8636\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8633\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8639\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8627\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8642\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8656\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8657\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8655\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8642\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8643\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8652\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8642\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8640\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8634\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8646\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8659\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8653\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8636\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8639\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8662\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8659\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8653\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8665\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8664\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8665\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8662\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8656\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8674\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8664\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8676\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8653\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:58:29.885016: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:58:30.177292: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 14ms/step - loss: 0.4072 - accuracy: 0.8216\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3508 - accuracy: 0.8562\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3438 - accuracy: 0.8558\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3409 - accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3388 - accuracy: 0.8593\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3381 - accuracy: 0.8601\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.8601\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3355 - accuracy: 0.8582\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3351 - accuracy: 0.8619\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8620\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3333 - accuracy: 0.8603\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3344 - accuracy: 0.8611\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8625\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8628\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3319 - accuracy: 0.8615\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3314 - accuracy: 0.8634\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8634\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8622\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8630\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8619\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8630\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.8634\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8637\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8642\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8631\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8642\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8627\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8652\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8630\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8648\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8632\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8652\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8648\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8628\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8645\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8622\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8639\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8663\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8644\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3261 - accuracy: 0.8634\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3260 - accuracy: 0.8640\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8657\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8640\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3253 - accuracy: 0.8638\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3253 - accuracy: 0.8658\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8650\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8649\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:58:54.127973: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:58:54.389995: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4160 - accuracy: 0.8177\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3541 - accuracy: 0.8508\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3456 - accuracy: 0.8546\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3422 - accuracy: 0.8587\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3425 - accuracy: 0.8580\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3386 - accuracy: 0.8587\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3379 - accuracy: 0.8593\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3379 - accuracy: 0.8584\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8577\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8588\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.8615\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8605\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.8606\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8594\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3349 - accuracy: 0.8605\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8600\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3330 - accuracy: 0.8607\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3341 - accuracy: 0.8589\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3333 - accuracy: 0.8609\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8611\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8609\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3329 - accuracy: 0.8613\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3325 - accuracy: 0.8606\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8599\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8608\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8600\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8595\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3307 - accuracy: 0.8618\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8597\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8627\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8621\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8617\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8597\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8607\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8617\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8637\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.8620\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.8620\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8624\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3277 - accuracy: 0.8636\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8620\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8626\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8630\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8627\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3275 - accuracy: 0.8639\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8619\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8614\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8632\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:59:18.230261: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:59:18.484651: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4011 - accuracy: 0.8290\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3532 - accuracy: 0.8524\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3452 - accuracy: 0.8534\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3436 - accuracy: 0.8570\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3412 - accuracy: 0.8570\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3389 - accuracy: 0.8572\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3383 - accuracy: 0.8582\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8587\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3379 - accuracy: 0.8547\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3362 - accuracy: 0.8588\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3359 - accuracy: 0.8581\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.8575\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8606\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8583\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3341 - accuracy: 0.8605\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8592\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3332 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8609\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3338 - accuracy: 0.8607\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.8614\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3328 - accuracy: 0.8600\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8596\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8621\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8612\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3319 - accuracy: 0.8594\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8607\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8615\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8588\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3316 - accuracy: 0.8601\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8620\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3306 - accuracy: 0.8624\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8621\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8611\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8617\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8632\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8613\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8609\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8618\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8626\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8619\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8615\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8627\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8613\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8626\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8628\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8614\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8607\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8631\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8624\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:59:42.679599: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:59:42.953386: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4128 - accuracy: 0.8171\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3490 - accuracy: 0.8559\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3414 - accuracy: 0.8584\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3360 - accuracy: 0.8595\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3341 - accuracy: 0.8632\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.8620\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3298 - accuracy: 0.8639\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8608\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8643\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8648\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8645\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8633\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8644\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8632\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8639\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.8628\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3240 - accuracy: 0.8636\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3241 - accuracy: 0.8640\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8657\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8643\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8657\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8673\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8661\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8676\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8658\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8664\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8665\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8670\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8680\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8658\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8687\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8667\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3193 - accuracy: 0.8682\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3194 - accuracy: 0.8670\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8680\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8674\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3192 - accuracy: 0.8684\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8689\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8681\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3191 - accuracy: 0.8679\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8706\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8687\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8687\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8704\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8676\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8686\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8676\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8682\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8686\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:00:06.851034: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:00:07.115632: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.3986 - accuracy: 0.8315\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3462 - accuracy: 0.8563\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8572\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3355 - accuracy: 0.8606\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3337 - accuracy: 0.8624\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8627\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8621\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8621\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8612\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8619\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3298 - accuracy: 0.8621\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8633\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8646\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8632\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8628\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8636\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8667\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8614\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8640\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8642\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8645\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8630\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8643\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3261 - accuracy: 0.8653\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8644\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8640\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8655\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3254 - accuracy: 0.8649\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8645\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8633\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8649\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8648\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8637\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.8650\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8650\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3236 - accuracy: 0.8665\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8648\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.8659\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8668\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8653\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8659\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8655\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8640\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3220 - accuracy: 0.8662\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8664\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8667\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8656\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8686\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8662\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:00:31.249654: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:00:31.526526: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4057 - accuracy: 0.8246\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3518 - accuracy: 0.8531\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3432 - accuracy: 0.8595\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3414 - accuracy: 0.8586\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3394 - accuracy: 0.8599\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.8599\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3384 - accuracy: 0.8581\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3355 - accuracy: 0.8624\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3351 - accuracy: 0.8602\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8614\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3350 - accuracy: 0.8594\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3332 - accuracy: 0.8625\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8611\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8617\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8622\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8617\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8621\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8607\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8639\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8614\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3321 - accuracy: 0.8625\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8611\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8617\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8634\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8645\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8627\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8637\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8644\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8646\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8642\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8637\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8638\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8630\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3287 - accuracy: 0.8646\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8644\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8673\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8638\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8649\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8643\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8634\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8646\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8646\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8642\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8651\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8658\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8640\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8661\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8655\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:00:56.138517: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:00:56.406552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4138 - accuracy: 0.8176\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3543 - accuracy: 0.8506\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3465 - accuracy: 0.8539\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3426 - accuracy: 0.8572\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3411 - accuracy: 0.8571\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3402 - accuracy: 0.8557\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3382 - accuracy: 0.8577\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8596\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8597\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3356 - accuracy: 0.8595\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3344 - accuracy: 0.8589\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3344 - accuracy: 0.8612\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8584\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8599\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3330 - accuracy: 0.8605\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8597\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8609\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3330 - accuracy: 0.8618\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8613\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8637\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8614\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8627\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.8632\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3315 - accuracy: 0.8614\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8627\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8638\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8631\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8634\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.8622\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3298 - accuracy: 0.8624\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8618\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8625\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8624\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8617\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8638\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8645\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8615\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8624\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8619\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8627\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8626\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8638\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8632\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8625\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8627\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8626\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8637\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8638\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:01:20.105507: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:01:20.372474: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4063 - accuracy: 0.8272\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3541 - accuracy: 0.8522\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3479 - accuracy: 0.8550\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3445 - accuracy: 0.8558\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3417 - accuracy: 0.8557\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3411 - accuracy: 0.8550\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3403 - accuracy: 0.8558\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8561\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3376 - accuracy: 0.8586\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3378 - accuracy: 0.8581\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3367 - accuracy: 0.8606\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8566\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3364 - accuracy: 0.8589\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3357 - accuracy: 0.8601\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8607\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8597\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3360 - accuracy: 0.8559\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3337 - accuracy: 0.8587\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3332 - accuracy: 0.8587\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8601\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8588\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3345 - accuracy: 0.8606\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8620\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8613\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3324 - accuracy: 0.8613\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8596\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8597\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8612\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8618\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8619\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8609\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8617\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8614\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.8625\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8613\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8613\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8628\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8619\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8612\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8628\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8640\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8621\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8613\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8637\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3275 - accuracy: 0.8648\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8622\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8628\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8615\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8622\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:01:44.239922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:01:44.508278: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4060 - accuracy: 0.8226\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3420 - accuracy: 0.8592\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3371 - accuracy: 0.8618\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8619\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8618\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3308 - accuracy: 0.8628\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8643\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8634\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8648\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.8644\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8638\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8664\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8640\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8655\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3236 - accuracy: 0.8638\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.8658\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.8657\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.8655\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3227 - accuracy: 0.8662\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8663\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3227 - accuracy: 0.8645\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8673\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3221 - accuracy: 0.8669\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8664\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3219 - accuracy: 0.8670\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3218 - accuracy: 0.8665\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8661\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.8676\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3212 - accuracy: 0.8671\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8682\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8689\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.8655\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8671\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8658\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8668\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8658\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8670\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.8687\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3193 - accuracy: 0.8688\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8671\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8680\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8680\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.8681\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8677\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3190 - accuracy: 0.8669\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8677\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8682\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3185 - accuracy: 0.8696\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.8668\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8684\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:02:08.457993: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:02:08.726063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4076 - accuracy: 0.8241\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3490 - accuracy: 0.8538\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3420 - accuracy: 0.8594\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.8580\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3369 - accuracy: 0.8612\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3362 - accuracy: 0.8596\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3338 - accuracy: 0.8627\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3321 - accuracy: 0.8624\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3332 - accuracy: 0.8619\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3324 - accuracy: 0.8609\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8630\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8650\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3307 - accuracy: 0.8625\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8643\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8633\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8631\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8631\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8646\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8637\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8649\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8644\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8642\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8651\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8665\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8643\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8658\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8649\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8638\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8638\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8644\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.8636\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8664\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.8659\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3254 - accuracy: 0.8644\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8643\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8655\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3240 - accuracy: 0.8661\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8661\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8653\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.8645\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8643\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3236 - accuracy: 0.8671\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8661\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8650\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.8667\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.8661\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8669\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3229 - accuracy: 0.8662\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8676\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8657\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:02:32.760868: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:02:33.016371: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 9ms/step - loss: 0.3883 - accuracy: 0.8331\n",
      "Epoch 2/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3459 - accuracy: 0.8567\n",
      "Epoch 3/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8579\n",
      "Epoch 4/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3384 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3368 - accuracy: 0.8594\n",
      "Epoch 6/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8598\n",
      "Epoch 7/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8588\n",
      "Epoch 8/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8608\n",
      "Epoch 9/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8606\n",
      "Epoch 10/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8620\n",
      "Epoch 11/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8607\n",
      "Epoch 12/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8602\n",
      "Epoch 13/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8622\n",
      "Epoch 14/50\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.3313 - accuracy: 0.8609\n",
      "Epoch 15/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8625\n",
      "Epoch 16/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8620\n",
      "Epoch 17/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8633\n",
      "Epoch 18/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8624\n",
      "Epoch 19/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8622\n",
      "Epoch 21/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8643\n",
      "Epoch 22/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8630\n",
      "Epoch 23/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8623\n",
      "Epoch 24/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8633\n",
      "Epoch 25/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8632\n",
      "Epoch 26/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8659\n",
      "Epoch 27/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8640\n",
      "Epoch 28/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8636\n",
      "Epoch 29/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8641\n",
      "Epoch 30/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8640\n",
      "Epoch 31/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8642\n",
      "Epoch 32/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8640\n",
      "Epoch 33/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8650\n",
      "Epoch 34/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8654\n",
      "Epoch 35/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8647\n",
      "Epoch 36/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8643\n",
      "Epoch 37/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8643\n",
      "Epoch 38/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8622\n",
      "Epoch 39/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8651\n",
      "Epoch 41/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8633\n",
      "Epoch 42/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8641\n",
      "Epoch 43/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8632\n",
      "Epoch 44/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8651\n",
      "Epoch 45/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8647\n",
      "Epoch 46/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8658\n",
      "Epoch 47/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8645\n",
      "Epoch 48/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8663\n",
      "Epoch 49/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8647\n",
      "Epoch 50/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=<function create_model at 0x4f94c3af0>, epochs=30),\n",
       "             param_grid={'batch_size': [32, 64, 96], 'epochs': [50],\n",
       "                         'optimizer': [<keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50>,\n",
       "                                       <keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490>]},\n",
       "             refit='f1_macro',\n",
       "             scoring=['accuracy', 'f1_macro', 'precision_macro',\n",
       "                      'recall_macro'])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 64, 'epochs': 50, 'optimizer': <keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490>}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:18:08.425784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 12ms/step - loss: 0.4709 - accuracy: 0.7861\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3751 - accuracy: 0.8440\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3542 - accuracy: 0.8495\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3437 - accuracy: 0.8556\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3381 - accuracy: 0.8574\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8595\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8611\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3267 - accuracy: 0.8634\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8655\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8659\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8673\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3155 - accuracy: 0.8673\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8687\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3106 - accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8714\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3076 - accuracy: 0.8699\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8712\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8739\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3034 - accuracy: 0.8735\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8730\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8750\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3009 - accuracy: 0.8751\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3013 - accuracy: 0.8740\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2966 - accuracy: 0.8762\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8750\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8757\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8768\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8768\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2923 - accuracy: 0.8785\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2905 - accuracy: 0.8800\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8818\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2890 - accuracy: 0.8820\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2880 - accuracy: 0.8804\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8813\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8830\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2850 - accuracy: 0.8839\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8827\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8839\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2812 - accuracy: 0.8845\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2810 - accuracy: 0.8841\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8847\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8861\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2758 - accuracy: 0.8880\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2763 - accuracy: 0.8868\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8891\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8887\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8911\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:18:43.288388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:18:43.570372: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4784 - accuracy: 0.7791\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3854 - accuracy: 0.8347\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3628 - accuracy: 0.8453\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8541\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8543\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3383 - accuracy: 0.8608\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8602\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8632\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8637\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8645\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8683\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8688\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8699\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3151 - accuracy: 0.8662\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8714\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3102 - accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3101 - accuracy: 0.8680\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3068 - accuracy: 0.8715\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3052 - accuracy: 0.8702\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8735\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3041 - accuracy: 0.8740\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8740\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8742\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2968 - accuracy: 0.8750\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2942 - accuracy: 0.8795\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8760\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2915 - accuracy: 0.8788\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2911 - accuracy: 0.8771\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8787\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8787\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2885 - accuracy: 0.8769\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8799\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2815 - accuracy: 0.8806\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8799\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8816\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8826\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8829\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8854\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8837\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2734 - accuracy: 0.8826\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2720 - accuracy: 0.8858\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8857\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8866\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.8862\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8867\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8878\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8879\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8852\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8885\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8899\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:19:17.923543: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:19:18.214063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.4750 - accuracy: 0.7877\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3872 - accuracy: 0.8341\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3640 - accuracy: 0.8464\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3530 - accuracy: 0.8503\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3462 - accuracy: 0.8546\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3405 - accuracy: 0.8568\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8580\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8609\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8609\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8633\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8653\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8649\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8671\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8658\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8681\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3146 - accuracy: 0.8693\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8701\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3085 - accuracy: 0.8726\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8744\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8748\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8751\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8740\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8762\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2997 - accuracy: 0.8776\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2987 - accuracy: 0.8763\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8789\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8768\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2931 - accuracy: 0.8799\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2900 - accuracy: 0.8793\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2896 - accuracy: 0.8793\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2886 - accuracy: 0.8795\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2881 - accuracy: 0.8794\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8835\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8819\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2841 - accuracy: 0.8833\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8808\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2814 - accuracy: 0.8810\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8814\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2780 - accuracy: 0.8832\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8848\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8856\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8848\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2736 - accuracy: 0.8854\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8880\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8897\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8878\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8875\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8886\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8923\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:19:52.460622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:19:52.729206: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4850 - accuracy: 0.7750\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3803 - accuracy: 0.8388\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3575 - accuracy: 0.8497\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3429 - accuracy: 0.8551\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8587\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8609\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8665\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8662\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3153 - accuracy: 0.8671\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3131 - accuracy: 0.8705\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3115 - accuracy: 0.8695\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3099 - accuracy: 0.8724\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8700\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8744\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3020 - accuracy: 0.8745\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3008 - accuracy: 0.8745\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2975 - accuracy: 0.8767\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2982 - accuracy: 0.8749\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2943 - accuracy: 0.8783\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8774\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8795\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2886 - accuracy: 0.8796\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8796\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8805\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8820\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8826\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8825\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8850\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8854\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8858\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2739 - accuracy: 0.8870\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.8864\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8868\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8893\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8876\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8911\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8903\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8916\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8914\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8932\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8922\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8955\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8932\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8951\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8961\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8961\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8943\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8988\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8984\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:20:27.099271: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:20:27.374418: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4757 - accuracy: 0.7844\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3759 - accuracy: 0.8409\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3578 - accuracy: 0.8499\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3460 - accuracy: 0.8555\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3403 - accuracy: 0.8603\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3363 - accuracy: 0.8619\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8614\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8628\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8651\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8689\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3204 - accuracy: 0.8652\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8679\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8694\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3150 - accuracy: 0.8689\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3122 - accuracy: 0.8704\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3111 - accuracy: 0.8712\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8720\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3078 - accuracy: 0.8715\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3064 - accuracy: 0.8733\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3028 - accuracy: 0.8761\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3038 - accuracy: 0.8738\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8727\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3004 - accuracy: 0.8743\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2990 - accuracy: 0.8724\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8752\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2971 - accuracy: 0.8754\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8781\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8779\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2943 - accuracy: 0.8767\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2904 - accuracy: 0.8763\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8776\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2891 - accuracy: 0.8835\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8814\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2876 - accuracy: 0.8804\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8818\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8802\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2854 - accuracy: 0.8805\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8838\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2825 - accuracy: 0.8817\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8827\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2786 - accuracy: 0.8826\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8835\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2779 - accuracy: 0.8839\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8843\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8862\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8856\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.8869\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8866\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8882\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Mean Accuracy: 0.8451858913250714\n",
      "Mean F1 Macro: 0.8309404592135102\n",
      "Mean Precision Macro: 0.8417203807511993\n",
      "Mean Recall Macro: 0.8249394258137215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:21:02.000896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features after HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=<function create_model at 0x4f94c3af0>, epochs=30),\n",
       "             param_grid={'batch_size': [32, 64, 96], 'epochs': [50],\n",
       "                         'optimizer': [<keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50>,\n",
       "                                       <keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490>]},\n",
       "             refit='f1_macro',\n",
       "             scoring=['accuracy', 'f1_macro', 'precision_macro',\n",
       "                      'recall_macro'])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc['Neural Network II subset features after HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
