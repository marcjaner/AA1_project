{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X1a2rkiAgHQ"
   },
   "source": [
    "#Â **AA1 Project** \n",
    "\n",
    "## Modeling delays in the air\n",
    "\n",
    "In order to predict whether a flight is likely to be delayed or not, create a ML model that will make predictions with that aim. \n",
    "\n",
    "First of all. Let's observe our data set and some first insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WFwwT0G--Vb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4naEd9v_gWI",
    "outputId": "f9312b58-6257-40ee-aac4-dd514d576d82"
   },
   "outputs": [],
   "source": [
    "airports = pd.read_csv('airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6p/292cfjv12xd9n9pzpglsj3w00000gn/T/ipykernel_4264/2534244135.py:1: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  flights = pd.read_csv('flights.csv')\n"
     ]
    }
   ],
   "source": [
    "flights = pd.read_csv('flights.csv')\n",
    "flights = flights.sample(frac=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsI8BfIVAdal"
   },
   "source": [
    "As we can se with the `describe` method, we can observe the range where each feature takes values at more or less. It is also interesting to observe the histograms of the different columns to see how each feature is distributed more or less, to get an intuition of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "WMan5A-f_l5x",
    "outputId": "5e0b5489-d212-4929-cf12-18f77639fec4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.981244</td>\n",
       "      <td>-98.378964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.616736</td>\n",
       "      <td>21.523492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.483450</td>\n",
       "      <td>-176.646030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.652040</td>\n",
       "      <td>-110.839385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.297610</td>\n",
       "      <td>-93.403070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.154675</td>\n",
       "      <td>-82.722995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.285450</td>\n",
       "      <td>-64.798560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LATITUDE   LONGITUDE\n",
       "count  319.000000  319.000000\n",
       "mean    38.981244  -98.378964\n",
       "std      8.616736   21.523492\n",
       "min     13.483450 -176.646030\n",
       "25%     33.652040 -110.839385\n",
       "50%     39.297610  -93.403070\n",
       "75%     43.154675  -82.722995\n",
       "max     71.285450  -64.798560"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()\n",
    "airports.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'YEAR'}>,\n",
       "        <AxesSubplot: title={'center': 'MONTH'}>,\n",
       "        <AxesSubplot: title={'center': 'DAY'}>,\n",
       "        <AxesSubplot: title={'center': 'DAY_OF_WEEK'}>,\n",
       "        <AxesSubplot: title={'center': 'FLIGHT_NUMBER'}>],\n",
       "       [<AxesSubplot: title={'center': 'SCHEDULED_DEPARTURE'}>,\n",
       "        <AxesSubplot: title={'center': 'DEPARTURE_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>,\n",
       "        <AxesSubplot: title={'center': 'WHEELS_OFF'}>],\n",
       "       [<AxesSubplot: title={'center': 'SCHEDULED_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'ELAPSED_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'AIR_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'DISTANCE'}>,\n",
       "        <AxesSubplot: title={'center': 'WHEELS_ON'}>],\n",
       "       [<AxesSubplot: title={'center': 'TAXI_IN'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_ARRIVAL'}>,\n",
       "        <AxesSubplot: title={'center': 'ARRIVAL_TIME'}>,\n",
       "        <AxesSubplot: title={'center': 'ARRIVAL_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'DIVERTED'}>],\n",
       "       [<AxesSubplot: title={'center': 'CANCELLED'}>,\n",
       "        <AxesSubplot: title={'center': 'AIR_SYSTEM_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'SECURITY_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'AIRLINE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'LATE_AIRCRAFT_DELAY'}>],\n",
       "       [<AxesSubplot: title={'center': 'WEATHER_DELAY'}>,\n",
       "        <AxesSubplot: >, <AxesSubplot: >, <AxesSubplot: >,\n",
       "        <AxesSubplot: >]], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flights.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the histograms, we can appreciate that some of the variables in our dataset present variables where the data appears to be quite concentrated and this could be explained by the presence of large outliers. Some other variables seem to be more uniformally distributed. However, the nature of the distribution that describes the data in our variables shouldn't worry ourselve to much right now as many data processment will be done (including gaussianization of the continuos variables and hot encoding of the categorical ones) before trainging and testing a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owSaZFCZ3-V2"
   },
   "source": [
    "## 1. Data cleaning\n",
    "### Nextly we will portray the schedule that we are going to follow. \n",
    "1. Check for duplicates: Check for and remove any duplicate rows in our dataset.\n",
    "\n",
    "2. Handle missing data: Identify any missing data and decide how to handle it. We will either remove the rows or fill in the missing data.\n",
    "\n",
    "3. Check for inconsistent data: Check for any inconsistent or erroneous data, such as values that are out of range or inconsistent with other data in the same row.\n",
    "\n",
    "4. Handle categorical data: If you have categorical data, decide how to handle it. One common approach is to use one-hot encoding.\n",
    "\n",
    "5. Normalize data: Normalize the data so that the features have similar ranges. This will prevent features with large ranges from dominating the model.\n",
    "\n",
    "6. Feature selection: Select the most relevant features for your model. You can use various techniques such as correlation analysis or principal component analysis (PCA).\n",
    "\n",
    "7. Train-test split: Finally, split your data into training and testing sets to evaluate the performance of your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9qgdnb740x-"
   },
   "source": [
    "### 1.1 Check for duplicates\n",
    "We firstly see that there are no duplicate samples with the following piece of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSrzDv455MWf",
    "outputId": "ffa33860-4374-4db7-ea5a-98cca5564064"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates = flights.duplicated()\n",
    "\n",
    "# Print the duplicate rows\n",
    "len(flights[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeODOfOB5pNx"
   },
   "source": [
    "### 1.2 Handle missing data\n",
    "In this section we have a bunch of different possibilities in order to approach the problem of missing data. \n",
    "1. Remove missing data: if the quantity of missing data is not that significative, maybe it is a good option to consider removing all those samples that contain `NaN` values since the subset of samples that is going to be deleted may not be significant while training the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W0tqcTO6hYt",
    "outputId": "9afbcca0-8c45-4053-d09b-209047d794f2"
   },
   "outputs": [],
   "source": [
    "# Count the number of NaN values in each column\n",
    "nan_counts = flights.isna().sum()\n",
    "# Print the results\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwRj6Uih68dv"
   },
   "source": [
    "From the previous output we see that the last 6 features are almost useless since the majority of the samples do not have any information of those, therefore we are going to `drop`. A part from that, we also need to delete columns that regard information that is not going to be available (a posteriori information from the flight) such as information of the time elapsed during time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g28GIbss7vHi"
   },
   "outputs": [],
   "source": [
    "cols_of_interest = ['ARRIVAL_DELAY','MONTH', 'DAY' ,'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_ARRIVAL', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME', 'DISTANCE']\n",
    "flights = flights[cols_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "jczIDNTZOJpP",
    "outputId": "2b732e2a-8915-470d-c24e-922daa387018"
   },
   "outputs": [],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV9TeZHT83xS"
   },
   "source": [
    "Once we've deleted those columns we can say that **maybe** and only **maybe** taking into account that we have more than 5M samples, deleting the other samples that contain at least one `NaN` value, may be reasonable. \n",
    "\n",
    "**CHECK WHETHER THIS IS REASONABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e82megy49YCh",
    "outputId": "c945bf9d-9c1f-4cd4-ad6b-2a5638b92934"
   },
   "outputs": [],
   "source": [
    "l_bef = len(flights)\n",
    "flights = flights.dropna(how='any')\n",
    "l_aft = len(flights)\n",
    "print(l_bef, l_aft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ0KSQzZ-D3g"
   },
   "source": [
    "We pass from $5819079$ to $5714008$ samples. In other words, we keep the $98.2\\%$ of the samples, so it may be a good option to work with these new subset of samples that still contain a vast quantity of information.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LO35Qxc-pB3"
   },
   "source": [
    "### 1.3 Check for inconsistent data\n",
    "In order to do so, we firstly observe an overview of our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "c3TuA6Cx_x5G",
    "outputId": "8b9139e8-da40-4805-efdf-01dec510440f"
   },
   "outputs": [],
   "source": [
    "flights.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will plot a few histograms of the variables in our dataset to get a deeper understanding of their nature. First of all we are going to implement a function that is going to allow us to plot the histogram and personalize a few parameters such as the title, labels for the axis and the options to apply logarithms to any of the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_of_column(data, title, x_title, y_title, x_log, y_log):\n",
    "    # Assign default title if none is provided\n",
    "    if not title:\n",
    "        title = \"Histogram\"\n",
    "\n",
    "    # Assign default title to x axis if none is provided\n",
    "    if not x_title:\n",
    "        x_title = \"Value\"\n",
    "    \n",
    "    # Assing default title to y axis if none is provided\n",
    "    if not y_title:\n",
    "        y_title = \"Count\"\n",
    "    \n",
    "    # Apply logarithm to x axis if required\n",
    "    if x_log:\n",
    "        # Set up histogram with logarithmic x-axis\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data, bins=10**np.linspace(np.log10(0.1), np.log10(data.max()), 50))\n",
    "\n",
    "        # Set x-axis to logarithmic scale\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "        # Add \"log\" to x axis title\n",
    "        x_title = \"Log \" + x_title\n",
    "    else:\n",
    "        # Create plot with histogram of DAY column\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data)\n",
    "\n",
    "\n",
    "    # Apply logarithm to y-axis if required\n",
    "    if y_log:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        # Add \"log\" to y axis title\n",
    "        y_title = \"Log \" + y_title\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a function that allows us to easily plot any column from the dataset that we are interested in, we will start by studying the nature of the columns that refer to delays on the flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the arrival_delay column\n",
    "plot_histogram_of_column(flights[\"ARRIVAL_DELAY\"], \"Distribution of arrival delay\", None, None, True, False)\n",
    "\n",
    "# Histogram of DEPARTURE_DELAY column\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_DELAY\"], \"Distribution of departure delay\", None, None, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that both histograms share a similar shape, which means that these two variables are strongly correlated. However, we expected to get this result since those flights that suffer departure with any form of delay are extremely likely to also arrive with a similar delay.\n",
    "\n",
    "Another interesting result extracted from the data is that there are quite a lot of flights that depart before their scheduled departure time. Moreover, if we compare closely both histograms, we will notice that the amount of flights that arrive before their arrival time is smaller than the number of flights that departure ahead of their departure time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to analyse how the variables containing data related with the date of the flight are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(flights[\"MONTH\"], \"Distribution of month\", None, None, False, False)\n",
    "\n",
    "# Create plot with histogram of DAY column\n",
    "plot_histogram_of_column(flights[\"DAY\"], \"Distribution of day\", None, None, False, True)\n",
    "\n",
    "# Histogram of DAY_OF_WEEK colum\n",
    "plot_histogram_of_column(flights[\"DAY_OF_WEEK\"], \"Distribution of day of week\", None, None, False, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we'll plot some other columns that could be interesting to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(flights[\"DISTANCE\"], \"Distribution of distance\", \"Distance\", None, False, True)\n",
    "\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_TIME\"], \"Distribution of departure time\", \"Time\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distance plot, we observe that the distance of the vast majority of flights range between a few hundred and a three thousand miles. The longest flights in the dataset are around 5000 miles. \n",
    "\n",
    "On the other hand, looking at the distribution of the departure time variabel, we see that fligh departure are almos uniformly distributed from 6 am to 8 pm. From 9 pm to 5 am, a much smaller number of departure take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWIhprxNkiCK",
    "outputId": "43978d06-cd15-4dda-b194-33eb6e73d502"
   },
   "outputs": [],
   "source": [
    "print(flights[\"DESTINATION_AIRPORT\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.loc[flights['WHEELS_OFF'] == 2400, 'WHEELS_OFF'] = 2359\n",
    "flights.loc[flights['DEPARTURE_TIME'] == 2400, 'DEPARTURE_TIME'] = 2359\n",
    "\n",
    "\n",
    "\n",
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZraBMwF8Miy"
   },
   "source": [
    "As we can infer from the output of the `decribe()` method, there are no values that are out of range or are non-sense in comparison with other values of the same column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the rows that contain integer values\n",
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: isinstance(x, int))]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: isinstance(x, int))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: str(x).isnumeric())]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: str(x).isnumeric())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZwbFfyp9k-l"
   },
   "source": [
    "### 1.4 Handle categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by looking at our columns to determine what type of data each one of them contains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4Mmph7r97oo",
    "outputId": "06cdb8d8-d82b-4c83-d198-60705beff9ec"
   },
   "outputs": [],
   "source": [
    "#Check nature of the columns \n",
    "for col in flights.columns:\n",
    "  print(col, flights[col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of the nature of each column we must determine which columns we are going to modify. Those columns are going to be the ones containing categorical variables. We are going to modify:\n",
    "- Month\n",
    "- Arrival_delay\n",
    "- Day\n",
    "- Day_of_week\n",
    "- Origin_airport\n",
    "- Destination_airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiHh3pHHfwVV"
   },
   "source": [
    "The next thing we want to do after observing the types of every feature is to transform some variables to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation we have chosen for the MONTH column is grouping the month by quarters. This will allow us to reduce the number of categories from 12 to 3. \n",
    "\n",
    "Additionally, when dividing the year in quarters we get a partition that closely matches the different travel seasons. Summer season matches almost perfectly with the second quarter while the first and third quarter match with winter season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "7qiBl0YzhBmX",
    "outputId": "946e44b0-5f07-4969-a137-28ebc8d1e52b"
   },
   "outputs": [],
   "source": [
    "# MONTH treatment: group months in quarters\n",
    "flights['Q_YEAR'] = flights['MONTH'].apply(lambda x: (x-1)//4 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrival_delay column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column which's data we need to process is the ARRIVAL_DELAY column. This column is the one we want to predict and the aim of our model is to be able to predict if a flight will arrive to its destination with some type of delay. Therefore, we want to convert this column into a column with binary values.\n",
    "\n",
    "We will create a new column in our dataset (i.e. DELAYED) which will have a 1 if the flight is delayed and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELAY TREATMENT\n",
    "flights['DELAYED'] = flights['ARRIVAL_DELAY'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAY column has values that range from 1 to 31, which it's unfeasible to work with. We have decided to divide the month in two fortnights and classify each day with the value of the fortnight they belong to. \n",
    "- From days 1 - 15 they will belong to the first fortnight. They will have a value of 1.\n",
    "- From days 15 - 31 they will belong to the second fortnight. The will have a value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uONZFls9ueiW",
    "outputId": "cadec565-7aa7-405b-8a98-f24f4f4e9988"
   },
   "outputs": [],
   "source": [
    "# DAY treatment \n",
    "\n",
    "flights['FORTNIGHT'] = pd.cut(flights['DAY'], bins=[0, 15, 31], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that this is an appropiate transformation because, usually, people earn they salaries at the end of the month. Therefore, people will have more money available during the first fortnight and this could mean that the number of passengers increase causing to be more flights during the first fortnight. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day_of_week column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same arguments we used for the DAY column apply to DAY_OF_WEEK. The values in this column range from 1 (i.e.: Monday) to 7 (i.e.: Sunday). Instead of working with each of this values, we are going to separate them into working days and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_OF_WEEK 1 is in-week days, 2 is weekend\n",
    "\n",
    "flights['WEEK_INFO'] = pd.cut(flights['DAY_OF_WEEK'], bins=[1, 5, 7], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airline column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to treat the AIRLINES column. Originally, our dataframe had around 15 different airlines and we considered that it was going to be unfeasible to work with all of them. Therefore, we have decided to divide them intro three major groups:\n",
    "- Major airlines\n",
    "- Low cost airlines\n",
    "- Regional airlines\n",
    "\n",
    "We believe that this is a valid division because the nature of the airline could possibly affect the amount of delayed flights they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRLINE\n",
    "\n",
    "# Define the airlines categories\n",
    "major_airlines = ['DL', 'AA', 'UA', 'US', 'AS']\n",
    "low_cost_airlines = ['WN', 'NK', 'F9', 'B6', 'VX']\n",
    "regional_airlines = ['EV', 'OO', 'MQ', 'HA']\n",
    "\n",
    "# create a new column with the airline category\n",
    "flights['AC'] = 'Other'\n",
    "flights.loc[flights['AIRLINE'].isin(major_airlines), 'AC'] = 'Major'\n",
    "flights.loc[flights['AIRLINE'].isin(low_cost_airlines), 'AC'] = 'Low-Cost'\n",
    "flights.loc[flights['AIRLINE'].isin(regional_airlines), 'AC'] = 'Regional'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Origin_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original dataset there are over 300 different airports. Although it would be great to be able to keep all of them as individual categories, we have also considered it unfeasible. So, in order to reduce the number of categories, we are going to divide the USA territory in four quadrants:\n",
    "- Upper left\n",
    "- Upper right\n",
    "- Bottom right\n",
    "- Bottom left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before being able to classify the airports as we just mentioned, we first need to get its coordinates. This can be achieved by extractinc the necessary data (i.e. longitude and latitude) from the *airports* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index of the dataset to the IATA_CODE (code that identifies each airport)\n",
    "airports = airports.set_index('IATA_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LONGITUDE_O column to each row in flights with the longitude of the origin airport\n",
    "flights['LONGITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "\n",
    "# Add LATITUDE_O column to each row in flights with the latitude of the origin airport\n",
    "flights['LATITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have detected that some rows have a *NaN* value in either their longitude or latitude. Since this happens rarely, we can delete those columns without loosing to many rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_O\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false/len(flights[\"LONGITUDE_O\"]))\n",
    "flights = flights.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have removed al conflictive rows, we can proceed with the classification of the airports by its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty column with airport location (i.e.: GR_O)\n",
    "flights['GR_O'] = ''\n",
    "\n",
    "# Classify the airports\n",
    "# Upper right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_RIGHT'\n",
    "\n",
    "# Upper left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_LEFT'\n",
    "\n",
    "# Bottom right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_RIGHT'\n",
    "\n",
    "# Bottom left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_LEFT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check that all airports have been given a location and that there are no empty quadrants, which would indicate that the classification is not correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[\"GR_O\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy8g04lNBpmY"
   },
   "source": [
    "We observe a huge amount of features which may overfit the model in case of using them all. Therefore we will try to choose an optimal subset of explanatory variables that are going to be able to give our model the enough information in order to do good predictions. \n",
    "\n",
    "**Observation: We cannot follow like this since we need to consider some categorical variables. REMEMBER to one_shot_encode them before going on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Destination_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the same transformations we just applied for the destination airport column. We first add the coordinates of the airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['LONGITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "flights['LATITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_D\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false)\n",
    "print(len(flights[\"LONGITUDE_D\"]))\n",
    "\n",
    "flights = flights.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And classify the destination airports based on their coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['GR_D'] = ''\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_LEFT'\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_LEFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[\"GR_D\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of the airports classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our airports classification, we will plot them over a USA map and assing a color to each quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gmplot\n",
    "\n",
    "# # Initialize the map with the first airport as the center point\n",
    "# gmap = gmplot.GoogleMapPlotter(flights.iloc[0]['LATITUDE_O'], flights.iloc[0]['LONGITUDE_O'], zoom=3)\n",
    "\n",
    "# # Define the color mapping for different values of GR_D\n",
    "# color_map = {\n",
    "#     'UPPER_RIGHT': 'red',\n",
    "#     'UPPER_LEFT': 'green',\n",
    "#     'BOTTOM_RIGHT': 'blue',\n",
    "#     'BOTTOM_LEFT': 'purple'\n",
    "# }\n",
    "\n",
    "# # Plot each airport with its corresponding color\n",
    "# for i, row in flights.iterrows():\n",
    "#     color = color_map.get(row['GR_D'], 'gray')\n",
    "#     gmap.marker(row['LATITUDE_O'], row['LONGITUDE_O'], color=color)\n",
    "\n",
    "# # Draw the map and save it to an HTML file\n",
    "# gmap.draw('airports_map.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify scheduled_time and departure_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset has a few columns with values ranging from 0000 to 2359. This values are found in columns scheduled_time and departure_time and they represent an hour and minutes. The format is hhmm (i.e.: 1915 is 19:15).\n",
    "\n",
    "Once again, having 2359 categories is unfeasible. The solution we have come up with is dividing the day between daytime and nightime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to classify the time segment based on the input time\n",
    "def classify_time(time):\n",
    "    sunrise = 600.0   # Define the time of sunrise as 6:00 am (in decimal format)\n",
    "    sunset = 1800.0   # Define the time of sunset as 6:00 pm (in decimal format)\n",
    "    \n",
    "    if time >= sunrise and time < sunset:\n",
    "        return 'Daytime'\n",
    "    else:\n",
    "        return 'Nighttime'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined the function that will allow us to apply the classification, we apply it to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change formatting of SCHEDULED_ARRIVAL and DEPARTURE_TIME\n",
    "flights['ArrivalDayNight'] = flights['SCHEDULED_ARRIVAL'].apply(classify_time)\n",
    "flights['DepartureDayNight'] = flights['DEPARTURE_TIME'].apply(classify_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing and processing with all categorical data, we are left with a few unwanted columns, those ones we have used to create new columns. We will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.describe()\n",
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[['DELAYED','DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE', 'Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O',\n",
    "       'GR_D', 'ArrivalDayNight', 'DepartureDayNight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "In order to do the process correctly, normalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n",
    "\n",
    "This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance. In our case, That is what we are going to observe right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X as the features (input variables)\n",
    "X = flights.drop(\"DELAYED\", axis=1)  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Define y as the target variable\n",
    "y = flights[\"DELAYED\"]  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we observe that the distribution of the features are the same than that of the whole dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have processed all our data, it migh be interesting to, once again, visualize the distribution from some of the new columns we have added to the dataset and study if we can extract any knowledge from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"Q_YEAR\"], \"Distribution of flights by quarter of the year\", \"Quarter\", None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the difference between the three quarters it is not huge, it is clear that, as we expected, the second quarter (i.e.: May to August) is the one with the greater amount of flights as it matches with the summer season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, False)\n",
    "plot_histogram_of_column(X_train[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the difference is minimal. However, if we apply the logarithm to the y axis we can see that the second forntingh accumulates more flights than the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"WEEK_INFO\"], \"Flights distribution by working days and weekends\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it could be expected, weekdays accumulate almost three times more flights than weekends. However, we must keep in mind that the weekday category groups five days while the weekend only groups two days. Probably, the difference wouldn't be this big if we were plotting the average number of flights per day. However, this escapes from the scope of this project as it doesn't provide any additional information that could be helpful to predict flight delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"AC\"], \"Flights distribution by airline category\", \"Airline category\", None, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"GR_O\"], \"Flights distribution by quadrant of origin airport\", \"Quadrant\", None, False,False)\n",
    "\n",
    "plot_histogram_of_column(X_train[\"GR_D\"], \"Flights distribution by quadrant of destination airport\", \"Quadrant\", None, False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the quadrants with the greater amount of flights are *UPPER_RIGHT* and *BOTTOM_RIGT*. This represents the east cost of the USA, where many of the biggest cities are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"DepartureDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)\n",
    "\n",
    "plot_histogram_of_column(X_train[\"ArrivalDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logically, most of the flights departure during daytime and this trend seems to hold for the arrival time. However, if we compare both histograms we will see that the amount of flights departing during daytime is considerably greater than the flights arriving during daytime. This mean that a considerable amount of flights arrive at their destination at night time even though day departed at daytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gausianization of continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models we will apply assume that our data is distributed following a gaussian distribution, we need to make sure that our data is gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we are going to determine which columns of our dataset are the ones containing continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare which variables are categorical and which ones are continuous. \n",
    "\n",
    "continuous = ['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE']\n",
    "\n",
    "X_train[continuous].hist()               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data doesn't follow a gaussian distribution. Before gaussianizing it, we will first remove any outliers that can be identified in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_outliers_zscore(data_series, threshold=3):\n",
    "    \"\"\"\n",
    "    Remove outliers from a pandas Series using the Z-score method.\n",
    "    :param data_series: a pandas Series containing the data\n",
    "    :param threshold: the number of standard deviations from the mean at which to consider a data point an outlier\n",
    "    :return: a new pandas Series with the outliers removed\n",
    "    \"\"\"\n",
    "    z_scores = np.abs((data_series - data_series.mean()) / data_series.std())\n",
    "    return data_series[z_scores <= threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers (no outliers in the whole dataset, no need to modify X_train and X_test)\n",
    "flights_cont = remove_outliers_zscore(flights[continuous])\n",
    "len(flights_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that no outliers are removed. We can now proceed with the normalization of the continuous data defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we normalize continuous variables\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Initialize transformer with number of quantiles and output distribution\n",
    "transformer = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "# Apply transformation to continuous columns\n",
    "for col in continuous:\n",
    "    flights[col] = transformer.fit_transform(flights[col].values.reshape(-1, 1))\n",
    "    X_train[col] = transformer.fit_transform(X_train[col].values.reshape(-1, 1))\n",
    "    X_test[col] = transformer.fit_transform(X_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot the normalized data, which now clearly follow a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[continuous].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing with the continuous variables, we will apply hot encoding to our categorical variables, which are:\n",
    "- Q_year\n",
    "- Fortnight\n",
    "- Week_info\n",
    "- Ac\n",
    "- Gr_o\n",
    "- Gr_d\n",
    "- ArrivalDayNight\n",
    "- DepartureDayNight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for categorical variables\n",
    "categorical = ['Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O','GR_D', 'ArrivalDayNight', 'DepartureDayNight']\n",
    "\n",
    "flights= pd.get_dummies(flights, columns=categorical)\n",
    "X_train = pd.get_dummies(X_train, columns = categorical)\n",
    "X_test = pd.get_dummies(X_test, columns = categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations between pair of variables\n",
    "From a pairplot diagram, we can infer several things:\n",
    "\n",
    "Correlation: The scatterplots in the pairplot diagram give us an idea of the relationship between variables. If the points in a scatterplot are closely clustered around a line or a curve, it suggests a strong correlation between those variables. \n",
    "\n",
    "Distributions: The histograms on the diagonal of the pairplot diagram show the distribution of each variable individually. \n",
    "\n",
    "Outliers: Outliers are data points that significantly deviate from the overall pattern in the scatterplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot_variables = ['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE', 'DELAYED']\n",
    "\n",
    "X_train['DELAYED'] = y_train\n",
    "\n",
    "sns.pairplot(data=X_train[pairplot_variables][:1000], hue='DELAYED')\n",
    "\n",
    "X_train = X_train.drop('DELAYED', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We comment this plot in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection (consider a subset of variables)\n",
    "Nextly we will check if getting a smaller subset of variables we can get better results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature selection model\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(random_state=42), threshold='mean')\n",
    "\n",
    "# Fit the feature selector on the training data\n",
    "feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training and test sets to include only the selected features\n",
    "X_train_selected = feature_selector.transform(X_train)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "\n",
    "# Get the original feature names for the selected features\n",
    "selected_feature_names = X.columns[selected_feature_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the two feature subsets during all the modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf = X_train[selected_feature_names]\n",
    "X_test_sf = X_test[selected_feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly take a look at the difference in columns between both feature subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric for choosing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we are going to decide which of the metrics to use in order to compare our models and decide which of them is the best. We firstly observe the balance between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your target variable is stored in a DataFrame or Series named \"y\"\n",
    "class_counts = y.value_counts()\n",
    "class_balance = class_counts / len(y) * 100\n",
    "\n",
    "print(\"Class Balance:\")\n",
    "print(class_balance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With class 0 at 62.41% and class 1 at 37.59%, there is a noticeable difference in class frequencies, but it does not indicate severe class imbalance. However, it is still worth considering the implications of the imbalance and how it might affect our modeling approach.\n",
    "\n",
    "When dealing with imbalanced data, it is important to be aware that standard evaluation metrics like accuracy may not provide an accurate representation of model performance. Instead, we need to focus on metrics that are more robust to class imbalance, such as precision, recall, F1 score, or area under the ROC curve (AUC-ROC).\n",
    "\n",
    "Nevertheless, since the imbalance is not that big, we will also consider the accuracy score in order to choose our models. **We mainly will take into account the F1-score (taking into account then Recall and precision that will take into account the slightly unbalanced classes that we have) and the accuracy since we don't have that much unbalance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "To start with we quickly remind of some of the options we have to optimize the learning of our models and that we will (more or less) take into account when modelling. \n",
    "\n",
    "**Hyperparameter Tuning**: Fine-tune the hyperparameters of the model to find the optimal combination for our specific problem. We are going to use techniques like grid search, random search, or Bayesian optimization to systematically explore the hyperparameter space and identify the best configuration.\n",
    "\n",
    "**Cross-Validation**: We will use cross-validation techniques, such as k-fold cross-validation, to evaluate our model's performance more reliably. This helps to assess how well our model generalizes to unseen data and reduces the risk of overfitting.\n",
    "\n",
    "**Ensemble Methods**: Explore ensemble methods like bagging, boosting, or stacking to combine multiple models and improve overall performance. Ensemble techniques can help capture different patterns in the data and reduce model variance.\n",
    "\n",
    "**Data Augmentation**: Since we have a lot of rows in our dataset, we will try with a small subset of this dataset and in the case this is not enough, we are going to proceed with picking a bigger subset of rows so that our model is able to sufficiently learn.\n",
    "\n",
    "**Early Stopping**: Implement early stopping during model training to prevent overfitting and find the optimal number of training epochs. Early stopping stops training when the model's performance on a validation set starts to deteriorate.\n",
    "\n",
    "**Model Architecture**: Experiment with different model architectures or network architectures, such as adding or removing layers, adjusting layer sizes, or trying different activation functions.\n",
    "\n",
    "**Monitoring and Debugging**: Monitor your model during training, track performance metrics, and analyze learning curves to identify issues like underfitting, overfitting, or convergence problems. Debug any potential errors, explore misclassified samples, and consider adjusting your model or data accordingly.\n",
    "\n",
    "\n",
    "\n",
    "In the following dataframe we are going to store the metrics for all of the models that we are going to fit in order to compare them. We are going to score metrics for every kind of model fitted with both subset of features and before and after HyperParameter tuning (HT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(index=[], columns= ['Accuracy', 'F1 Macro', 'Precision Macro', 'Recall Macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "First we comment the hyperparameters we are going to tune: \n",
    "\n",
    "C:The C parameter in logistic regression controls the inverse of the regularization strength. It determines the trade-off between fitting the training data and preventing overfitting. Higher values of C result in less regularization, allowing the model to fit the training data more closely.\n",
    "\n",
    "penalty: The penalty parameter specifies the type of regularization used in logistic regression.\n",
    "'l1' penalty corresponds to L1 regularization. It adds a penalty term proportional to the absolute value of the coefficients, promoting sparsity by encouraging some coefficients to become exactly zero.\n",
    "\n",
    "solver: The solver parameter determines the algorithm used to solve the optimization problem in logistic regression.\n",
    "'saga' solver is an extension of the Stochastic Average Gradient (SAG) solver. It supports both L1 and L2 regularization and is suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly fit the models for both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT  0.825189  0.812466   \n",
       "\n",
       "                                           Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT        0.819173     0.807884  "
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation using the Logistic Regression model\n",
    "cv_results_logreg_1 = cross_validate(logreg, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression all features Before HT',:] = [cv_results_logreg_1['test_accuracy'].mean(), cv_results_logreg_1['test_f1_macro'].mean(),cv_results_logreg_1['test_precision_macro'].mean() , cv_results_logreg_1['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  "
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation using the Logistic Regression model\n",
    "cv_results_logreg_2 = cross_validate(logreg, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression subset features Before HT',:] = [cv_results_logreg_2['test_accuracy'].mean(), cv_results_logreg_2['test_f1_macro'].mean(),cv_results_logreg_2['test_precision_macro'].mean() , cv_results_logreg_2['test_recall_macro'].mean()]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning with the small subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "225 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.61446351        nan 0.61446351 0.78591898\n",
      " 0.78591898 0.8098642  0.7861585  0.78591898        nan        nan\n",
      "        nan        nan        nan 0.82542847 0.82542847        nan\n",
      " 0.82542847 0.82542847        nan        nan 0.81034324        nan\n",
      " 0.81944446 0.82351488 0.82351488 0.82088073 0.82351488 0.82351488\n",
      "        nan        nan        nan        nan        nan 0.82542847\n",
      " 0.82542847        nan 0.82542847 0.82542847        nan        nan\n",
      " 0.821599          nan 0.82255565 0.82375326 0.82375326 0.82351345\n",
      " 0.82375326 0.82375326        nan        nan        nan        nan\n",
      "        nan 0.82542847 0.82542847        nan 0.82542847 0.82542847\n",
      "        nan        nan 0.82566828        nan 0.82518953 0.82518895\n",
      " 0.82518895 0.82518895 0.82518895 0.82518895        nan        nan\n",
      "        nan        nan        nan 0.82542847 0.82542847        nan\n",
      " 0.82542847 0.82542847        nan        nan 0.82518924        nan\n",
      " 0.82518924 0.82494943 0.82494943 0.82494943 0.82494943 0.82494943\n",
      "        nan        nan        nan        nan        nan 0.82542847\n",
      " 0.82542847        nan 0.82542847 0.82542847]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "              }\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.82519</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.807537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "Logistic Regression all features after HT       0.82519  0.812316   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  \n",
       "Logistic Regression all features after HT            0.819435     0.807537  "
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model with the best hyperparameters\n",
    "logreg = LogisticRegression(C=1, penalty='l1', solver = 'saga')\n",
    "\n",
    "cv_results_logreg_ht_1 = cross_validate(logreg, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression all features after HT',:] = [cv_results_logreg_ht_1['test_accuracy'].mean(), cv_results_logreg_ht_1['test_f1_macro'].mean(),cv_results_logreg_ht_1['test_precision_macro'].mean() , cv_results_logreg_ht_1['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.82519</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.807537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.823513</td>\n",
       "      <td>0.810733</td>\n",
       "      <td>0.817392</td>\n",
       "      <td>0.806288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "Logistic Regression all features after HT       0.82519  0.812316   \n",
       "Logistic Regression subset features after HT   0.823513  0.810733   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  \n",
       "Logistic Regression all features after HT            0.819435     0.807537  \n",
       "Logistic Regression subset features after HT         0.817392     0.806288  "
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model with the best hyperparameters\n",
    "logreg = LogisticRegression(C=1, penalty='l1', solver = 'saga')\n",
    "\n",
    "cv_results_logreg_ht_2 = cross_validate(logreg, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression subset features after HT',:] = [cv_results_logreg_ht_2['test_accuracy'].mean(), cv_results_logreg_ht_2['test_f1_macro'].mean(),cv_results_logreg_ht_2['test_precision_macro'].mean() , cv_results_logreg_ht_2['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the best of the models that turns to be the logistic regression of all features after HT and observe the sparsity of the coefficients to consider possible modifications on the subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
      "       'DISTANCE', 'Q_YEAR_1', 'Q_YEAR_3', 'FORTNIGHT_1', 'WEEK_INFO_2',\n",
      "       'AC_Major', 'GR_O_BOTTOM_LEFT', 'GR_O_UPPER_RIGHT', 'GR_D_BOTTOM_RIGHT',\n",
      "       'GR_D_UPPER_RIGHT', 'ArrivalDayNight_Daytime',\n",
      "       'DepartureDayNight_Nighttime'],\n",
      "      dtype='object')\n",
      "[[ 2.03393335  1.01124331 -0.06597823 -1.62977287  1.39365111  0.1867674\n",
      "  -0.03381661 -0.03837241 -0.23188308 -0.48826042 -0.1615467  -0.04739157\n",
      "   0.01435584 -0.18122299 -0.28641746 -0.05955507]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Fit the logistic regression model to your training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "X_train_copy = X_train\n",
    "\n",
    "# Get the coefficients of the logistic regression model\n",
    "coefficients = logreg.coef_[0]\n",
    "\n",
    "# Create a mask indicating which coefficients are smaller than 0.01\n",
    "mask = np.abs(coefficients) < 0.01\n",
    "\n",
    "# Iterate until all coefficients are larger than 0.01\n",
    "while np.sum(mask) > 0:\n",
    "    # Get the indices of the features to be eliminated\n",
    "    feature_indices_to_eliminate = np.where(mask)[0]\n",
    "\n",
    "    # Eliminate the features from X_train\n",
    "    X_train_copy = X_train_copy.drop(X_train_copy.columns[feature_indices_to_eliminate], axis=1)\n",
    "\n",
    "    # Fit the logistic regression model to the updated training data\n",
    "    logreg.fit(X_train_copy, y_train)\n",
    "\n",
    "    # Get the updated coefficients\n",
    "    coefficients = logreg.coef_[0]\n",
    "\n",
    "    # Update the mask\n",
    "    mask = np.abs(coefficients) < 0.01\n",
    "\n",
    "# Print the final selected features\n",
    "other_selected_features = X_train_copy.columns\n",
    "print(\"Selected Features:\", other_selected_features)\n",
    "\n",
    "logreg.fit(X_train_copy, y_train)\n",
    "f_coefficients = logreg.coef_\n",
    "print(f_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider this third model using this new subset of features that we found in between and observe if it works out better. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "225 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.61446351        nan 0.61446351 0.78687792\n",
      " 0.78687792 0.81345844 0.78687792 0.78687792        nan        nan\n",
      "        nan        nan        nan 0.82566828 0.82566828        nan\n",
      " 0.82566828 0.82566828        nan        nan 0.81034324        nan\n",
      " 0.81944446 0.82207804 0.82207804 0.82136006 0.82207804 0.82207804\n",
      "        nan        nan        nan        nan        nan 0.82566828\n",
      " 0.82566828        nan 0.82566828 0.82566828        nan        nan\n",
      " 0.821599          nan 0.82231613 0.82495086 0.82495086 0.8242323\n",
      " 0.82495086 0.82495086        nan        nan        nan        nan\n",
      "        nan 0.82566828 0.82566828        nan 0.82566828 0.82566828\n",
      "        nan        nan 0.82614761        nan 0.82638713 0.82686589\n",
      " 0.82686589 0.82662636 0.82686589 0.82686589        nan        nan\n",
      "        nan        nan        nan 0.82566828 0.82566828        nan\n",
      " 0.82566828 0.82566828        nan        nan 0.82518924        nan\n",
      " 0.82566828 0.82542876 0.82542876 0.82542876 0.82542876 0.82542876\n",
      "        nan        nan        nan        nan        nan 0.82566828\n",
      " 0.82566828        nan 0.82566828 0.82566828]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "              }\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_copy, y_train)\n",
    "\n",
    "# Get the best hyperparameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.82519</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.807537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.824471</td>\n",
       "      <td>0.811069</td>\n",
       "      <td>0.819437</td>\n",
       "      <td>0.80568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "Logistic Regression all features after HT       0.82519  0.812316   \n",
       "Logistic Regression subset features after HT   0.824471  0.811069   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  \n",
       "Logistic Regression all features after HT            0.819435     0.807537  \n",
       "Logistic Regression subset features after HT         0.819437      0.80568  "
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model with the best hyperparameters\n",
    "logreg = LogisticRegression(C=0.1, penalty='l2', solver = 'newton-cg')\n",
    "\n",
    "cv_results_logreg_ht_2 = cross_validate(logreg, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression subset features after HT',:] = [cv_results_logreg_ht_2['test_accuracy'].mean(), cv_results_logreg_ht_2['test_f1_macro'].mean(),cv_results_logreg_ht_2['test_precision_macro'].mean() , cv_results_logreg_ht_2['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we observe that this third subset of variables using the sparsity is not better, we will only consider it at the end with the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVC)\n",
    "First of all we recall the hyperparameters we are going to tune:\n",
    "\n",
    "1.C: Regularization parameter. It controls the trade-off between allowing training errors and maximizing the margin. Larger values of C penalize errors more, leading to a potentially narrower margin.\n",
    "\n",
    "2.kernel: Specifies the type of kernel function used for mapping the input data to a higher-dimensional feature space. Common options include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "3.gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels. It defines the influence of training samples on the decision boundary.\n",
    "\n",
    "We fit the SVC using the whole subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.82519</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.807537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.824471</td>\n",
       "      <td>0.811069</td>\n",
       "      <td>0.819437</td>\n",
       "      <td>0.80568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.847218</td>\n",
       "      <td>0.833674</td>\n",
       "      <td>0.849047</td>\n",
       "      <td>0.825115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "Logistic Regression all features after HT       0.82519  0.812316   \n",
       "Logistic Regression subset features after HT   0.824471  0.811069   \n",
       "SVC all features Before HT                     0.847218  0.833674   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  \n",
       "Logistic Regression all features after HT            0.819435     0.807537  \n",
       "Logistic Regression subset features after HT         0.819437      0.80568  \n",
       "SVC all features Before HT                           0.849047     0.825115  "
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only using the best features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.82519</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.807537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.824471</td>\n",
       "      <td>0.811069</td>\n",
       "      <td>0.819437</td>\n",
       "      <td>0.80568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.847218</td>\n",
       "      <td>0.833674</td>\n",
       "      <td>0.849047</td>\n",
       "      <td>0.825115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.855361</td>\n",
       "      <td>0.841582</td>\n",
       "      <td>0.861282</td>\n",
       "      <td>0.831394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "Logistic Regression all features after HT       0.82519  0.812316   \n",
       "Logistic Regression subset features after HT   0.824471  0.811069   \n",
       "SVC all features Before HT                     0.847218  0.833674   \n",
       "SVC subset features Before HT                  0.855361  0.841582   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  \n",
       "Logistic Regression all features after HT            0.819435     0.807537  \n",
       "Logistic Regression subset features after HT         0.819437      0.80568  \n",
       "SVC all features Before HT                           0.849047     0.825115  \n",
       "SVC subset features Before HT                        0.861282     0.831394  "
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have very similar results, let us compute the hyperparameter tuning with the SVC using only the small part of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.8420185635651973\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the SVC model\n",
    "clf = SVC()\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit SVC using the hyperparameters that we found and using both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.82519</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.807537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.824471</td>\n",
       "      <td>0.811069</td>\n",
       "      <td>0.819437</td>\n",
       "      <td>0.80568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.847218</td>\n",
       "      <td>0.833674</td>\n",
       "      <td>0.849047</td>\n",
       "      <td>0.825115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.855361</td>\n",
       "      <td>0.841582</td>\n",
       "      <td>0.861282</td>\n",
       "      <td>0.831394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.847218</td>\n",
       "      <td>0.833674</td>\n",
       "      <td>0.849047</td>\n",
       "      <td>0.825115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "Logistic Regression all features after HT       0.82519  0.812316   \n",
       "Logistic Regression subset features after HT   0.824471  0.811069   \n",
       "SVC all features Before HT                     0.847218  0.833674   \n",
       "SVC subset features Before HT                  0.855361  0.841582   \n",
       "SVC all features After HT                      0.847218  0.833674   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  \n",
       "Logistic Regression all features after HT            0.819435     0.807537  \n",
       "Logistic Regression subset features after HT         0.819437      0.80568  \n",
       "SVC all features Before HT                           0.849047     0.825115  \n",
       "SVC subset features Before HT                        0.861282     0.831394  \n",
       "SVC all features After HT                            0.849047     0.825115  "
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', gamma = 'scale', C=1, coef0 = 0.0, degree = 2)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT      0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT   0.827208  0.811949   \n",
       "Logistic Regression all features after HT       0.827685  0.813427   \n",
       "Logistic Regression subset features after HT    0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT  0.825298  0.811114   \n",
       "SVC all features Before HT                      0.843914  0.828612   \n",
       "SVC subset features Before HT                   0.852983  0.836777   \n",
       "SVC all features After HT                       0.843914  0.828612   \n",
       "SVC subset features after HT                    0.852983  0.836777   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT            0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT         0.821335     0.806075  \n",
       "Logistic Regression all features after HT             0.819975     0.808968  \n",
       "Logistic Regression subset features after HT          0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT        0.816978     0.807055  \n",
       "SVC all features Before HT                            0.842771     0.820213  \n",
       "SVC subset features Before HT                         0.858082     0.825723  \n",
       "SVC all features After HT                             0.842771     0.820213  \n",
       "SVC subset features after HT                          0.858082     0.825723  "
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', gamma = 'scale', C=1)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features after HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now observing that the subset of features is now doing better job than that of all the features surprisingly. The best combination is the last one we fitted.\n",
    "\n",
    "We now check how the model generalizes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "The parameters GaussianNB are the following: Priors: Prior probabilities of the classes. If specified, the priors are not adjusted according to the data. Var_smoothing:Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "\n",
    "However we are only going to tune var_smoothing:\n",
    "\n",
    "To proceed, we are going to fit the Naive Bayes model for both subsets of the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT      0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT   0.827208  0.811949   \n",
       "Logistic Regression all features after HT       0.827685  0.813427   \n",
       "Logistic Regression subset features after HT    0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT  0.825298  0.811114   \n",
       "SVC all features Before HT                      0.843914  0.828612   \n",
       "SVC subset features Before HT                   0.852983  0.836777   \n",
       "SVC all features After HT                       0.843914  0.828612   \n",
       "SVC subset features after HT                    0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT     0.768496  0.750591   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT            0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT         0.821335     0.806075  \n",
       "Logistic Regression all features after HT             0.819975     0.808968  \n",
       "Logistic Regression subset features after HT          0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT        0.816978     0.807055  \n",
       "SVC all features Before HT                            0.842771     0.820213  \n",
       "SVC subset features Before HT                         0.858082     0.825723  \n",
       "SVC all features After HT                             0.842771     0.820213  \n",
       "SVC subset features after HT                          0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT           0.754136     0.748402  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = GaussianNB()\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT      0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT   0.827208  0.811949   \n",
       "Logistic Regression all features after HT       0.827685  0.813427   \n",
       "Logistic Regression subset features after HT    0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT  0.825298  0.811114   \n",
       "SVC all features Before HT                      0.843914  0.828612   \n",
       "SVC subset features Before HT                   0.852983  0.836777   \n",
       "SVC all features After HT                       0.843914  0.828612   \n",
       "SVC subset features after HT                    0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT     0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT  0.829594  0.814404   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT            0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT         0.821335     0.806075  \n",
       "Logistic Regression all features after HT             0.819975     0.808968  \n",
       "Logistic Regression subset features after HT          0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT        0.816978     0.807055  \n",
       "SVC all features Before HT                            0.842771     0.820213  \n",
       "SVC subset features Before HT                         0.858082     0.825723  \n",
       "SVC all features After HT                             0.842771     0.820213  \n",
       "SVC subset features after HT                          0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT           0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT        0.823963     0.808477  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = GaussianNB()\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since reducing features helps the model fit better we recall the other subset found from sparsity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = GaussianNB()\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_copy, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset 2 features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in this case it doesn not work any better we neglect this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute hyperparameter tuning using the smallest subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__var_smoothing': 1e-09}\n",
      "Best Score:  0.8295942720763723\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.25 ,0.3 , 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] \n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the model using the best parameter we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829117</td>\n",
       "      <td>0.811929</td>\n",
       "      <td>0.82673</td>\n",
       "      <td>0.803805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829117  0.811929   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT            0.82673     0.803805  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = GaussianNB(var_smoothing = 1e-09)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  "
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = GaussianNB(var_smoothing = 1e-09)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the best model is by far the one using the subset of features (the other one is probably overfitting) and in this case the hyperparamter tuning hasn't been helpful since the F1 score has decreased and the accuracy has almost remained the same. Therefore we belief that the best model is that of Gaussian Naive Bayes subset features After HT (or before). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "n_components: This hyperparameter specifies the number of components (dimensions) to retain after performing dimensionality reduction with LDA. By default, n_components is set to None, which means it will retain (n_classes - 1) components, where n_classes is the number of distinct classes in the data. In the param_grid, values of [None, 1, 2, 3] are provided to try different numbers of retained components. By setting n_components to a specific value, you can explicitly control the dimensionality of the reduced feature space.\n",
    "\n",
    "shrinkage: Shrinkage is a regularization technique used to improve the estimation of the covariance matrix in LDA, especially when the number of samples is small or the covariance matrix is ill-conditioned. The shrinkage hyperparameter controls the degree of shrinkage applied. \n",
    "In the param_grid, the values [None, 'auto', 0.1, 0.5] are provided to try different levels of shrinkage:\n",
    "None: No shrinkage is applied.\n",
    "'auto': Shrinkage is estimated using the Ledoit-Wolf lemma, which automatically determines the amount of shrinkage based on the data.\n",
    "0.1, 0.5: Specific values between 0 and 1 can be provided to manually set the shrinkage intensity.\n",
    "\n",
    "solver: This hyperparameter specifies the solver used for LDA computation. LDA can be solved using different algorithms, and the solver parameter determines the specific algorithm to use. \n",
    "In the param_grid, the values ['svd', 'lsqr', 'eigen'] are provided to try different solver algorithms:\n",
    "'svd': Singular Value Decomposition (SVD) solver, which computes the exact solution but can be slower for large datasets.\n",
    "'lsqr': Least Squares solver, which can handle both shrinkage and regularized covariance matrix.\n",
    "'eigen': Eigenvalue Decomposition solver, which computes the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "We now fit the Linear Discriminant Analysis classifier for both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  "
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do hyperparameter tuning with the small subset of features dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'n_components': None, 'shrinkage': None, 'solver': 'lsqr', 'tol': 1e-06}\n",
      "Best Score:  0.824343675417661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "800 fits failed out of a total of 1200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 10 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 8 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 15 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 615, in fit\n",
      "    raise NotImplementedError(\"shrinkage not supported with 'svd' solver.\")\n",
      "NotImplementedError: shrinkage not supported with 'svd' solver.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 608, in fit\n",
      "    raise ValueError(\n",
      "ValueError: n_components cannot be larger than min(n_features, n_classes - 1).\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.82386635 0.82386635 0.82386635 0.82386635 0.82386635 0.82434368\n",
      " 0.82434368 0.82434368 0.82434368 0.82434368        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82052506 0.82052506 0.82052506 0.82052506\n",
      " 0.82052506 0.82052506 0.82052506 0.82052506 0.82052506 0.82052506\n",
      "        nan        nan        nan        nan        nan 0.82195704\n",
      " 0.82195704 0.82195704 0.82195704 0.82195704 0.82195704 0.82195704\n",
      " 0.82195704 0.82195704 0.82195704        nan        nan        nan\n",
      "        nan        nan 0.81718377 0.81718377 0.81718377 0.81718377\n",
      " 0.81718377 0.81718377 0.81718377 0.81718377 0.81718377 0.81718377\n",
      " 0.82386635 0.82386635 0.82386635 0.82386635 0.82386635 0.82434368\n",
      " 0.82434368 0.82434368 0.82434368 0.82434368        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82052506 0.82052506 0.82052506 0.82052506\n",
      " 0.82052506 0.82052506 0.82052506 0.82052506 0.82052506 0.82052506\n",
      "        nan        nan        nan        nan        nan 0.82195704\n",
      " 0.82195704 0.82195704 0.82195704 0.82195704 0.82195704 0.82195704\n",
      " 0.82195704 0.82195704 0.82195704        nan        nan        nan\n",
      "        nan        nan 0.81718377 0.81718377 0.81718377 0.81718377\n",
      " 0.81718377 0.81718377 0.81718377 0.81718377 0.81718377 0.81718377\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create an LDA classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_components': [None, 1, 2, 3],    # Values to try for n_components\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5],    # Values to try for shrinkage\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],    # Values to try for solver\n",
    "    'tol': [1e-6,1e-5,1e-4, 1e-3, 1e-2]    # Values to try for tol\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=lda, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'n_components': None, 'shrinkage': None, 'solver': 'lsqr', 'tol': 1e-06}\n",
      "Best Score:  0.824343675417661\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning we fit the LDA with the hyperparameters we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  "
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components = None,shrinkage = None,  solver = 'lsqr', tol = 1e-06)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "\n",
    "We now fit the normal model for the perceptron:\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "penalty: Specifies the penalty term used in the update rule to handle misclassifications. It can be set to 'l1', 'l2', or 'elasticnet'. The default value is 'l2'.\n",
    "\n",
    "alpha: The constant that multiplies the penalty term if regularization is applied. It controls the strength of the regularization. The default value is 0.0001.\n",
    "\n",
    "fit_intercept: Indicates whether an intercept term should be included in the model. If set to True, the perceptron learns an intercept term. The default value is True.\n",
    "\n",
    "max_iter: The maximum number of passes over the training data (epochs) for training the perceptron. The default value is 1000.\n",
    "\n",
    "tol: The tolerance for the stopping criterion. It specifies the minimum change in the average loss for training to continue. The default value is 1e-3.\n",
    "\n",
    "shuffle: Determines whether to shuffle the training data before each epoch during training. The default value is True.\n",
    "\n",
    "eta0: The initial learning rate. It controls the step size at each update during training. The default value is 1.0.\n",
    "\n",
    "early_stopping: If set to True, training will stop when validation loss does not improve anymore. The default value is False.\n",
    "\n",
    "validation_fraction: The proportion of training data to use for early stopping validation. The default value is 0.1.\n",
    "\n",
    "n_iter_no_change: The maximum number of epochs to wait for the validation loss to improve when early_stopping is enabled. The default value is 5.\n",
    "\n",
    "We now fit the models for both subsets of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  "
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  "
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the hyper parameters using the smallest subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'alpha': 0.01, 'eta0': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "Best Score:  0.7937947494033414\n"
     ]
    }
   ],
   "source": [
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization penalty type\n",
    "    'alpha': [0.0001, 0.001, 0.01],         # Regularization parameter\n",
    "    'max_iter': [1000, 2000, 3000],         # Maximum number of iterations\n",
    "    'eta0': [0.1, 0.01, 0.001],             # Initial learning rate\n",
    "    'tol': [10,1,1e-1,1e-2,1e-3]               # Tolerance for stopping criterion\n",
    "}\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=perceptron, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit with the found hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron(alpha = 0.01, eta0=0.1, max_iter=1000, penalty='l1', tol=0.001)\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron(alpha = 0.01, eta0=0.1, max_iter=1000, penalty='l1', tol=0.001)\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we observe that the best model is by far the last we fitted: Perceptron subset features After HT, since it has the best accuracy and f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boost \n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "n_estimators: The number of boosting stages to perform.\n",
    "\n",
    "learning_rate: The learning rate or shrinkage parameter, which controls the contribution of each tree.\n",
    "\n",
    "max_depth: The maximum depth of individual trees in the ensemble.\n",
    "\n",
    "subsample: The subsample ratio of the training instances.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node.\n",
    "\n",
    "We fit the model for both subset of features: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  "
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 3, 'min_samples_split': 3, 'subsample': 0.8}\n",
      "Best F1 Score: 0.8345598372213162\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    #'n_estimators': [50, 100, 200],  # Number of boosting stages to perform\n",
    "    #'learning_rate': [0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [3, 4, 5],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [2, 3, 4]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the left hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 3, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Best F1 Score: 0.8338054772529937\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],  # Number of boosting stages to perform\n",
    "    'learning_rate': [1,0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [4],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [3]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  "
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=50, subsample=0.8)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=50, subsample=0.8)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain slightly better results with the Gradient Boost subset features After HT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Hyperparameters:\n",
    "\n",
    "n_estimators: The number of decision trees in the random forest. Increasing the number of estimators typically improves performance but increases computational complexity. It represents the ensemble size.\n",
    "\n",
    "max_depth: The maximum depth of each decision tree in the forest. Higher values increase model complexity and can lead to overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. Larger values prevent overfitting by requiring a certain number of samples in each split.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. Similar to min_samples_split, larger values help control overfitting by requiring a minimum number of samples in each leaf.\n",
    "\n",
    "max_features: The number of features to consider when looking for the best split. Reducing this number can help control overfitting. Values such as 'sqrt' or 'log2' can be used to consider a square root or logarithm of the total features, respectively.\n",
    "\n",
    "bootstrap: Determines whether bootstrap samples are used when building trees. Setting it to True enables bootstrap sampling, while False disables it. Bootstrap sampling introduces randomness into the training process and helps improve model diversity.\n",
    "\n",
    "criterion: The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We have an additional parameter (not to tune) : criterion: The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We first fit the normal model with both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.826409</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.817306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "Random Forest subset features Before HT           0.842482  0.826409   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  \n",
       "Random Forest subset features Before HT                 0.842672     0.817306  "
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to do hyperparameter tuning with the small subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 15, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    #'n_estimators': [100, 200, 500],\n",
    "    #'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 15, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.826409</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.817306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.851551</td>\n",
       "      <td>0.834517</td>\n",
       "      <td>0.85828</td>\n",
       "      <td>0.822803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "Random Forest subset features Before HT           0.842482  0.826409   \n",
       "Random Forest all features After HT               0.851551  0.834517   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  \n",
       "Random Forest subset features Before HT                 0.842672     0.817306  \n",
       "Random Forest all features After HT                      0.85828     0.822803  "
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='entropy', max_features='log2', min_samples_leaf=4, min_samples_split=15,random_state=42)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.826409</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.817306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.851551</td>\n",
       "      <td>0.834517</td>\n",
       "      <td>0.85828</td>\n",
       "      <td>0.822803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.843437</td>\n",
       "      <td>0.827419</td>\n",
       "      <td>0.843711</td>\n",
       "      <td>0.818313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "Random Forest subset features Before HT           0.842482  0.826409   \n",
       "Random Forest all features After HT               0.851551  0.834517   \n",
       "Random Forest subset features After HT            0.843437  0.827419   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  \n",
       "Random Forest subset features Before HT                 0.842672     0.817306  \n",
       "Random Forest all features After HT                      0.85828     0.822803  \n",
       "Random Forest subset features After HT                  0.843711     0.818313  "
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='gini', max_depth=None, max_features='log2', min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=42)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Nets\n",
    "Hyperparameters: \n",
    "\n",
    "Learning rate: The learning rate determines how much the weights are updated in response to the estimated error each time the model weights are updated. Choosing the right learning rate can be crucial as a value too small may result in a long training process that could get stuck, while a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "Optimizer: what type of optimizer you want to use\n",
    "\n",
    "Batch size: This is the number of samples to work through before updating the internal model parameters. A smaller batch size can lead to more updates and potentially faster convergence, but it also introduces more variance, which can lead to instability in the learning process.\n",
    "\n",
    "Activation function: Different activation functions can result in significant differences in the performance of a neural network. The Rectified Linear Unit (ReLU) and its variants (like Leaky ReLU, Parametric ReLU) are often a good starting point for many problems.\n",
    "\n",
    "We now do some neural nets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in /Users/marcamps/env/lib/python3.8/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: packaging>=0.21 in /Users/marcamps/env/lib/python3.8/site-packages (from scikeras) (23.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/marcamps/env/lib/python3.8/site-packages (from scikeras) (1.2.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/marcamps/env/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/marcamps/env/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/marcamps/.local/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/marcamps/.local/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.24.2)\r\n"
     ]
    }
   ],
   "source": [
    "# install needed packages for following modeling\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network I \n",
    "Fit neural network with all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/105 [..............................] - ETA: 25s - loss: 0.6994 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:03:48.019535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 1s 8ms/step - loss: 0.4931 - accuracy: 0.7754\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3968 - accuracy: 0.8314\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3702 - accuracy: 0.8446\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3503 - accuracy: 0.8524\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3404 - accuracy: 0.8560\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3320 - accuracy: 0.8608\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8614\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3201 - accuracy: 0.8638\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3139 - accuracy: 0.8683\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3090 - accuracy: 0.8754\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:03:53.827683: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 18:03:54.047020: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 1s 8ms/step - loss: 0.4830 - accuracy: 0.7788\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3894 - accuracy: 0.8333\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3590 - accuracy: 0.8471\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3457 - accuracy: 0.8554\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8599\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8677\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8650\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3110 - accuracy: 0.8719\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8701\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8806\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:03:59.889637: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 18:04:00.102695: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 1s 8ms/step - loss: 0.5068 - accuracy: 0.7689\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.4018 - accuracy: 0.8315\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3731 - accuracy: 0.8366\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3506 - accuracy: 0.8477\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3381 - accuracy: 0.8617\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8614\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8602\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8617\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8671\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3106 - accuracy: 0.8725\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:04:05.837008: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/105 [..............................] - ETA: 30s - loss: 0.6857 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:04:06.049799: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 1s 8ms/step - loss: 0.4893 - accuracy: 0.7797\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3787 - accuracy: 0.8408\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3567 - accuracy: 0.8560\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3397 - accuracy: 0.8605\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8668\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8620\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8671\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3147 - accuracy: 0.8701\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3081 - accuracy: 0.8749\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3023 - accuracy: 0.8764\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:04:11.782510: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/105 [..............................] - ETA: 1:09 - loss: 0.7173 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:04:12.404108: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 1s 7ms/step - loss: 0.4826 - accuracy: 0.7800\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3925 - accuracy: 0.8306\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3642 - accuracy: 0.8405\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3482 - accuracy: 0.8521\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8638\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8668\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3175 - accuracy: 0.8656\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3131 - accuracy: 0.8683\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8716\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 0s 5ms/step - loss: 0.3007 - accuracy: 0.8767\n",
      "27/27 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 18:04:18.141806: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with the subset of variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 5/53 [=>............................] - ETA: 0s - loss: 0.6595 - accuracy: 0.7125 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:04:57.713624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 8ms/step - loss: 0.5134 - accuracy: 0.7989\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4068 - accuracy: 0.8222\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3840 - accuracy: 0.8300\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3667 - accuracy: 0.8413\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3573 - accuracy: 0.8508\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3482 - accuracy: 0.8520\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3414 - accuracy: 0.8580\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3382 - accuracy: 0.8646\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3356 - accuracy: 0.8574\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8604\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:05:00.769357: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:05:00.965464: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 8ms/step - loss: 0.5520 - accuracy: 0.7625\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4125 - accuracy: 0.8174\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3831 - accuracy: 0.8347\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3659 - accuracy: 0.8479\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3575 - accuracy: 0.8556\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3477 - accuracy: 0.8556\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3436 - accuracy: 0.8604\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3419 - accuracy: 0.8544\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3369 - accuracy: 0.8640\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3345 - accuracy: 0.8675\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:05:04.012860: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:05:04.197124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 8ms/step - loss: 0.5595 - accuracy: 0.7607\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4150 - accuracy: 0.8270\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3841 - accuracy: 0.8371\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3680 - accuracy: 0.8419\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3543 - accuracy: 0.8520\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3449 - accuracy: 0.8610\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8604\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.8652\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8568\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8646\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:05:07.223844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:05:07.409463: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 8ms/step - loss: 0.5474 - accuracy: 0.7673\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4104 - accuracy: 0.8162\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3837 - accuracy: 0.8305\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3706 - accuracy: 0.8425\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3606 - accuracy: 0.8449\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3522 - accuracy: 0.8538\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3456 - accuracy: 0.8568\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3426 - accuracy: 0.8610\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3403 - accuracy: 0.8580\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8616\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:05:10.418036: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:05:10.602636: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 8ms/step - loss: 0.5437 - accuracy: 0.7530\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4023 - accuracy: 0.8300\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3764 - accuracy: 0.8431\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3589 - accuracy: 0.8508\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3473 - accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3442 - accuracy: 0.8598\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3349 - accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8628\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8663\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8652\n",
      "14/14 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:05:13.611648: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.807884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.824231</td>\n",
       "      <td>0.811369</td>\n",
       "      <td>0.81836</td>\n",
       "      <td>0.806757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.82519</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.807537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.824471</td>\n",
       "      <td>0.811069</td>\n",
       "      <td>0.819437</td>\n",
       "      <td>0.80568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.847218</td>\n",
       "      <td>0.833674</td>\n",
       "      <td>0.849047</td>\n",
       "      <td>0.825115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.855361</td>\n",
       "      <td>0.841582</td>\n",
       "      <td>0.861282</td>\n",
       "      <td>0.831394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.847218</td>\n",
       "      <td>0.833674</td>\n",
       "      <td>0.849047</td>\n",
       "      <td>0.825115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I all features before HT</th>\n",
       "      <td>0.848895</td>\n",
       "      <td>0.836013</td>\n",
       "      <td>0.849592</td>\n",
       "      <td>0.828449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT     0.825189  0.812466   \n",
       "Logistic Regression subset features Before HT  0.824231  0.811369   \n",
       "Logistic Regression all features after HT       0.82519  0.812316   \n",
       "Logistic Regression subset features after HT   0.824471  0.811069   \n",
       "SVC all features Before HT                     0.847218  0.833674   \n",
       "SVC subset features Before HT                  0.855361  0.841582   \n",
       "SVC all features After HT                      0.847218  0.833674   \n",
       "Neural Network I all features before HT        0.848895  0.836013   \n",
       "\n",
       "                                              Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT           0.819173     0.807884  \n",
       "Logistic Regression subset features Before HT         0.81836     0.806757  \n",
       "Logistic Regression all features after HT            0.819435     0.807537  \n",
       "Logistic Regression subset features after HT         0.819437      0.80568  \n",
       "SVC all features Before HT                           0.849047     0.825115  \n",
       "SVC subset features Before HT                        0.861282     0.831394  \n",
       "SVC all features After HT                            0.849047     0.825115  \n",
       "Neural Network I all features before HT              0.849592     0.828449  "
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/53 [====>.........................] - ETA: 0s - loss: 0.6384 - accuracy: 0.6938 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 12:42:07.459679: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 6ms/step - loss: 0.5408 - accuracy: 0.7834\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4142 - accuracy: 0.8240\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3837 - accuracy: 0.8305\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3687 - accuracy: 0.8371\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3547 - accuracy: 0.8496\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3475 - accuracy: 0.8580\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3400 - accuracy: 0.8580\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3384 - accuracy: 0.8646\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8669\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8610\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8628\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.8658\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3180 - accuracy: 0.8640\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8622\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3194 - accuracy: 0.8658\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.8663\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8693\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8681\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8693\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8652\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3120 - accuracy: 0.8735\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3152 - accuracy: 0.8693\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8681\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3110 - accuracy: 0.8717\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3081 - accuracy: 0.8735\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3104 - accuracy: 0.8753\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.8771\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.8699\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3100 - accuracy: 0.8723\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.8765\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3053 - accuracy: 0.8729\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8681\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3067 - accuracy: 0.8735\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3045 - accuracy: 0.8747\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3031 - accuracy: 0.8777\n",
      "Epoch 37/50\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.3072 - accuracy: 0.8714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64, 96],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network II\n",
    "We firstly fit the neural network with both subsests of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 1/53 [..............................] - ETA: 14s - loss: 0.7084 - accuracy: 0.4062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:10:54.470629: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5848 - accuracy: 0.6981\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4537 - accuracy: 0.8150\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4049 - accuracy: 0.8371\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3817 - accuracy: 0.8395\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3666 - accuracy: 0.8479\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3547 - accuracy: 0.8550\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3447 - accuracy: 0.8490\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3316 - accuracy: 0.8610\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8753\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8717\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8759\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8741\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2983 - accuracy: 0.8825\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.8801\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2881 - accuracy: 0.8801\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.8872\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2798 - accuracy: 0.8878\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2747 - accuracy: 0.8896\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2689 - accuracy: 0.8902\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2715 - accuracy: 0.8920\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2621 - accuracy: 0.8950\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2597 - accuracy: 0.8938\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2527 - accuracy: 0.8974\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2493 - accuracy: 0.9021\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2495 - accuracy: 0.8956\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2412 - accuracy: 0.9075\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2350 - accuracy: 0.9075\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2367 - accuracy: 0.9039\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2317 - accuracy: 0.9105\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2297 - accuracy: 0.9075\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:11:02.806927: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:11:03.002502: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5686 - accuracy: 0.7226\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4504 - accuracy: 0.8174\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4027 - accuracy: 0.8288\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3804 - accuracy: 0.8490\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3600 - accuracy: 0.8526\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.8544\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3378 - accuracy: 0.8652\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8681\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3220 - accuracy: 0.8675\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8741\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8753\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.8705\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3017 - accuracy: 0.8771\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2960 - accuracy: 0.8789\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2903 - accuracy: 0.8765\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2893 - accuracy: 0.8842\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.8848\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2775 - accuracy: 0.8908\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2743 - accuracy: 0.8872\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2698 - accuracy: 0.8914\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2643 - accuracy: 0.8896\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.8920\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2573 - accuracy: 0.8956\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2532 - accuracy: 0.9004\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2476 - accuracy: 0.9051\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2451 - accuracy: 0.9016\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2417 - accuracy: 0.9010\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2366 - accuracy: 0.9039\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2321 - accuracy: 0.9039\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2308 - accuracy: 0.9039\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:11:11.284420: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:11:11.475522: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5623 - accuracy: 0.7255\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4169 - accuracy: 0.8305\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3800 - accuracy: 0.8443\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3634 - accuracy: 0.8514\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3531 - accuracy: 0.8574\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3386 - accuracy: 0.8622\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8652\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3209 - accuracy: 0.8675\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8717\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3088 - accuracy: 0.8699\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3016 - accuracy: 0.8819\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2960 - accuracy: 0.8819\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2921 - accuracy: 0.8807\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2867 - accuracy: 0.8866\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2801 - accuracy: 0.8878\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2775 - accuracy: 0.8914\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2724 - accuracy: 0.8866\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8938\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.8908\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2609 - accuracy: 0.8974\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2563 - accuracy: 0.8968\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2545 - accuracy: 0.8974\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2494 - accuracy: 0.8992\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2447 - accuracy: 0.9027\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2423 - accuracy: 0.8998\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2383 - accuracy: 0.9075\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2395 - accuracy: 0.9057\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2270 - accuracy: 0.9111\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2276 - accuracy: 0.9051\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2186 - accuracy: 0.9141\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:11:20.014551: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:11:20.206538: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 10ms/step - loss: 0.5200 - accuracy: 0.7631\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4221 - accuracy: 0.8162\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3914 - accuracy: 0.8294\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3792 - accuracy: 0.8383\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3618 - accuracy: 0.8526\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3556 - accuracy: 0.8484\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3440 - accuracy: 0.8496\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3395 - accuracy: 0.8580\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8580\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8693\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8729\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3129 - accuracy: 0.8741\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3043 - accuracy: 0.8747\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3002 - accuracy: 0.8753\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8842\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2899 - accuracy: 0.8801\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2887 - accuracy: 0.8801\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2835 - accuracy: 0.8890\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2798 - accuracy: 0.8896\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.8866\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.8944\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.8938\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2626 - accuracy: 0.8950\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2586 - accuracy: 0.9021\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2550 - accuracy: 0.8950\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2550 - accuracy: 0.9051\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2512 - accuracy: 0.9016\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2461 - accuracy: 0.9051\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2411 - accuracy: 0.9093\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2389 - accuracy: 0.9081\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:11:28.504743: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:11:28.696484: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 10ms/step - loss: 0.5949 - accuracy: 0.6927\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4499 - accuracy: 0.8115\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3959 - accuracy: 0.8347\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3745 - accuracy: 0.8431\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3616 - accuracy: 0.8520\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3437 - accuracy: 0.8628\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3388 - accuracy: 0.8598\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8646\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3162 - accuracy: 0.8646\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8652\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3052 - accuracy: 0.8765\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2970 - accuracy: 0.8717\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2934 - accuracy: 0.8765\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2902 - accuracy: 0.8795\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2860 - accuracy: 0.8789\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2788 - accuracy: 0.8890\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.8825\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2730 - accuracy: 0.8920\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2709 - accuracy: 0.8890\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8956\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2607 - accuracy: 0.8998\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2584 - accuracy: 0.8980\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2521 - accuracy: 0.8998\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2500 - accuracy: 0.9057\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2413 - accuracy: 0.9051\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2399 - accuracy: 0.9087\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2360 - accuracy: 0.9081\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2344 - accuracy: 0.9033\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2301 - accuracy: 0.9093\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2264 - accuracy: 0.9141\n",
      "14/14 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:11:36.784251: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with small subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/27 [>.............................] - ETA: 7s - loss: 0.7365 - accuracy: 0.3281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:12:10.568910: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 15ms/step - loss: 0.6493 - accuracy: 0.6295\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.5257 - accuracy: 0.7995\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.4480 - accuracy: 0.8126\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4136 - accuracy: 0.8186\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3959 - accuracy: 0.8288\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3859 - accuracy: 0.8329\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3764 - accuracy: 0.8359\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3684 - accuracy: 0.8371\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3618 - accuracy: 0.8449\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3588 - accuracy: 0.8437\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3524 - accuracy: 0.8490\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3499 - accuracy: 0.8502\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3464 - accuracy: 0.8562\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3422 - accuracy: 0.8556\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3388 - accuracy: 0.8622\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3363 - accuracy: 0.8622\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3342 - accuracy: 0.8640\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3324 - accuracy: 0.8628\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8634\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3301 - accuracy: 0.8652\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8628\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3287 - accuracy: 0.8663\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8634\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8628\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.8681\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.8634\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3209 - accuracy: 0.8675\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3214 - accuracy: 0.8675\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3202 - accuracy: 0.8687\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8669\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8693\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.8669\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3180 - accuracy: 0.8693\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8669\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3193 - accuracy: 0.8646\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8687\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8699\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8699\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8705\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8687\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8717\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3114 - accuracy: 0.8729\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8675\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8723\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8729\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3121 - accuracy: 0.8693\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.8687\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8681\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.8741\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3105 - accuracy: 0.8747\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:12:18.252330: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:12:18.437322: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 14ms/step - loss: 0.6320 - accuracy: 0.6492\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.5034 - accuracy: 0.7864\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.4482 - accuracy: 0.7983\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.4225 - accuracy: 0.8198\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4061 - accuracy: 0.8252\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3947 - accuracy: 0.8252\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3841 - accuracy: 0.8323\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3752 - accuracy: 0.8353\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3691 - accuracy: 0.8461\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3640 - accuracy: 0.8419\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3575 - accuracy: 0.8508\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3520 - accuracy: 0.8550\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3490 - accuracy: 0.8586\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3452 - accuracy: 0.8556\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3428 - accuracy: 0.8550\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3420 - accuracy: 0.8610\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3403 - accuracy: 0.8568\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3380 - accuracy: 0.8556\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3386 - accuracy: 0.8598\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8592\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3319 - accuracy: 0.8616\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3319 - accuracy: 0.8586\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8586\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8634\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8628\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3291 - accuracy: 0.8634\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8652\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3260 - accuracy: 0.8646\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8640\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3244 - accuracy: 0.8646\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8652\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8646\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8622\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8681\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8675\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8652\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8634\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3233 - accuracy: 0.8675\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3203 - accuracy: 0.8652\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8669\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8669\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3209 - accuracy: 0.8663\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8675\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8687\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8723\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8622\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3179 - accuracy: 0.8669\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3171 - accuracy: 0.8687\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3159 - accuracy: 0.8675\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8681\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:12:25.902911: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:12:26.090499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 11ms/step - loss: 0.6005 - accuracy: 0.7488\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.4792 - accuracy: 0.8085\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4281 - accuracy: 0.8150\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4068 - accuracy: 0.8210\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.8276\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3834 - accuracy: 0.8341\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3741 - accuracy: 0.8431\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3659 - accuracy: 0.8449\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3607 - accuracy: 0.8479\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3518 - accuracy: 0.8514\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3467 - accuracy: 0.8574\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3443 - accuracy: 0.8568\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3431 - accuracy: 0.8544\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3374 - accuracy: 0.8616\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3335 - accuracy: 0.8658\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8634\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8646\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8646\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8675\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3235 - accuracy: 0.8687\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8658\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8699\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8669\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8652\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8675\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8652\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8687\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8705\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3120 - accuracy: 0.8663\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8693\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3098 - accuracy: 0.8699\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3092 - accuracy: 0.8741\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3076 - accuracy: 0.8729\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3078 - accuracy: 0.8711\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3112 - accuracy: 0.8711\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3114 - accuracy: 0.8705\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3055 - accuracy: 0.8741\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3050 - accuracy: 0.8711\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3075 - accuracy: 0.8711\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3068 - accuracy: 0.8741\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3046 - accuracy: 0.8747\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8729\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3030 - accuracy: 0.8735\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3017 - accuracy: 0.8783\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3016 - accuracy: 0.8729\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3009 - accuracy: 0.8723\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.2999 - accuracy: 0.8777\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3014 - accuracy: 0.8789\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.2999 - accuracy: 0.8705\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3008 - accuracy: 0.8747\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:12:33.371780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:12:33.552968: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 13ms/step - loss: 0.6020 - accuracy: 0.7357\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4827 - accuracy: 0.8037\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8168\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4022 - accuracy: 0.8216\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3876 - accuracy: 0.8300\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3782 - accuracy: 0.8323\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3705 - accuracy: 0.8377\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3649 - accuracy: 0.8431\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3593 - accuracy: 0.8425\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3537 - accuracy: 0.8496\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3503 - accuracy: 0.8538\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3459 - accuracy: 0.8550\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3438 - accuracy: 0.8592\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3419 - accuracy: 0.8586\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3381 - accuracy: 0.8598\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8604\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3345 - accuracy: 0.8652\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8634\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.8610\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8610\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8663\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8610\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8652\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3233 - accuracy: 0.8693\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.8663\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8628\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3239 - accuracy: 0.8652\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8669\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8711\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.8693\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8646\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3194 - accuracy: 0.8687\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8675\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.8681\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8675\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3160 - accuracy: 0.8693\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3178 - accuracy: 0.8669\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3163 - accuracy: 0.8634\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3157 - accuracy: 0.8687\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8705\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8687\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8687\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3119 - accuracy: 0.8681\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3100 - accuracy: 0.8687\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8735\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3112 - accuracy: 0.8723\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3105 - accuracy: 0.8699\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3104 - accuracy: 0.8717\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.8729\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3085 - accuracy: 0.8705\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:12:40.872102: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:12:41.064493: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 13ms/step - loss: 0.6166 - accuracy: 0.7440\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4923 - accuracy: 0.8132\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.4237 - accuracy: 0.8216\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3968 - accuracy: 0.8329\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3827 - accuracy: 0.8347\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3739 - accuracy: 0.8431\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3673 - accuracy: 0.8479\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3620 - accuracy: 0.8479\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3565 - accuracy: 0.8532\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3506 - accuracy: 0.8556\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3467 - accuracy: 0.8568\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3441 - accuracy: 0.8598\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3405 - accuracy: 0.8580\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3360 - accuracy: 0.8604\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3344 - accuracy: 0.8622\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3315 - accuracy: 0.8658\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8658\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8699\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8634\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8681\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8652\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3233 - accuracy: 0.8658\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8681\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8699\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8693\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8717\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3163 - accuracy: 0.8699\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8735\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8753\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8681\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8693\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8723\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8717\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3110 - accuracy: 0.8741\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3118 - accuracy: 0.8747\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3100 - accuracy: 0.8765\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3106 - accuracy: 0.8723\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3078 - accuracy: 0.8753\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3093 - accuracy: 0.8765\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3091 - accuracy: 0.8729\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3062 - accuracy: 0.8765\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3053 - accuracy: 0.8771\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3048 - accuracy: 0.8795\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3038 - accuracy: 0.8765\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3043 - accuracy: 0.8741\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3036 - accuracy: 0.8783\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3024 - accuracy: 0.8789\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3012 - accuracy: 0.8795\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3016 - accuracy: 0.8825\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.3022 - accuracy: 0.8783\n",
      "7/7 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:12:48.467223: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:18:08.425784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 12ms/step - loss: 0.4709 - accuracy: 0.7861\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3751 - accuracy: 0.8440\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3542 - accuracy: 0.8495\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3437 - accuracy: 0.8556\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3381 - accuracy: 0.8574\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8595\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8611\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3267 - accuracy: 0.8634\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8655\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8659\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8673\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3155 - accuracy: 0.8673\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8687\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3106 - accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8714\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3076 - accuracy: 0.8699\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8712\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8739\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3034 - accuracy: 0.8735\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8730\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8750\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3009 - accuracy: 0.8751\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3013 - accuracy: 0.8740\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2966 - accuracy: 0.8762\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8750\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8757\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8768\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8768\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2923 - accuracy: 0.8785\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2905 - accuracy: 0.8800\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8818\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2890 - accuracy: 0.8820\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2880 - accuracy: 0.8804\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8813\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8830\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2850 - accuracy: 0.8839\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8827\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8839\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2812 - accuracy: 0.8845\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2810 - accuracy: 0.8841\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8847\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8861\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2758 - accuracy: 0.8880\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2763 - accuracy: 0.8868\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8891\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8887\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8911\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:18:43.288388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:18:43.570372: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4784 - accuracy: 0.7791\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3854 - accuracy: 0.8347\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3628 - accuracy: 0.8453\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8541\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8543\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3383 - accuracy: 0.8608\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8602\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8632\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8637\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8645\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8683\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8688\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8699\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3151 - accuracy: 0.8662\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8714\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3102 - accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3101 - accuracy: 0.8680\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3068 - accuracy: 0.8715\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3052 - accuracy: 0.8702\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8735\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3041 - accuracy: 0.8740\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8740\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8742\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2968 - accuracy: 0.8750\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2942 - accuracy: 0.8795\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8760\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2915 - accuracy: 0.8788\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2911 - accuracy: 0.8771\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8787\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8787\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2885 - accuracy: 0.8769\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8799\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2815 - accuracy: 0.8806\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8799\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8816\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8826\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8829\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8854\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8837\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2734 - accuracy: 0.8826\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2720 - accuracy: 0.8858\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8857\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8866\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.8862\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8867\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8878\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8879\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8852\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8885\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8899\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:19:17.923543: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:19:18.214063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.4750 - accuracy: 0.7877\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3872 - accuracy: 0.8341\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3640 - accuracy: 0.8464\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3530 - accuracy: 0.8503\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3462 - accuracy: 0.8546\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3405 - accuracy: 0.8568\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8580\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8609\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8609\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8633\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8653\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8649\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8671\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8658\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8681\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3146 - accuracy: 0.8693\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8701\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3085 - accuracy: 0.8726\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8744\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8748\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8751\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8740\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8762\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2997 - accuracy: 0.8776\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2987 - accuracy: 0.8763\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8789\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8768\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2931 - accuracy: 0.8799\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2900 - accuracy: 0.8793\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2896 - accuracy: 0.8793\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2886 - accuracy: 0.8795\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2881 - accuracy: 0.8794\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8835\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8819\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2841 - accuracy: 0.8833\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8808\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2814 - accuracy: 0.8810\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8814\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2780 - accuracy: 0.8832\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8848\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8856\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8848\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2736 - accuracy: 0.8854\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8880\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8897\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8878\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8875\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8886\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8923\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:19:52.460622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:19:52.729206: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4850 - accuracy: 0.7750\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3803 - accuracy: 0.8388\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3575 - accuracy: 0.8497\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3429 - accuracy: 0.8551\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8587\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8609\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8665\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8662\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3153 - accuracy: 0.8671\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3131 - accuracy: 0.8705\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3115 - accuracy: 0.8695\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3099 - accuracy: 0.8724\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8700\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8744\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3020 - accuracy: 0.8745\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3008 - accuracy: 0.8745\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2975 - accuracy: 0.8767\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2982 - accuracy: 0.8749\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2943 - accuracy: 0.8783\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8774\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8795\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2886 - accuracy: 0.8796\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8796\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8805\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8820\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8826\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8825\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8850\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8854\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8858\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2739 - accuracy: 0.8870\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.8864\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8868\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8893\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8876\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8911\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8903\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8916\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8914\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8932\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8922\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8955\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8932\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8951\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8961\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8961\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8943\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8988\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8984\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:20:27.099271: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:20:27.374418: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4757 - accuracy: 0.7844\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3759 - accuracy: 0.8409\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3578 - accuracy: 0.8499\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3460 - accuracy: 0.8555\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3403 - accuracy: 0.8603\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3363 - accuracy: 0.8619\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8614\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8628\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8651\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8689\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3204 - accuracy: 0.8652\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8679\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8694\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3150 - accuracy: 0.8689\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3122 - accuracy: 0.8704\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3111 - accuracy: 0.8712\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8720\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3078 - accuracy: 0.8715\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3064 - accuracy: 0.8733\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3028 - accuracy: 0.8761\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3038 - accuracy: 0.8738\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8727\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3004 - accuracy: 0.8743\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2990 - accuracy: 0.8724\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8752\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2971 - accuracy: 0.8754\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8781\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8779\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2943 - accuracy: 0.8767\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2904 - accuracy: 0.8763\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8776\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2891 - accuracy: 0.8835\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8814\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2876 - accuracy: 0.8804\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8818\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8802\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2854 - accuracy: 0.8805\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8838\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2825 - accuracy: 0.8817\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8827\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2786 - accuracy: 0.8826\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8835\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2779 - accuracy: 0.8839\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8843\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8862\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8856\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.8869\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8866\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8882\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Mean Accuracy: 0.8451858913250714\n",
      "Mean F1 Macro: 0.8309404592135102\n",
      "Mean Precision Macro: 0.8417203807511993\n",
      "Mean Recall Macro: 0.8249394258137215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:21:02.000896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features after HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.826409</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.817306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.851551</td>\n",
       "      <td>0.834517</td>\n",
       "      <td>0.85828</td>\n",
       "      <td>0.822803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.843437</td>\n",
       "      <td>0.827419</td>\n",
       "      <td>0.843711</td>\n",
       "      <td>0.818313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I all features before HT</th>\n",
       "      <td>0.851074</td>\n",
       "      <td>0.836683</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.828712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset features before HT</th>\n",
       "      <td>0.860143</td>\n",
       "      <td>0.845134</td>\n",
       "      <td>0.86563</td>\n",
       "      <td>0.834484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II all features before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.829382</td>\n",
       "      <td>0.834267</td>\n",
       "      <td>0.826339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II subset features before HT</th>\n",
       "      <td>0.859189</td>\n",
       "      <td>0.845056</td>\n",
       "      <td>0.861443</td>\n",
       "      <td>0.835724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "Random Forest subset features Before HT           0.842482  0.826409   \n",
       "Random Forest all features After HT               0.851551  0.834517   \n",
       "Random Forest subset features After HT            0.843437  0.827419   \n",
       "Neural Network I all features before HT           0.851074  0.836683   \n",
       "Neural Network I subset features before HT        0.860143  0.845134   \n",
       "Neural Network II all features before HT          0.841527  0.829382   \n",
       "Neural Network II subset features before HT       0.859189  0.845056   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  \n",
       "Random Forest subset features Before HT                 0.842672     0.817306  \n",
       "Random Forest all features After HT                      0.85828     0.822803  \n",
       "Random Forest subset features After HT                  0.843711     0.818313  \n",
       "Neural Network I all features before HT                 0.851035     0.828712  \n",
       "Neural Network I subset features before HT               0.86563     0.834484  \n",
       "Neural Network II all features before HT                0.834267     0.826339  \n",
       "Neural Network II subset features before HT             0.861443     0.835724  "
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we cannot do hyperparameter tuning because of the vast quantity of time needed (no time left enough), we are going to reconsider the other subset of features obtained from the sparsity of logistic regression and evaluate it on both neural networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_copy.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/53 [..............................] - ETA: 15s - loss: 0.7001 - accuracy: 0.5938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:15:47.076331: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5208 - accuracy: 0.7763\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4047 - accuracy: 0.8335\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3720 - accuracy: 0.8496\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3533 - accuracy: 0.8556\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8574\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3359 - accuracy: 0.8580\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8693\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3209 - accuracy: 0.8681\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3079 - accuracy: 0.8771\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3041 - accuracy: 0.8807\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:15:50.145495: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/53 [..............................] - ETA: 14s - loss: 0.6811 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:15:50.365693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5253 - accuracy: 0.7703\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4055 - accuracy: 0.8288\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3818 - accuracy: 0.8371\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3594 - accuracy: 0.8496\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3478 - accuracy: 0.8490\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.8586\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8628\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8687\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3161 - accuracy: 0.8663\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3073 - accuracy: 0.8693\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:15:53.432160: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:15:53.623505: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5384 - accuracy: 0.7321\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4020 - accuracy: 0.8311\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3743 - accuracy: 0.8455\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3563 - accuracy: 0.8437\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3451 - accuracy: 0.8544\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8610\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8669\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3163 - accuracy: 0.8735\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3104 - accuracy: 0.8741\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3021 - accuracy: 0.8795\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:15:56.674135: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:15:56.877753: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5590 - accuracy: 0.7434\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4025 - accuracy: 0.8282\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3752 - accuracy: 0.8413\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3606 - accuracy: 0.8490\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3494 - accuracy: 0.8502\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3394 - accuracy: 0.8490\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8556\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8658\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3220 - accuracy: 0.8699\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8723\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:16:00.007799: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:16:00.199690: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5243 - accuracy: 0.7494\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3988 - accuracy: 0.8317\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3726 - accuracy: 0.8437\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3588 - accuracy: 0.8484\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3421 - accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3343 - accuracy: 0.8580\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.8628\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3185 - accuracy: 0.8640\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3157 - accuracy: 0.8669\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.8723\n",
      "14/14 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:16:03.370697: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_copy, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I subset 2 features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.826409</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.817306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.851551</td>\n",
       "      <td>0.834517</td>\n",
       "      <td>0.85828</td>\n",
       "      <td>0.822803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.843437</td>\n",
       "      <td>0.827419</td>\n",
       "      <td>0.843711</td>\n",
       "      <td>0.818313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I all features before HT</th>\n",
       "      <td>0.851074</td>\n",
       "      <td>0.836683</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.828712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset features before HT</th>\n",
       "      <td>0.860143</td>\n",
       "      <td>0.845134</td>\n",
       "      <td>0.86563</td>\n",
       "      <td>0.834484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II all features before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.829382</td>\n",
       "      <td>0.834267</td>\n",
       "      <td>0.826339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II subset features before HT</th>\n",
       "      <td>0.859189</td>\n",
       "      <td>0.845056</td>\n",
       "      <td>0.861443</td>\n",
       "      <td>0.835724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset 2 features before HT</th>\n",
       "      <td>0.848687</td>\n",
       "      <td>0.835267</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.828578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "Random Forest subset features Before HT           0.842482  0.826409   \n",
       "Random Forest all features After HT               0.851551  0.834517   \n",
       "Random Forest subset features After HT            0.843437  0.827419   \n",
       "Neural Network I all features before HT           0.851074  0.836683   \n",
       "Neural Network I subset features before HT        0.860143  0.845134   \n",
       "Neural Network II all features before HT          0.841527  0.829382   \n",
       "Neural Network II subset features before HT       0.859189  0.845056   \n",
       "Neural Network I subset 2 features before HT      0.848687  0.835267   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  \n",
       "Random Forest subset features Before HT                 0.842672     0.817306  \n",
       "Random Forest all features After HT                      0.85828     0.822803  \n",
       "Random Forest subset features After HT                  0.843711     0.818313  \n",
       "Neural Network I all features before HT                 0.851035     0.828712  \n",
       "Neural Network I subset features before HT               0.86563     0.834484  \n",
       "Neural Network II all features before HT                0.834267     0.826339  \n",
       "Neural Network II subset features before HT             0.861443     0.835724  \n",
       "Neural Network I subset 2 features before HT            0.845886     0.828578  "
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_copy.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:02.790277: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 10ms/step - loss: 0.5558 - accuracy: 0.7363\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4326 - accuracy: 0.8150\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3927 - accuracy: 0.8353\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3763 - accuracy: 0.8419\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3603 - accuracy: 0.8484\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3499 - accuracy: 0.8496\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3419 - accuracy: 0.8604\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3330 - accuracy: 0.8604\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8652\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8705\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3171 - accuracy: 0.8675\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8705\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3071 - accuracy: 0.8723\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3056 - accuracy: 0.8795\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3016 - accuracy: 0.8729\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3023 - accuracy: 0.8759\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2964 - accuracy: 0.8801\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2926 - accuracy: 0.8765\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2887 - accuracy: 0.8801\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2883 - accuracy: 0.8795\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2847 - accuracy: 0.8908\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2798 - accuracy: 0.8848\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.8914\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2744 - accuracy: 0.8884\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2726 - accuracy: 0.8890\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2698 - accuracy: 0.8914\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.8920\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8914\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2613 - accuracy: 0.8956\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2585 - accuracy: 0.8956\n",
      " 1/14 [=>............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:11.289716: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 53ms/step\n",
      "Epoch 1/30\n",
      " 1/53 [..............................] - ETA: 14s - loss: 0.8151 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:12.144403: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.6223 - accuracy: 0.6521\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4778 - accuracy: 0.8204\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4146 - accuracy: 0.8311\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3895 - accuracy: 0.8305\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3745 - accuracy: 0.8401\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3608 - accuracy: 0.8508\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3498 - accuracy: 0.8532\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3406 - accuracy: 0.8538\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3359 - accuracy: 0.8598\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8574\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3240 - accuracy: 0.8681\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8652\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3161 - accuracy: 0.8723\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8735\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3109 - accuracy: 0.8783\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3081 - accuracy: 0.8753\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3044 - accuracy: 0.8759\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3043 - accuracy: 0.8777\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2996 - accuracy: 0.8771\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2977 - accuracy: 0.8795\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2947 - accuracy: 0.8848\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2901 - accuracy: 0.8872\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2900 - accuracy: 0.8848\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2868 - accuracy: 0.8842\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2850 - accuracy: 0.8860\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2824 - accuracy: 0.8866\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2787 - accuracy: 0.8896\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8872\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2766 - accuracy: 0.8896\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.8860\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:20.445263: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/53 [..............................] - ETA: 15s - loss: 0.7573 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:20.658933: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.6029 - accuracy: 0.6927\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4429 - accuracy: 0.8192\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3927 - accuracy: 0.8371\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3697 - accuracy: 0.8389\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3537 - accuracy: 0.8437\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3426 - accuracy: 0.8646\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8574\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8687\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3221 - accuracy: 0.8663\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8699\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3106 - accuracy: 0.8723\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3064 - accuracy: 0.8729\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3024 - accuracy: 0.8729\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3008 - accuracy: 0.8753\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2952 - accuracy: 0.8777\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.8807\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2902 - accuracy: 0.8783\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.8801\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.8831\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2797 - accuracy: 0.8837\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2789 - accuracy: 0.8908\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2760 - accuracy: 0.8837\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2711 - accuracy: 0.8878\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2701 - accuracy: 0.8884\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.8884\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.8950\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8950\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2580 - accuracy: 0.8956\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2587 - accuracy: 0.8956\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2540 - accuracy: 0.8950\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:28.892745: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:17:29.087607: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5978 - accuracy: 0.6778\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4514 - accuracy: 0.8049\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4020 - accuracy: 0.8222\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3851 - accuracy: 0.8305\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3719 - accuracy: 0.8395\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3644 - accuracy: 0.8431\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3539 - accuracy: 0.8514\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3444 - accuracy: 0.8532\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3374 - accuracy: 0.8610\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8622\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8705\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.8729\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8705\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8735\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3051 - accuracy: 0.8771\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3017 - accuracy: 0.8753\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2963 - accuracy: 0.8807\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2956 - accuracy: 0.8789\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2910 - accuracy: 0.8819\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2887 - accuracy: 0.8848\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2870 - accuracy: 0.8813\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2818 - accuracy: 0.8872\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2790 - accuracy: 0.8872\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2770 - accuracy: 0.8848\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.8914\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2693 - accuracy: 0.8890\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2660 - accuracy: 0.8926\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2660 - accuracy: 0.8884\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8956\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2604 - accuracy: 0.8944\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:37.383795: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-25 17:17:37.576149: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 9ms/step - loss: 0.5671 - accuracy: 0.7303\n",
      "Epoch 2/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4271 - accuracy: 0.8240\n",
      "Epoch 3/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.8252\n",
      "Epoch 4/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3704 - accuracy: 0.8401\n",
      "Epoch 5/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3568 - accuracy: 0.8443\n",
      "Epoch 6/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8484\n",
      "Epoch 7/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3417 - accuracy: 0.8568\n",
      "Epoch 8/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8592\n",
      "Epoch 9/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8550\n",
      "Epoch 10/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3244 - accuracy: 0.8640\n",
      "Epoch 11/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8675\n",
      "Epoch 12/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8669\n",
      "Epoch 13/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8681\n",
      "Epoch 14/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3094 - accuracy: 0.8729\n",
      "Epoch 15/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3021 - accuracy: 0.8741\n",
      "Epoch 16/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8753\n",
      "Epoch 17/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3013 - accuracy: 0.8783\n",
      "Epoch 18/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.8813\n",
      "Epoch 19/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2918 - accuracy: 0.8825\n",
      "Epoch 20/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2892 - accuracy: 0.8777\n",
      "Epoch 21/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.8848\n",
      "Epoch 22/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2822 - accuracy: 0.8842\n",
      "Epoch 23/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2815 - accuracy: 0.8860\n",
      "Epoch 24/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2783 - accuracy: 0.8831\n",
      "Epoch 25/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2728 - accuracy: 0.8837\n",
      "Epoch 26/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2730 - accuracy: 0.8878\n",
      "Epoch 27/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.8878\n",
      "Epoch 28/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2673 - accuracy: 0.8842\n",
      "Epoch 29/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2632 - accuracy: 0.8872\n",
      "Epoch 30/30\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8914\n",
      "14/14 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:17:45.856161: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_copy, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II subset 2 features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.826409</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.817306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.851551</td>\n",
       "      <td>0.834517</td>\n",
       "      <td>0.85828</td>\n",
       "      <td>0.822803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.843437</td>\n",
       "      <td>0.827419</td>\n",
       "      <td>0.843711</td>\n",
       "      <td>0.818313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I all features before HT</th>\n",
       "      <td>0.851074</td>\n",
       "      <td>0.836683</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.828712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset features before HT</th>\n",
       "      <td>0.860143</td>\n",
       "      <td>0.845134</td>\n",
       "      <td>0.86563</td>\n",
       "      <td>0.834484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II all features before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.829382</td>\n",
       "      <td>0.834267</td>\n",
       "      <td>0.826339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II subset features before HT</th>\n",
       "      <td>0.859189</td>\n",
       "      <td>0.845056</td>\n",
       "      <td>0.861443</td>\n",
       "      <td>0.835724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset 2 features before HT</th>\n",
       "      <td>0.848687</td>\n",
       "      <td>0.835267</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.828578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II subset 2 features before HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.831673</td>\n",
       "      <td>0.844929</td>\n",
       "      <td>0.824333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "Random Forest subset features Before HT           0.842482  0.826409   \n",
       "Random Forest all features After HT               0.851551  0.834517   \n",
       "Random Forest subset features After HT            0.843437  0.827419   \n",
       "Neural Network I all features before HT           0.851074  0.836683   \n",
       "Neural Network I subset features before HT        0.860143  0.845134   \n",
       "Neural Network II all features before HT          0.841527  0.829382   \n",
       "Neural Network II subset features before HT       0.859189  0.845056   \n",
       "Neural Network I subset 2 features before HT      0.848687  0.835267   \n",
       "Neural Network II subset 2 features before HT     0.846301  0.831673   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  \n",
       "Random Forest subset features Before HT                 0.842672     0.817306  \n",
       "Random Forest all features After HT                      0.85828     0.822803  \n",
       "Random Forest subset features After HT                  0.843711     0.818313  \n",
       "Neural Network I all features before HT                 0.851035     0.828712  \n",
       "Neural Network I subset features before HT               0.86563     0.834484  \n",
       "Neural Network II all features before HT                0.834267     0.826339  \n",
       "Neural Network II subset features before HT             0.861443     0.835724  \n",
       "Neural Network I subset 2 features before HT            0.845886     0.828578  \n",
       "Neural Network II subset 2 features before HT           0.844929     0.824333  "
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset features before HT</th>\n",
       "      <td>0.860143</td>\n",
       "      <td>0.845134</td>\n",
       "      <td>0.86563</td>\n",
       "      <td>0.834484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II subset features before HT</th>\n",
       "      <td>0.859189</td>\n",
       "      <td>0.845056</td>\n",
       "      <td>0.861443</td>\n",
       "      <td>0.835724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.852983</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.858082</td>\n",
       "      <td>0.825723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I all features before HT</th>\n",
       "      <td>0.851074</td>\n",
       "      <td>0.836683</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.828712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset 2 features before HT</th>\n",
       "      <td>0.848687</td>\n",
       "      <td>0.835267</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.828578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.851551</td>\n",
       "      <td>0.834517</td>\n",
       "      <td>0.85828</td>\n",
       "      <td>0.822803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.831969</td>\n",
       "      <td>0.847551</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II subset 2 features before HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.831673</td>\n",
       "      <td>0.844929</td>\n",
       "      <td>0.824333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.846301</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.821887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network II all features before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.829382</td>\n",
       "      <td>0.834267</td>\n",
       "      <td>0.826339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>0.845014</td>\n",
       "      <td>0.820229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.842771</td>\n",
       "      <td>0.820213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.844496</td>\n",
       "      <td>0.818714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.843437</td>\n",
       "      <td>0.827419</td>\n",
       "      <td>0.843711</td>\n",
       "      <td>0.818313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.826409</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.817306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.841527</td>\n",
       "      <td>0.825406</td>\n",
       "      <td>0.840897</td>\n",
       "      <td>0.816554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.814404</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.808477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features after HT</th>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.819975</td>\n",
       "      <td>0.808968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.821335</td>\n",
       "      <td>0.806075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features Before HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.827208</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset 2 features after HT</th>\n",
       "      <td>0.825298</td>\n",
       "      <td>0.811114</td>\n",
       "      <td>0.816978</td>\n",
       "      <td>0.807055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression subset features after HT</th>\n",
       "      <td>0.826253</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.804055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression all features Before HT</th>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.810444</td>\n",
       "      <td>0.816632</td>\n",
       "      <td>0.806168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.81607</td>\n",
       "      <td>0.805537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.823866</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>0.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.793795</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.799129</td>\n",
       "      <td>0.780304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset 2 features Before HT</th>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.765911</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.754136</td>\n",
       "      <td>0.748402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.753222</td>\n",
       "      <td>0.741948</td>\n",
       "      <td>0.751317</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.770406</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.726954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.720764</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.735047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Accuracy  F1 Macro  \\\n",
       "Neural Network I subset features before HT        0.860143  0.845134   \n",
       "Neural Network II subset features before HT       0.859189  0.845056   \n",
       "SVC subset features after HT                      0.852983  0.836777   \n",
       "SVC subset features Before HT                     0.852983  0.836777   \n",
       "Neural Network I all features before HT           0.851074  0.836683   \n",
       "Neural Network I subset 2 features before HT      0.848687  0.835267   \n",
       "Random Forest all features After HT               0.851551  0.834517   \n",
       "Gradient Boost subset features After HT           0.847255  0.831969   \n",
       "Neural Network II subset 2 features before HT     0.846301  0.831673   \n",
       "Gradient Boost all features After HT              0.846301  0.830796   \n",
       "Neural Network II all features before HT          0.841527  0.829382   \n",
       "Gradient Boost all features Before HT             0.844869  0.829132   \n",
       "SVC all features After HT                         0.843914  0.828612   \n",
       "SVC all features Before HT                        0.843914  0.828612   \n",
       "Gradient Boost subset features Before HT          0.843914  0.827903   \n",
       "Random Forest subset features After HT            0.843437  0.827419   \n",
       "Random Forest subset features Before HT           0.842482  0.826409   \n",
       "Random Forest all features Before HT              0.841527  0.825406   \n",
       "Gaussian Naive Bayes subset features After HT     0.829594  0.814404   \n",
       "Gaussian Naive Bayes subset features Before HT    0.829594  0.814404   \n",
       "Logistic Regression all features after HT         0.827685  0.813427   \n",
       "Logistic Regression subset features Before HT     0.827208  0.811949   \n",
       "LDA subset features Before HT                     0.827208  0.811632   \n",
       "LDA subset features After HT                      0.827208  0.811632   \n",
       "Logistic Regression subset 2 features after HT    0.825298  0.811114   \n",
       "Logistic Regression subset features after HT      0.826253  0.810464   \n",
       "Logistic Regression all features Before HT        0.824821  0.810444   \n",
       "LDA all features After HT                         0.824344  0.809855   \n",
       "LDA all features Before HT                        0.823866  0.809278   \n",
       "Perceptron subset features After HT               0.793795  0.779462   \n",
       "Gaussian Naive Bayes subset 2 features Before HT  0.778998  0.761343   \n",
       "Gaussian Naive Bayes all features After HT        0.768496  0.750591   \n",
       "Gaussian Naive Bayes all features Before HT       0.768496  0.750591   \n",
       "Perceptron subset features Before HT              0.753222  0.741948   \n",
       "Perceptron all features After HT                  0.770406  0.727697   \n",
       "Perceptron all features Before HT                 0.720764  0.710684   \n",
       "\n",
       "                                                 Precision Macro Recall Macro  \n",
       "Neural Network I subset features before HT               0.86563     0.834484  \n",
       "Neural Network II subset features before HT             0.861443     0.835724  \n",
       "SVC subset features after HT                            0.858082     0.825723  \n",
       "SVC subset features Before HT                           0.858082     0.825723  \n",
       "Neural Network I all features before HT                 0.851035     0.828712  \n",
       "Neural Network I subset 2 features before HT            0.845886     0.828578  \n",
       "Random Forest all features After HT                      0.85828     0.822803  \n",
       "Gradient Boost subset features After HT                 0.847551     0.823165  \n",
       "Neural Network II subset 2 features before HT           0.844929     0.824333  \n",
       "Gradient Boost all features After HT                    0.846391     0.821887  \n",
       "Neural Network II all features before HT                0.834267     0.826339  \n",
       "Gradient Boost all features Before HT                   0.845014     0.820229  \n",
       "SVC all features After HT                               0.842771     0.820213  \n",
       "SVC all features Before HT                              0.842771     0.820213  \n",
       "Gradient Boost subset features Before HT                0.844496     0.818714  \n",
       "Random Forest subset features After HT                  0.843711     0.818313  \n",
       "Random Forest subset features Before HT                 0.842672     0.817306  \n",
       "Random Forest all features Before HT                    0.840897     0.816554  \n",
       "Gaussian Naive Bayes subset features After HT           0.823963     0.808477  \n",
       "Gaussian Naive Bayes subset features Before HT          0.823963     0.808477  \n",
       "Logistic Regression all features after HT               0.819975     0.808968  \n",
       "Logistic Regression subset features Before HT           0.821335     0.806075  \n",
       "LDA subset features Before HT                           0.821979     0.805316  \n",
       "LDA subset features After HT                            0.821979     0.805316  \n",
       "Logistic Regression subset 2 features after HT          0.816978     0.807055  \n",
       "Logistic Regression subset features after HT            0.820737     0.804055  \n",
       "Logistic Regression all features Before HT              0.816632     0.806168  \n",
       "LDA all features After HT                                0.81607     0.805537  \n",
       "LDA all features Before HT                              0.815635       0.8049  \n",
       "Perceptron subset features After HT                     0.799129     0.780304  \n",
       "Gaussian Naive Bayes subset 2 features Before HT        0.765911     0.758596  \n",
       "Gaussian Naive Bayes all features After HT              0.754136     0.748402  \n",
       "Gaussian Naive Bayes all features Before HT             0.754136     0.748402  \n",
       "Perceptron subset features Before HT                    0.751317     0.751276  \n",
       "Perceptron all features After HT                        0.797468     0.726954  \n",
       "Perceptron all features Before HT                       0.743656     0.735047  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the DataFrame by the 'F1 Macro' column in descending order\n",
    "sorted_df = results_df.sort_values(by='F1 Macro', ascending=False)\n",
    "\n",
    "sorted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_sf\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define a function to create your neural network model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_sf' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:21:40.567986: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 1s 8ms/step - loss: 0.5027 - accuracy: 0.7819\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 0s 6ms/step - loss: 0.3991 - accuracy: 0.8263\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3739 - accuracy: 0.8406\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3590 - accuracy: 0.8530\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3498 - accuracy: 0.8558\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3420 - accuracy: 0.8616\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8606\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3342 - accuracy: 0.8601\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8635\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8692\n",
      "17/17 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:21:44.459126: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "keras_classifier.fit(X_train_sf, y_train)\n",
    "\n",
    "# Predict using the trained model on the test data\n",
    "y_pred = keras_classifier.predict(X_test[selected_feature_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8683206106870229\n",
      "Precision: 0.8520408163265306\n",
      "Recall: 0.8067632850241546\n",
      "F1 Score: 0.8287841191066998\n"
     ]
    }
   ],
   "source": [
    "# Obtain the predicted probabilities from the model\n",
    "y_pred_prob = keras_classifier.predict_proba(X_test[selected_feature_names])\n",
    "\n",
    "# Set the decision threshold to 0.45\n",
    "threshold = 0.45\n",
    "\n",
    "# Apply the threshold to the predicted probabilities\n",
    "y_pred = (y_pred_prob[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(true, pred):\n",
    "    \"\"\"\n",
    "    Function for pretty printing confusion matrices\n",
    "    \"\"\"\n",
    "    true.name = 'target'\n",
    "    pred.name = 'predicted'\n",
    "    cm = pd.crosstab(true.reset_index(drop=True), pred.reset_index(drop=True))\n",
    "    cm = cm[cm.index]\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test, pd.Series(y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
