{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X1a2rkiAgHQ"
   },
   "source": [
    "#Â **AA1 Project** \n",
    "\n",
    "## Modeling delays in the air\n",
    "\n",
    "In order to predict whether a flight is likely to be delayed or not, create a ML model that will make predictions with that aim. \n",
    "\n",
    "First of all. Let's observe our data set and some first insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WFwwT0G--Vb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will be working with the *flights* dataset downloaded from *Kaggle*. However, there is some information from the airports that appear in the dataset that is stored in the *airports* file so we will also need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4naEd9v_gWI",
    "outputId": "f9312b58-6257-40ee-aac4-dd514d576d82"
   },
   "outputs": [],
   "source": [
    "airports = pd.read_csv('airports.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the amount of data in the *flights* file, we will be using only a random subset of it so we are able to get good results while avoid slowing the execution of the project to unacceptable and unfeasible levels. We will be working with 0.1% of the original datai, i.e.: XXXXX rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv('flights.csv')\n",
    "flights = flights.sample(frac=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsI8BfIVAdal"
   },
   "source": [
    "As we can se with the `describe` method, we can observe the range where each feature takes values at more or less. It is also interesting to observe the histograms of the different columns to see how each feature is distributed more or less, to get an intuition of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "WMan5A-f_l5x",
    "outputId": "5e0b5489-d212-4929-cf12-18f77639fec4"
   },
   "outputs": [],
   "source": [
    "flights.describe()\n",
    "airports.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the histograms, we can appreciate that some of the variables in our dataset present variables where the data appears to be quite concentrated and this could be explained by the presence of large outliers. Some other variables seem to be more uniformally distributed. However, the nature of the distribution that describes the data in our variables shouldn't worry ourselve to much right now as many data processment will be done (including gaussianization of the continuos variables and hot encoding of the categorical ones) before trainging and testing a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "owSaZFCZ3-V2"
   },
   "source": [
    "## 1. Data cleaning\n",
    "### Nextly we will portray the schedule that we are going to follow. \n",
    "1. **Check for duplicates:** Check for and remove any duplicate rows in our dataset.\n",
    "\n",
    "2. **Handle missing data:** Identify any missing data and decide how to handle it. We will either remove the rows or fill in the missing data.\n",
    "\n",
    "3. **Check for inconsistent data:** Check for any inconsistent or erroneous data, such as values that are out of range or inconsistent with other data in the same row.\n",
    "\n",
    "4. **Handle categorical data:** If you have categorical data, decide how to handle it. One common approach is to use one-hot encoding.\n",
    "\n",
    "5. **Normalize data:** Normalize the data so that the features have similar ranges. This will prevent features with large ranges from dominating the model.\n",
    "\n",
    "6. **Feature selection:** Select the most relevant features for your model. You can use various techniques such as correlation analysis or principal component analysis (PCA).\n",
    "\n",
    "7. **Train-test split:** Finally, split your data into training and testing sets to evaluate the performance of your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9qgdnb740x-"
   },
   "source": [
    "### 1.1 Check for duplicates\n",
    "We firstly see that there are no duplicate samples with the following piece of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSrzDv455MWf",
    "outputId": "ffa33860-4374-4db7-ea5a-98cca5564064"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = flights.duplicated()\n",
    "\n",
    "# Print the duplicate rows\n",
    "len(flights[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeODOfOB5pNx"
   },
   "source": [
    "### 1.2 Handle missing data\n",
    "In this section we have a bunch of different possibilities in order to approach the problem of missing data. \n",
    "1. Remove missing data: if the quantity of missing data is not that significative, maybe it is a good option to consider removing all those samples that contain `NaN` values since the subset of samples that is going to be deleted may not be significant while training the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W0tqcTO6hYt",
    "outputId": "9afbcca0-8c45-4053-d09b-209047d794f2"
   },
   "outputs": [],
   "source": [
    "# Count the number of NaN values in each column\n",
    "nan_counts = flights.isna().sum()\n",
    "# Print the results\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwRj6Uih68dv"
   },
   "source": [
    "From the previous output we see that the last 6 features are almost useless since the majority of the samples do not have any information of those, therefore we are going to `drop`. A part from that, we also need to delete columns that regard information that is not going to be available (a posteriori information from the flight) such as information of the time elapsed during time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g28GIbss7vHi"
   },
   "outputs": [],
   "source": [
    "cols_of_interest = ['ARRIVAL_DELAY','MONTH', 'DAY' ,'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_ARRIVAL', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME', 'DISTANCE']\n",
    "flights = flights[cols_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "jczIDNTZOJpP",
    "outputId": "2b732e2a-8915-470d-c24e-922daa387018"
   },
   "outputs": [],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jV9TeZHT83xS"
   },
   "source": [
    "Once we've deleted those columns we can say that **maybe** and only **maybe** taking into account that in the original dataset we have more than 5M samples, deleting the other samples that contain at least one `NaN` value, may be reasonable. \n",
    "\n",
    "**CHECK WHETHER THIS IS REASONABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e82megy49YCh",
    "outputId": "c945bf9d-9c1f-4cd4-ad6b-2a5638b92934"
   },
   "outputs": [],
   "source": [
    "l_bef = len(flights)\n",
    "flights = flights.dropna(how='any')\n",
    "l_aft = len(flights)\n",
    "print(l_bef, l_aft)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ0KSQzZ-D3g"
   },
   "source": [
    "In the original dataset, we pass from $5819079$ to $5714008$ samples. In other words, we keep the $98.2\\%$ of the samples, so it may be a good option to work with these new subset of samples that still contain a vast quantity of information.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LO35Qxc-pB3"
   },
   "source": [
    "### 1.3 Check for inconsistent data\n",
    "In order to do so, we firstly observe an overview of our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "c3TuA6Cx_x5G",
    "outputId": "8b9139e8-da40-4805-efdf-01dec510440f"
   },
   "outputs": [],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will plot a few histograms of the variables in our dataset to get a deeper understanding of their nature. First of all we are going to implement a function that is going to allow us to plot the histogram and personalize a few parameters such as the title, labels for the axis and the options to apply logarithms to any of the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_of_column(data, title, x_title, y_title, x_log, y_log):\n",
    "    # Assign default title if none is provided\n",
    "    if not title:\n",
    "        title = \"Histogram\"\n",
    "\n",
    "    # Assign default title to x axis if none is provided\n",
    "    if not x_title:\n",
    "        x_title = \"Value\"\n",
    "    \n",
    "    # Assing default title to y axis if none is provided\n",
    "    if not y_title:\n",
    "        y_title = \"Count\"\n",
    "    \n",
    "    # Apply logarithm to x axis if required\n",
    "    if x_log:\n",
    "        # Set up histogram with logarithmic x-axis\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data, bins=10**np.linspace(np.log10(0.1), np.log10(data.max()), 50))\n",
    "\n",
    "        # Set x-axis to logarithmic scale\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "        # Add \"log\" to x axis title\n",
    "        x_title = \"Log \" + x_title\n",
    "    else:\n",
    "        # Create plot with histogram of DAY column\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data)\n",
    "\n",
    "\n",
    "    # Apply logarithm to y-axis if required\n",
    "    if y_log:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        # Add \"log\" to y axis title\n",
    "        y_title = \"Log \" + y_title\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a function that allows us to easily plot any column from the dataset that we are interested in, we will start by studying the nature of the columns that refer to delays on the flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the arrival_delay column\n",
    "plot_histogram_of_column(flights[\"ARRIVAL_DELAY\"], \"Distribution of arrival delay\", None, None, True, False)\n",
    "\n",
    "# Histogram of DEPARTURE_DELAY column\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_DELAY\"], \"Distribution of departure delay\", None, None, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that both histograms share a similar shape, which means that these two variables are strongly correlated. However, we expected to get this result since those flights that suffer departure with any form of delay are extremely likely to also arrive with a similar delay.\n",
    "\n",
    "Another interesting result extracted from the data is that there are quite a lot of flights that depart before their scheduled departure time. Moreover, if we compare closely both histograms, we will notice that the amount of flights that arrive before their arrival time is smaller than the number of flights that departure ahead of their departure time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to analyse how the variables containing data related with the date of the flight are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(flights[\"MONTH\"], \"Distribution of month\", None, None, False, False)\n",
    "\n",
    "# Create plot with histogram of DAY column\n",
    "plot_histogram_of_column(flights[\"DAY\"], \"Distribution of day\", None, None, False, True)\n",
    "\n",
    "# Histogram of DAY_OF_WEEK colum\n",
    "plot_histogram_of_column(flights[\"DAY_OF_WEEK\"], \"Distribution of day of week\", None, None, False, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we'll plot some other columns that could be interesting to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(flights[\"DISTANCE\"], \"Distribution of distance\", \"Distance\", None, False, True)\n",
    "\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_TIME\"], \"Distribution of departure time\", \"Time\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distance plot, we observe that the distance of the vast majority of flights range between a few hundred and a three thousand miles. The longest flights in the dataset are around 5000 miles. \n",
    "\n",
    "On the other hand, looking at the distribution of the departure time variabel, we see that fligh departure are almos uniformly distributed from 6 am to 8 pm. From 9 pm to 5 am, a much smaller number of departure take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWIhprxNkiCK",
    "outputId": "43978d06-cd15-4dda-b194-33eb6e73d502"
   },
   "outputs": [],
   "source": [
    "print(flights[\"DESTINATION_AIRPORT\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.loc[flights['WHEELS_OFF'] == 2400, 'WHEELS_OFF'] = 2359\n",
    "flights.loc[flights['DEPARTURE_TIME'] == 2400, 'DEPARTURE_TIME'] = 2359\n",
    "\n",
    "\n",
    "\n",
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZraBMwF8Miy"
   },
   "source": [
    "As we can infer from the output of the `decribe()` method, there are no values that are out of range or are non-sense in comparison with other values of the same column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the rows that contain integer values\n",
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: isinstance(x, int))]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: isinstance(x, int))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: str(x).isnumeric())]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: str(x).isnumeric())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZwbFfyp9k-l"
   },
   "source": [
    "### 1.4 Handle categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by looking at our columns to determine what type of data each one of them contains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4Mmph7r97oo",
    "outputId": "06cdb8d8-d82b-4c83-d198-60705beff9ec"
   },
   "outputs": [],
   "source": [
    "#Check nature of the columns \n",
    "for col in flights.columns:\n",
    "  print(col, flights[col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of the nature of each column we must determine which columns we are going to modify. Those columns are going to be the ones containing categorical variables. We are going to modify:\n",
    "- Month\n",
    "- Arrival_delay\n",
    "- Day\n",
    "- Day_of_week\n",
    "- Origin_airport\n",
    "- Destination_airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiHh3pHHfwVV"
   },
   "source": [
    "The next thing we want to do after observing the types of every feature is to transform some variables to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation we have chosen for the MONTH column is grouping the month by quarters. This will allow us to reduce the number of categories from 12 to 3. \n",
    "\n",
    "Additionally, when dividing the year in quarters we get a partition that closely matches the different travel seasons. Summer season matches almost perfectly with the second quarter while the first and third quarter match with winter season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "7qiBl0YzhBmX",
    "outputId": "946e44b0-5f07-4969-a137-28ebc8d1e52b"
   },
   "outputs": [],
   "source": [
    "# MONTH treatment: group months in quarters\n",
    "flights['Q_YEAR'] = flights['MONTH'].apply(lambda x: (x-1)//4 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrival_delay column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column which's data we need to process is the ARRIVAL_DELAY column. This column is the one we want to predict and the aim of our model is to be able to predict if a flight will arrive to its destination with some type of delay. Therefore, we want to convert this column into a column with binary values.\n",
    "\n",
    "We will create a new column in our dataset (i.e. DELAYED) which will have a 1 if the flight is delayed and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELAY TREATMENT\n",
    "flights['DELAYED'] = flights['ARRIVAL_DELAY'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAY column has values that range from 1 to 31, which it's unfeasible to work with. We have decided to divide the month in two fortnights and classify each day with the value of the fortnight they belong to. \n",
    "- From days 1 - 15 they will belong to the first fortnight. They will have a value of 1.\n",
    "- From days 15 - 31 they will belong to the second fortnight. The will have a value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uONZFls9ueiW",
    "outputId": "cadec565-7aa7-405b-8a98-f24f4f4e9988"
   },
   "outputs": [],
   "source": [
    "# DAY treatment \n",
    "\n",
    "flights['FORTNIGHT'] = pd.cut(flights['DAY'], bins=[0, 15, 31], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that this is an appropiate transformation because, usually, people earn they salaries at the end of the month. Therefore, people will have more money available during the first fortnight and this could mean that the number of passengers increase causing to be more flights during the first fortnight. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day_of_week column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same arguments we used for the DAY column apply to DAY_OF_WEEK. The values in this column range from 1 (i.e.: Monday) to 7 (i.e.: Sunday). Instead of working with each of this values, we are going to separate them into working days and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_OF_WEEK 1 is in-week days, 2 is weekend\n",
    "\n",
    "flights['WEEK_INFO'] = pd.cut(flights['DAY_OF_WEEK'], bins=[1, 5, 7], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airline column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to treat the AIRLINES column. Originally, our dataframe had around 15 different airlines and we considered that it was going to be unfeasible to work with all of them. Therefore, we have decided to divide them intro three major groups:\n",
    "- Major airlines\n",
    "- Low cost airlines\n",
    "- Regional airlines\n",
    "\n",
    "We believe that this is a valid division because the nature of the airline could possibly affect the amount of delayed flights they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRLINE\n",
    "\n",
    "# Define the airlines categories\n",
    "major_airlines = ['DL', 'AA', 'UA', 'US', 'AS']\n",
    "low_cost_airlines = ['WN', 'NK', 'F9', 'B6', 'VX']\n",
    "regional_airlines = ['EV', 'OO', 'MQ', 'HA']\n",
    "\n",
    "# create a new column with the airline category\n",
    "flights['AC'] = 'Other'\n",
    "flights.loc[flights['AIRLINE'].isin(major_airlines), 'AC'] = 'Major'\n",
    "flights.loc[flights['AIRLINE'].isin(low_cost_airlines), 'AC'] = 'Low-Cost'\n",
    "flights.loc[flights['AIRLINE'].isin(regional_airlines), 'AC'] = 'Regional'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Origin_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original dataset there are over 300 different airports. Although it would be great to be able to keep all of them as individual categories, we have also considered it unfeasible. So, in order to reduce the number of categories, we are going to divide the USA territory in four quadrants:\n",
    "- Upper left\n",
    "- Upper right\n",
    "- Bottom right\n",
    "- Bottom left"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before being able to classify the airports as we just mentioned, we first need to get its coordinates. This can be achieved by extracting the necessary data (i.e. longitude and latitude) from the *airports* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index of the dataset to the IATA_CODE (code that identifies each airport)\n",
    "airports = airports.set_index('IATA_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LONGITUDE_O column to each row in flights with the longitude of the origin airport\n",
    "flights['LONGITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "\n",
    "# Add LATITUDE_O column to each row in flights with the latitude of the origin airport\n",
    "flights['LATITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have detected that some rows have a *NaN* value in either their longitude or latitude. Since this happens rarely, we can delete those columns without loosing to many rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_O\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false/len(flights[\"LONGITUDE_O\"]))\n",
    "flights = flights.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have removed al conflictive rows, we can proceed with the classification of the airports by its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty column with airport location (i.e.: GR_O)\n",
    "flights['GR_O'] = ''\n",
    "\n",
    "# Classify the airports\n",
    "# Upper right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_RIGHT'\n",
    "\n",
    "# Upper left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_LEFT'\n",
    "\n",
    "# Bottom right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_RIGHT'\n",
    "\n",
    "# Bottom left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_LEFT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check that all airports have been given a location and that there are no empty quadrants, which would indicate that the classification is not correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[\"GR_O\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Destination_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the same transformations we just applied for the destination airport column. We first add the coordinates of the airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['LONGITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "flights['LATITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_D\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false)\n",
    "print(len(flights[\"LONGITUDE_D\"]))\n",
    "\n",
    "flights = flights.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And classify the destination airports based on their coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['GR_D'] = ''\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_LEFT'\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_LEFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[\"GR_D\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify scheduled_time and departure_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset has a few columns with values ranging from 0000 to 2359. This values are found in columns scheduled_time and departure_time and they represent an hour and minutes. The format is hhmm (i.e.: 1915 is 19:15).\n",
    "\n",
    "Once again, having 2359 categories is unfeasible. The solution we have come up with is dividing the day between daytime and nightime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to classify the time segment based on the input time\n",
    "def classify_time(time):\n",
    "    sunrise = 600.0   # Define the time of sunrise as 6:00 am (in decimal format)\n",
    "    sunset = 1800.0   # Define the time of sunset as 6:00 pm (in decimal format)\n",
    "    \n",
    "    if time >= sunrise and time < sunset:\n",
    "        return 'Daytime'\n",
    "    else:\n",
    "        return 'Nighttime'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined the function that will allow us to apply the classification, we apply it to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change formatting of SCHEDULED_ARRIVAL and DEPARTURE_TIME\n",
    "flights['ArrivalDayNight'] = flights['SCHEDULED_ARRIVAL'].apply(classify_time)\n",
    "flights['DepartureDayNight'] = flights['DEPARTURE_TIME'].apply(classify_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing with and processing all categorical data, we are left with a few unwanted columns, those ones we have used to create new columns. We will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.describe()\n",
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[['DELAYED','DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE', 'Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O',\n",
    "       'GR_D', 'ArrivalDayNight', 'DepartureDayNight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "In order to do the process correctly, normalization across instances should be done after splitting the data between training and test set, using only the data from the training set. This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance. In our case, that is what we are going to do right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X as the features (input variables)\n",
    "X = flights.drop(\"DELAYED\", axis=1)  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Define y as the target variable\n",
    "y = flights[\"DELAYED\"]  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we observe that the distribution of the features are the same than that of the whole dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have processed all our data, it migh be interesting to, once again, visualize the distribution from some of the new columns we have added to the dataset and study if we can extract any knowledge from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"Q_YEAR\"], \"Distribution of flights by quarter of the year\", \"Quarter\", None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the difference between the three quarters it is not huge, it is clear that, as we expected, the second quarter (i.e.: May to August) is the one with the greater amount of flights as it matches with the summer season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, False)\n",
    "plot_histogram_of_column(X_train[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the difference is minimal. However, if we apply the logarithm to the y axis we can see that the second forntingh accumulates more flights than the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"WEEK_INFO\"], \"Flights distribution by working days and weekends\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it could be expected, weekdays accumulate almost three times more flights than weekends. However, we must keep in mind that the weekday category groups five days while the weekend only groups two days. Probably, the difference wouldn't be this big if we were plotting the average number of flights per day. However, this escapes from the scope of this project as it doesn't provide any additional information that could be helpful to predict flight delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"AC\"], \"Flights distribution by airline category\", \"Airline category\", None, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"GR_O\"], \"Flights distribution by quadrant of origin airport\", \"Quadrant\", None, False,False)\n",
    "\n",
    "plot_histogram_of_column(X_train[\"GR_D\"], \"Flights distribution by quadrant of destination airport\", \"Quadrant\", None, False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the quadrants with the greater amount of flights are *UPPER_RIGHT* and *BOTTOM_RIGT*. This represents the east cost of the USA, where many of the biggest cities are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_column(X_train[\"DepartureDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)\n",
    "\n",
    "plot_histogram_of_column(X_train[\"ArrivalDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logically, most of the flights departure during daytime and this trend seems to hold for the arrival time. However, if we compare both histograms we will see that the amount of flights departing during daytime is considerably greater than the flights arriving during daytime. This mean that a considerable amount of flights arrive at their destination at night time even though day departed at daytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gausianization of continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models we will apply assume that our data is distributed following a gaussian distribution, we need to make sure that our data is gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we are going to determine which columns of our dataset are the ones containing continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare which variables are categorical and which ones are continuous. \n",
    "\n",
    "continuous = ['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE']\n",
    "\n",
    "X_train[continuous].hist()               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data doesn't follow a gaussian distribution. Before gaussianizing it, we will first remove any outliers that can be identified in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_outliers_zscore(data_series, threshold=3):\n",
    "    \"\"\"\n",
    "    Remove outliers from a pandas Series using the Z-score method.\n",
    "    :param data_series: a pandas Series containing the data\n",
    "    :param threshold: the number of standard deviations from the mean at which to consider a data point an outlier\n",
    "    :return: a new pandas Series with the outliers removed\n",
    "    \"\"\"\n",
    "    z_scores = np.abs((data_series - data_series.mean()) / data_series.std())\n",
    "    return data_series[z_scores <= threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers (no outliers in the whole dataset, no need to modify X_train and X_test)\n",
    "X_train = remove_outliers_zscore(X_train[continuous])\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that no outliers are removed. We can now proceed with the normalization of the continuous data defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we normalize continuous variables\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Initialize transformer with number of quantiles and output distribution\n",
    "transformer = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "# Apply transformation to continuous columns\n",
    "for col in continuous:\n",
    "    flights[col] = transformer.fit_transform(flights[col].values.reshape(-1, 1))\n",
    "    X_train[col] = transformer.fit_transform(X_train[col].values.reshape(-1, 1))\n",
    "    X_test[col] = transformer.fit_transform(X_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot the normalized data, which now clearly follow a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[continuous].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing with the continuous variables, we will apply hot encoding to our categorical variables, which are:\n",
    "- Q_year\n",
    "- Fortnight\n",
    "- Week_info\n",
    "- Ac\n",
    "- Gr_o\n",
    "- Gr_d\n",
    "- ArrivalDayNight\n",
    "- DepartureDayNight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for categorical variables\n",
    "categorical = ['Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O','GR_D', 'ArrivalDayNight', 'DepartureDayNight']\n",
    "\n",
    "flights= pd.get_dummies(flights, columns=categorical)\n",
    "X_train = pd.get_dummies(X_train, columns = categorical)\n",
    "X_test = pd.get_dummies(X_test, columns = categorical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations between pair of variables\n",
    "From a pairplot diagram, we can infer several things:\n",
    "\n",
    "- Correlation: The scatterplots in the pairplot diagram give us an idea of the relationship between variables. If the points in a scatterplot are closely clustered around a line or a curve, it suggests a strong correlation between those variables. \n",
    "\n",
    "- Distributions: The histograms on the diagonal of the pairplot diagram show the distribution of each variable individually. \n",
    "\n",
    "- Outliers: Outliers are data points that significantly deviate from the overall pattern in the scatterplots. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we believe is worth mentioning that we have included this graph just after normalizing our continuous variables since it improves drastically the colclusions that can be extracted from it because the pairplots obtained with the raw data where strongly influenced by some large values and it's analisis was quite difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot_variables = ['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE', 'DELAYED']\n",
    "\n",
    "X_train['DELAYED'] = y_train\n",
    "\n",
    "sns.pairplot(data=X_train[pairplot_variables][:1000], hue='DELAYED')\n",
    "\n",
    "X_train = X_train.drop('DELAYED', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this plot will be commented in the final report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection (consider a subset of variables)\n",
    "Nextly we will check if getting a smaller subset of variables we can get better results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature selection model\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(random_state=42), threshold='mean')\n",
    "\n",
    "# Fit the feature selector on the training data\n",
    "feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training and test sets to include only the selected features\n",
    "X_train_selected = feature_selector.transform(X_train)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "\n",
    "# Get the original feature names for the selected features\n",
    "selected_feature_names = X.columns[selected_feature_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the two feature subsets during all the modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf = X_train[selected_feature_names]\n",
    "X_test_sf = X_test[selected_feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly take a look at the difference in columns between both feature subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric for choosing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we are going to decide which of the metrics to use in order to compare our models and decide which of them is the best. We firstly observe the balance between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your target variable is stored in a DataFrame or Series named \"y\"\n",
    "class_counts = y.value_counts()\n",
    "class_balance = class_counts / len(y) * 100\n",
    "\n",
    "print(\"Class Balance:\")\n",
    "print(class_balance)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With class 0 (i.e. on-time flights) at 62.41% and class 1 (i.e. delayed flights) at 37.59%, there is a noticeable difference in class frequencies, but it does not indicate severe class unbalance. However, it is still worth considering the implications of the unbalance and how it might affect our modeling approach.\n",
    "\n",
    "When dealing with unbalanced data, it is important to be aware that standard evaluation metrics like accuracy may not provide an accurate representation of model performance. Instead, we need to focus on metrics that are more robust to class unbalance, such as precision, recall, F1 score, or area under the ROC curve (AUC-ROC).\n",
    "\n",
    "Nevertheless, since the unbalance is not that big, we will also consider the accuracy score in order to choose our models. **We mainly will take into account the F1-score (taking into account then Recall and precision that will take into account the slightly unbalanced classes that we have) and the accuracy since we don't have that much unbalance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "To start with we quickly remind of some of the options we have to optimize the learning of our models and that we will (more or less) take into account when modelling. \n",
    "\n",
    "**Hyperparameter Tuning**: Fine-tune the hyperparameters of the model to find the optimal combination for our specific problem. We are going to use techniques like grid search, random search, or Bayesian optimization to systematically explore the hyperparameter space and identify the best configuration.\n",
    "\n",
    "**Cross-Validation**: We will use cross-validation techniques, such as k-fold cross-validation, to evaluate our model's performance more reliably. This helps to assess how well our model generalizes to unseen data and reduces the risk of overfitting.\n",
    "\n",
    "**Ensemble Methods**: Explore ensemble methods like bagging, boosting, or stacking to combine multiple models and improve overall performance. Ensemble techniques can help capture different patterns in the data and reduce model variance.\n",
    "\n",
    "**Data Augmentation**: Since we have a lot of rows in our dataset, we will try with a small subset of this dataset and in the case this is not enough, we are going to proceed with picking a bigger subset of rows so that our model is able to sufficiently learn.\n",
    "\n",
    "**Early Stopping**: Implement early stopping during model training to prevent overfitting and find the optimal number of training epochs. Early stopping stops training when the model's performance on a validation set starts to deteriorate.\n",
    "\n",
    "**Model Architecture**: Experiment with different model architectures or network architectures, such as adding or removing layers, adjusting layer sizes, or trying different activation functions.\n",
    "\n",
    "**Monitoring and Debugging**: Monitor your model during training, track performance metrics, and analyze learning curves to identify issues like underfitting, overfitting, or convergence problems. Debug any potential errors, explore misclassified samples, and consider adjusting your model or data accordingly.\n",
    "\n",
    "\n",
    "\n",
    "In the following dataframe we are going to store the metrics for all of the models that we are going to fit in order to compare them. We are going to score metrics for every kind of model fitted with both subset of features and before and after HyperParameter tuning (HT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(index=[], columns= ['Accuracy', 'F1 Macro', 'Precision Macro', 'Recall Macro'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "First we comment the hyperparameters we are going to tune: \n",
    "\n",
    "**C** :The C parameter in logistic regression controls the inverse of the regularization strength. It determines the trade-off between fitting the training data and preventing overfitting. Higher values of C result in less regularization, allowing the model to fit the training data more closely.\n",
    "\n",
    "**penalty:** The penalty parameter specifies the type of regularization used in logistic regression.\n",
    "'l1' penalty corresponds to L1 regularization. It adds a penalty term proportional to the absolute value of the coefficients, promoting sparsity by encouraging some coefficients to become exactly zero.\n",
    "\n",
    "**solver:** The solver parameter determines the algorithm used to solve the optimization problem in logistic regression.\n",
    "'saga' solver is an extension of the Stochastic Average Gradient (SAG) solver. It supports both L1 and L2 regularization and is suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly fit the models for both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation using the Logistic Regression model\n",
    "cv_results_logreg_1 = cross_validate(logreg, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression all features Before HT',:] = [cv_results_logreg_1['test_accuracy'].mean(), cv_results_logreg_1['test_f1_macro'].mean(),cv_results_logreg_1['test_precision_macro'].mean() , cv_results_logreg_1['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation using the Logistic Regression model\n",
    "cv_results_logreg_2 = cross_validate(logreg, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression subset features Before HT',:] = [cv_results_logreg_2['test_accuracy'].mean(), cv_results_logreg_2['test_f1_macro'].mean(),cv_results_logreg_2['test_precision_macro'].mean() , cv_results_logreg_2['test_recall_macro'].mean()]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning with the small subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "              }\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model with the best hyperparameters\n",
    "logreg = LogisticRegression(C=1, penalty='l1', solver = 'saga')\n",
    "\n",
    "cv_results_logreg_ht_1 = cross_validate(logreg, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression all features after HT',:] = [cv_results_logreg_ht_1['test_accuracy'].mean(), cv_results_logreg_ht_1['test_f1_macro'].mean(),cv_results_logreg_ht_1['test_precision_macro'].mean() , cv_results_logreg_ht_1['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model with the best hyperparameters\n",
    "logreg = LogisticRegression(C=1, penalty='l1', solver = 'saga')\n",
    "\n",
    "cv_results_logreg_ht_2 = cross_validate(logreg, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression subset features after HT',:] = [cv_results_logreg_ht_2['test_accuracy'].mean(), cv_results_logreg_ht_2['test_f1_macro'].mean(),cv_results_logreg_ht_2['test_precision_macro'].mean() , cv_results_logreg_ht_2['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the best of the models that turns to be the logistic regression of all features after HT and observe the sparsity of the coefficients to consider possible modifications on the subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Fit the logistic regression model to your training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "X_train_copy = X_train\n",
    "\n",
    "# Get the coefficients of the logistic regression model\n",
    "coefficients = logreg.coef_[0]\n",
    "\n",
    "# Create a mask indicating which coefficients are smaller than 0.01\n",
    "mask = np.abs(coefficients) < 0.01\n",
    "\n",
    "# Iterate until all coefficients are larger than 0.01\n",
    "while np.sum(mask) > 0:\n",
    "    # Get the indices of the features to be eliminated\n",
    "    feature_indices_to_eliminate = np.where(mask)[0]\n",
    "\n",
    "    # Eliminate the features from X_train\n",
    "    X_train_copy = X_train_copy.drop(X_train_copy.columns[feature_indices_to_eliminate], axis=1)\n",
    "\n",
    "    # Fit the logistic regression model to the updated training data\n",
    "    logreg.fit(X_train_copy, y_train)\n",
    "\n",
    "    # Get the updated coefficients\n",
    "    coefficients = logreg.coef_[0]\n",
    "\n",
    "    # Update the mask\n",
    "    mask = np.abs(coefficients) < 0.01\n",
    "\n",
    "# Print the final selected features\n",
    "other_selected_features = X_train_copy.columns\n",
    "print(\"Selected Features:\", other_selected_features)\n",
    "\n",
    "logreg.fit(X_train_copy, y_train)\n",
    "f_coefficients = logreg.coef_\n",
    "print(f_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider this third model using this new subset of features that we found in between and observe if it works out better. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "              }\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_copy, y_train)\n",
    "\n",
    "# Get the best hyperparameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model with the best hyperparameters\n",
    "logreg = LogisticRegression(C=0.1, penalty='l2', solver = 'newton-cg')\n",
    "\n",
    "cv_results_logreg_ht_2 = cross_validate(logreg, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Logistic Regression subset features after HT',:] = [cv_results_logreg_ht_2['test_accuracy'].mean(), cv_results_logreg_ht_2['test_f1_macro'].mean(),cv_results_logreg_ht_2['test_precision_macro'].mean() , cv_results_logreg_ht_2['test_recall_macro'].mean()]\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we observe that this third subset of variables using the sparsity is not better, we will only consider it at the end with the final model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVC)\n",
    "First of all we recall the hyperparameters we are going to tune:\n",
    "\n",
    "- **C:** Regularization parameter. It controls the trade-off between allowing training errors and maximizing the margin. Larger values of C penalize errors more, leading to a potentially narrower margin.\n",
    "\n",
    "- **kernel:** Specifies the type of kernel function used for mapping the input data to a higher-dimensional feature space. Common options include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "- **gamma:** Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels. It defines the influence of training samples on the decision boundary.\n",
    "\n",
    "We fit the SVC using the whole subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only using the best features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have very similar results, let us compute the hyperparameter tuning with the SVC using only the small part of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the SVC model\n",
    "clf = SVC()\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit SVC using the hyperparameters that we found and using both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma = 'scale', C=1, coef0 = 0.0, degree = 2)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma = 'scale', C=1)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features after HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now observing that the subset of features is now doing better job than that of all the features surprisingly. The best combination is the last one we fitted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "The parameters for a  GaussianNB are the following: \n",
    "- **Priors:** Prior probabilities of the classes. If specified, the priors are not adjusted according to the data. \n",
    "- **Var_smoothing:** Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "\n",
    "However we are only going to tune var_smoothing. To proceed, we are going to fit the Naive Bayes model for both subsets of the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = GaussianNB()\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = GaussianNB()\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since reducing features helps the model fit better we recall the other subset found from sparsity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = GaussianNB()\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_copy, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset 2 features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in this case it doesn not work any better we neglect this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute hyperparameter tuning using the smallest subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.25 ,0.3 , 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] \n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the model using the best parameter we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = GaussianNB(var_smoothing = 1e-09)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = GaussianNB(var_smoothing = 1e-09)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the best model is by far the one using the subset of features (the other one is probably overfitting) and in this case the hyperparamter tuning hasn't been helpful since the F1 score has decreased and the accuracy has almost remained the same. Therefore we belief that the best model is that of Gaussian Naive Bayes subset features After HT (or before). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "n_components: This hyperparameter specifies the number of components (dimensions) to retain after performing dimensionality reduction with LDA. By default, n_components is set to None, which means it will retain (n_classes - 1) components, where n_classes is the number of distinct classes in the data. In the param_grid, values of [None, 1, 2, 3] are provided to try different numbers of retained components. By setting n_components to a specific value, you can explicitly control the dimensionality of the reduced feature space.\n",
    "\n",
    "shrinkage: Shrinkage is a regularization technique used to improve the estimation of the covariance matrix in LDA, especially when the number of samples is small or the covariance matrix is ill-conditioned. The shrinkage hyperparameter controls the degree of shrinkage applied. \n",
    "In the param_grid, the values [None, 'auto', 0.1, 0.5] are provided to try different levels of shrinkage:\n",
    "None: No shrinkage is applied.\n",
    "'auto': Shrinkage is estimated using the Ledoit-Wolf lemma, which automatically determines the amount of shrinkage based on the data.\n",
    "0.1, 0.5: Specific values between 0 and 1 can be provided to manually set the shrinkage intensity.\n",
    "\n",
    "solver: This hyperparameter specifies the solver used for LDA computation. LDA can be solved using different algorithms, and the solver parameter determines the specific algorithm to use. \n",
    "In the param_grid, the values ['svd', 'lsqr', 'eigen'] are provided to try different solver algorithms:\n",
    "'svd': Singular Value Decomposition (SVD) solver, which computes the exact solution but can be slower for large datasets.\n",
    "'lsqr': Least Squares solver, which can handle both shrinkage and regularized covariance matrix.\n",
    "'eigen': Eigenvalue Decomposition solver, which computes the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "We now fit the Linear Discriminant Analysis classifier for both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do hyperparameter tuning with the small subset of features dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LDA classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_components': [None, 1, 2, 3],    # Values to try for n_components\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5],    # Values to try for shrinkage\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],    # Values to try for solver\n",
    "    'tol': [1e-6,1e-5,1e-4, 1e-3, 1e-2]    # Values to try for tol\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=lda, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning we fit the LDA with the hyperparameters we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components = None,shrinkage = None,  solver = 'lsqr', tol = 1e-06)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "We now fit the normal model for the perceptron. The hyperparameters are:\n",
    "\n",
    "- **penalty:** Specifies the penalty term used in the update rule to handle misclassifications. It can be set to 'l1', 'l2', or 'elasticnet'. The default value is 'l2'.\n",
    "\n",
    "- **alpha:** The constant that multiplies the penalty term if regularization is applied. It controls the strength of the regularization. The default value is 0.0001.\n",
    "\n",
    "- **fit_intercept:** Indicates whether an intercept term should be included in the model. If set to True, the perceptron learns an intercept term. The default value is True.\n",
    "\n",
    "- **max_iter:** The maximum number of passes over the training data (epochs) for training the perceptron. The default value is 1000.\n",
    "\n",
    "- **tol:** The tolerance for the stopping criterion. It specifies the minimum change in the average loss for training to continue. The default value is 1e-3.\n",
    "\n",
    "- **shuffle:** Determines whether to shuffle the training data before each epoch during training. The default value is True.\n",
    "\n",
    "- **eta0:** The initial learning rate. It controls the step size at each update during training. The default value is 1.0.\n",
    "\n",
    "- **early_stopping:** If set to True, training will stop when validation loss does not improve anymore. The default value is False.\n",
    "\n",
    "- **validation_fraction:** The proportion of training data to use for early stopping validation. The default value is 0.1.\n",
    "\n",
    "- **n_iter_no_change:** The maximum number of epochs to wait for the validation loss to improve when early_stopping is enabled. The default value is 5.\n",
    "\n",
    "We now fit the models for both subsets of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perceptron = Perceptron()\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perceptron = Perceptron()\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the hyper parameters using the smallest subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Here we define the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization penalty type\n",
    "    'alpha': [0.0001, 0.001, 0.01],         # Regularization parameter\n",
    "    'max_iter': [1000, 2000, 3000],         # Maximum number of iterations\n",
    "    'eta0': [0.1, 0.01, 0.001],             # Initial learning rate\n",
    "    'tol': [10,1,1e-1,1e-2,1e-3]            # Tolerance for stopping criterion\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=perceptron, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit with the found hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perceptron = Perceptron(alpha = 0.01, eta0=0.1, max_iter=1000, penalty='l1', tol=0.001)\n",
    "\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perceptron = Perceptron(alpha = 0.01, eta0=0.1, max_iter=1000, penalty='l1', tol=0.001)\n",
    "\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we observe that the best model is by far the last we fitted: Perceptron subset features After HT, since it has the best accuracy and f1 score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost \n",
    "\n",
    "The hyperparameters for the gradient boost are:\n",
    "\n",
    "- **n_estimators:** The number of boosting stages to perform.\n",
    "\n",
    "- **learning_rate:** The learning rate or shrinkage parameter, which controls the contribution of each tree.\n",
    "\n",
    "- **max_depth:** The maximum depth of individual trees in the ensemble.\n",
    "\n",
    "- **subsample:** The subsample ratio of the training instances.\n",
    "\n",
    "- **min_samples_split:** The minimum number of samples required to split an internal node.\n",
    "\n",
    "We fit the model for both subset of features: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# We define the hyperparameter grid \n",
    "param_grid = {\n",
    "    #'n_estimators': [50, 100, 200],  # Number of boosting stages to perform\n",
    "    #'learning_rate': [0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [3, 4, 5],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [2, 3, 4]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the left hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],  # Number of boosting stages to perform\n",
    "    'learning_rate': [1,0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [4],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [3]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=50, subsample=0.8)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=50, subsample=0.8)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain slightly better results with the Gradient Boost subset features After HT. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "The hyperparameters we can tune for the random forest are:\n",
    "\n",
    "- **n_estimators:** The number of decision trees in the random forest. Increasing the number of estimators typically improves performance but increases computational complexity. It represents the ensemble size.\n",
    "\n",
    "- **max_depth:** The maximum depth of each decision tree in the forest. Higher values increase model complexity and can lead to overfitting.\n",
    "\n",
    "- **min_samples_split:** The minimum number of samples required to split an internal node. Larger values prevent overfitting by requiring a certain number of samples in each split.\n",
    "\n",
    "- **min_samples_leaf:** The minimum number of samples required to be at a leaf node. Similar to min_samples_split, larger values help control overfitting by requiring a minimum number of samples in each leaf.\n",
    "\n",
    "- **max_features:** The number of features to consider when looking for the best split. Reducing this number can help control overfitting. Values such as 'sqrt' or 'log2' can be used to consider a square root or logarithm of the total features, respectively.\n",
    "\n",
    "- **bootstrap:** Determines whether bootstrap samples are used when building trees. Setting it to True enables bootstrap sampling, while False disables it. Bootstrap sampling introduces randomness into the training process and helps improve model diversity.\n",
    "\n",
    "- **criterion:** The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We have an additional parameter (not to tune) which is criterion: The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We first fit the normal model with both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to do hyperparameter tuning with the small subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# We must first define the parameter grid\n",
    "param_grid = {\n",
    "    #'n_estimators': [100, 200, 500],\n",
    "    #'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use the results of the hyperparameter tunning to fit betters random forest for both of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='entropy', max_features='log2', min_samples_leaf=4, min_samples_split=15,random_state=42)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='gini', max_depth=None, max_features='log2', min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=42)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets\n",
    "Hyperparameters: \n",
    "\n",
    "- **Learning rate**: The learning rate determines how much the weights are updated in response to the estimated error each time the model weights are updated. Choosing the right learning rate can be crucial as a value too small may result in a long training process that could get stuck, while a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "- **Optimizer:** what type of optimizer you want to use\n",
    "\n",
    "- **Batch size:** This is the number of samples to work through before updating the internal model parameters. A smaller batch size can lead to more updates and potentially faster convergence, but it also introduces more variance, which can lead to instability in the learning process.\n",
    "\n",
    "- **Activation function:** Different activation functions can result in significant differences in the performance of a neural network. The Rectified Linear Unit (ReLU) and its variants (like Leaky ReLU, Parametric ReLU) are often a good starting point for many problems.\n",
    "\n",
    "We now do some neural nets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install needed packages for following modeling\n",
    "!pip install scikeras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network I \n",
    "Fit neural network with all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with the subset of variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64, 96],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network II\n",
    "We firstly fit the neural network with both subsests of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with small subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features after HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we cannot do hyperparameter tuning because of the vast quantity of time needed (no time left enough), we are going to reconsider the other subset of features obtained from the sparsity of logistic regression and evaluate it on both neural networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_copy.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_copy, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I subset 2 features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_copy.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_copy, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II subset 2 features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tried and tested all the previous classifiers, we are going to select the one with the best overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'F1 Macro' column in descending order\n",
    "sorted_df = results_df.sort_values(by='F1 Macro', ascending=False)\n",
    "\n",
    "sorted_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best results are obtained using the first neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "keras_classifier.fit(X_train_sf, y_train)\n",
    "\n",
    "# Predict using the trained model on the test data\n",
    "y_pred = keras_classifier.predict(X_test[selected_feature_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted probabilities from the model\n",
    "y_pred_prob = keras_classifier.predict_proba(X_test[selected_feature_names])\n",
    "\n",
    "# Set the decision threshold to 0.45\n",
    "threshold = 0.45\n",
    "\n",
    "# Apply the threshold to the predicted probabilities\n",
    "y_pred = (y_pred_prob[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final classifier, we can compute the confusion matrix in order to analyse more in depth how is our model performing. This results are commented in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(true, pred):\n",
    "    \"\"\"\n",
    "    Function for pretty printing confusion matrices\n",
    "    \"\"\"\n",
    "    true.name = 'target'\n",
    "    pred.name = 'predicted'\n",
    "    cm = pd.crosstab(true.reset_index(drop=True), pred.reset_index(drop=True))\n",
    "    cm = cm[cm.index]\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test, pd.Series(y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
