{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X1a2rkiAgHQ"
   },
   "source": [
    "#Â **AA1 Project** \n",
    "\n",
    "## Modeling delays in the air\n",
    "\n",
    "In order to predict whether a flight is likely to be delayed or not, create a ML model that will make predictions with that aim. \n",
    "\n",
    "First of all. Let's observe our data set and some first insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9WFwwT0G--Vb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.ticker import ScalarFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4naEd9v_gWI",
    "outputId": "f9312b58-6257-40ee-aac4-dd514d576d82"
   },
   "outputs": [],
   "source": [
    "airports = pd.read_csv('airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marcj\\AppData\\Local\\Temp\\ipykernel_12260\\2102207835.py:1: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  flights = pd.read_csv('flights.csv')\n"
     ]
    }
   ],
   "source": [
    "flights = pd.read_csv('flights.csv')\n",
    "flights = flights.sample(frac=0.0025, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsI8BfIVAdal"
   },
   "source": [
    "As we can se with the `describe` method, we can observe the range where each feature takes values at more or less. It is also interesting to observe the histograms of the different columns to see how each feature is distributed more or less, to get an intuition of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "WMan5A-f_l5x",
    "outputId": "5e0b5489-d212-4929-cf12-18f77639fec4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.981244</td>\n",
       "      <td>-98.378964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.616736</td>\n",
       "      <td>21.523492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.483450</td>\n",
       "      <td>-176.646030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.652040</td>\n",
       "      <td>-110.839385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.297610</td>\n",
       "      <td>-93.403070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.154675</td>\n",
       "      <td>-82.722995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.285450</td>\n",
       "      <td>-64.798560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LATITUDE   LONGITUDE\n",
       "count  319.000000  319.000000\n",
       "mean    38.981244  -98.378964\n",
       "std      8.616736   21.523492\n",
       "min     13.483450 -176.646030\n",
       "25%     33.652040 -110.839385\n",
       "50%     39.297610  -93.403070\n",
       "75%     43.154675  -82.722995\n",
       "max     71.285450  -64.798560"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()\n",
    "airports.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'YEAR'}>,\n",
       "        <Axes: title={'center': 'MONTH'}>,\n",
       "        <Axes: title={'center': 'DAY'}>,\n",
       "        <Axes: title={'center': 'DAY_OF_WEEK'}>,\n",
       "        <Axes: title={'center': 'FLIGHT_NUMBER'}>],\n",
       "       [<Axes: title={'center': 'SCHEDULED_DEPARTURE'}>,\n",
       "        <Axes: title={'center': 'DEPARTURE_TIME'}>,\n",
       "        <Axes: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <Axes: title={'center': 'TAXI_OUT'}>,\n",
       "        <Axes: title={'center': 'WHEELS_OFF'}>],\n",
       "       [<Axes: title={'center': 'SCHEDULED_TIME'}>,\n",
       "        <Axes: title={'center': 'ELAPSED_TIME'}>,\n",
       "        <Axes: title={'center': 'AIR_TIME'}>,\n",
       "        <Axes: title={'center': 'DISTANCE'}>,\n",
       "        <Axes: title={'center': 'WHEELS_ON'}>],\n",
       "       [<Axes: title={'center': 'TAXI_IN'}>,\n",
       "        <Axes: title={'center': 'SCHEDULED_ARRIVAL'}>,\n",
       "        <Axes: title={'center': 'ARRIVAL_TIME'}>,\n",
       "        <Axes: title={'center': 'ARRIVAL_DELAY'}>,\n",
       "        <Axes: title={'center': 'DIVERTED'}>],\n",
       "       [<Axes: title={'center': 'CANCELLED'}>,\n",
       "        <Axes: title={'center': 'AIR_SYSTEM_DELAY'}>,\n",
       "        <Axes: title={'center': 'SECURITY_DELAY'}>,\n",
       "        <Axes: title={'center': 'AIRLINE_DELAY'}>,\n",
       "        <Axes: title={'center': 'LATE_AIRCRAFT_DELAY'}>],\n",
       "       [<Axes: title={'center': 'WEATHER_DELAY'}>, <Axes: >, <Axes: >,\n",
       "        <Axes: >, <Axes: >]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flights.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the histograms, we can appreciate that some of the variables in our dataset present variables where the data appears to be quite concentrated and this could be explained by the presence of large outliers. Some other variables seem to be more uniformally distributed. However, the nature of the distribution that describes the data in our variables shouldn't worry ourselve to much right now as many data processment will be done (including gaussianization of the continuos variables and hot encoding of the categorical ones) before trainging and testing a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owSaZFCZ3-V2"
   },
   "source": [
    "## 1. Data cleaning\n",
    "### Nextly we will portray the schedule that we are going to follow. \n",
    "1. Check for duplicates: Check for and remove any duplicate rows in our dataset.\n",
    "\n",
    "2. Handle missing data: Identify any missing data and decide how to handle it. We will either remove the rows or fill in the missing data.\n",
    "\n",
    "3. Check for inconsistent data: Check for any inconsistent or erroneous data, such as values that are out of range or inconsistent with other data in the same row.\n",
    "\n",
    "4. Handle categorical data: If you have categorical data, decide how to handle it. One common approach is to use one-hot encoding.\n",
    "\n",
    "5. Normalize data: Normalize the data so that the features have similar ranges. This will prevent features with large ranges from dominating the model.\n",
    "\n",
    "6. Feature selection: Select the most relevant features for your model. You can use various techniques such as correlation analysis or principal component analysis (PCA).\n",
    "\n",
    "7. Train-test split: Finally, split your data into training and testing sets to evaluate the performance of your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9qgdnb740x-"
   },
   "source": [
    "### 1.1 Check for duplicates\n",
    "We firstly see that there are no duplicate samples with the following piece of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSrzDv455MWf",
    "outputId": "ffa33860-4374-4db7-ea5a-98cca5564064"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates = flights.duplicated()\n",
    "\n",
    "# Print the duplicate rows\n",
    "len(flights[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeODOfOB5pNx"
   },
   "source": [
    "### 1.2 Handle missing data\n",
    "In this section we have a bunch of different possibilities in order to approach the problem of missing data. \n",
    "1. Remove missing data: if the quantity of missing data is not that significative, maybe it is a good option to consider removing all those samples that contain `NaN` values since the subset of samples that is going to be deleted may not be significant while training the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W0tqcTO6hYt",
    "outputId": "9afbcca0-8c45-4053-d09b-209047d794f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR                       0\n",
      "MONTH                      0\n",
      "DAY                        0\n",
      "DAY_OF_WEEK                0\n",
      "AIRLINE                    0\n",
      "FLIGHT_NUMBER              0\n",
      "TAIL_NUMBER               34\n",
      "ORIGIN_AIRPORT             0\n",
      "DESTINATION_AIRPORT        0\n",
      "SCHEDULED_DEPARTURE        0\n",
      "DEPARTURE_TIME           204\n",
      "DEPARTURE_DELAY          204\n",
      "TAXI_OUT                 217\n",
      "WHEELS_OFF               217\n",
      "SCHEDULED_TIME             0\n",
      "ELAPSED_TIME             253\n",
      "AIR_TIME                 253\n",
      "DISTANCE                   0\n",
      "WHEELS_ON                226\n",
      "TAXI_IN                  226\n",
      "SCHEDULED_ARRIVAL          0\n",
      "ARRIVAL_TIME             226\n",
      "ARRIVAL_DELAY            253\n",
      "DIVERTED                   0\n",
      "CANCELLED                  0\n",
      "CANCELLATION_REASON    14330\n",
      "AIR_SYSTEM_DELAY       11803\n",
      "SECURITY_DELAY         11803\n",
      "AIRLINE_DELAY          11803\n",
      "LATE_AIRCRAFT_DELAY    11803\n",
      "WEATHER_DELAY          11803\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in each column\n",
    "nan_counts = flights.isna().sum()\n",
    "# Print the results\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwRj6Uih68dv"
   },
   "source": [
    "From the previous output we see that the last 6 features are almost useless since the majority of the samples do not have any information of those, therefore we are going to `drop`. A part from that, we also need to delete columns that regard information that is not going to be available (a posteriori information from the flight) such as information of the time elapsed during time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "g28GIbss7vHi"
   },
   "outputs": [],
   "source": [
    "cols_of_interest = ['ARRIVAL_DELAY','MONTH', 'DAY' ,'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_ARRIVAL', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME', 'DISTANCE']\n",
    "flights = flights[cols_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "jczIDNTZOJpP",
    "outputId": "2b732e2a-8915-470d-c24e-922daa387018"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14344.000000</td>\n",
       "      <td>14344.000000</td>\n",
       "      <td>14331.000000</td>\n",
       "      <td>14331.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.397761</td>\n",
       "      <td>6.505774</td>\n",
       "      <td>15.762373</td>\n",
       "      <td>3.890019</td>\n",
       "      <td>1495.689098</td>\n",
       "      <td>1336.571249</td>\n",
       "      <td>9.245259</td>\n",
       "      <td>16.187217</td>\n",
       "      <td>1360.064057</td>\n",
       "      <td>142.108675</td>\n",
       "      <td>823.882596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37.434848</td>\n",
       "      <td>3.415867</td>\n",
       "      <td>8.794217</td>\n",
       "      <td>1.977637</td>\n",
       "      <td>507.325283</td>\n",
       "      <td>497.630899</td>\n",
       "      <td>35.066599</td>\n",
       "      <td>8.955900</td>\n",
       "      <td>498.626400</td>\n",
       "      <td>75.483655</td>\n",
       "      <td>609.618878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>922.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1522.000000</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>647.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1919.000000</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1758.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>947.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>965.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>4983.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARRIVAL_DELAY         MONTH           DAY   DAY_OF_WEEK  \\\n",
       "count   14295.000000  14548.000000  14548.000000  14548.000000   \n",
       "mean        4.397761      6.505774     15.762373      3.890019   \n",
       "std        37.434848      3.415867      8.794217      1.977637   \n",
       "min       -62.000000      1.000000      1.000000      1.000000   \n",
       "25%       -13.000000      4.000000      8.000000      2.000000   \n",
       "50%        -5.000000      7.000000     16.000000      4.000000   \n",
       "75%         8.000000      9.000000     23.000000      6.000000   \n",
       "max       947.000000     12.000000     31.000000      7.000000   \n",
       "\n",
       "       SCHEDULED_ARRIVAL  DEPARTURE_TIME  DEPARTURE_DELAY      TAXI_OUT  \\\n",
       "count       14548.000000    14344.000000     14344.000000  14331.000000   \n",
       "mean         1495.689098     1336.571249         9.245259     16.187217   \n",
       "std           507.325283      497.630899        35.066599      8.955900   \n",
       "min             1.000000        1.000000       -36.000000      3.000000   \n",
       "25%          1110.000000      922.000000        -5.000000     11.000000   \n",
       "50%          1522.000000     1331.000000        -2.000000     14.000000   \n",
       "75%          1919.000000     1743.000000         8.000000     19.000000   \n",
       "max          2359.000000     2400.000000       965.000000    145.000000   \n",
       "\n",
       "         WHEELS_OFF  SCHEDULED_TIME      DISTANCE  \n",
       "count  14331.000000    14548.000000  14548.000000  \n",
       "mean    1360.064057      142.108675    823.882596  \n",
       "std      498.626400       75.483655    609.618878  \n",
       "min        1.000000       20.000000     31.000000  \n",
       "25%      936.000000       86.000000    373.000000  \n",
       "50%     1345.000000      123.000000    647.000000  \n",
       "75%     1758.000000      174.000000   1066.000000  \n",
       "max     2359.000000      680.000000   4983.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV9TeZHT83xS"
   },
   "source": [
    "Once we've deleted those columns we can say that **maybe** and only **maybe** taking into account that we have more than 5M samples, deleting the other samples that contain at least one `NaN` value, may be reasonable. \n",
    "\n",
    "**CHECK WHETHER THIS IS REASONABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e82megy49YCh",
    "outputId": "c945bf9d-9c1f-4cd4-ad6b-2a5638b92934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14548 14295\n"
     ]
    }
   ],
   "source": [
    "l_bef = len(flights)\n",
    "flights = flights.dropna(how='any')\n",
    "l_aft = len(flights)\n",
    "print(l_bef, l_aft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ0KSQzZ-D3g"
   },
   "source": [
    "We pass from $5819079$ to $5714008$ samples. In other words, we keep the $98.2\\%$ of the samples, so it may be a good option to work with these new subset of samples that still contain a vast quantity of information.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LO35Qxc-pB3"
   },
   "source": [
    "### 1.3 Check for inconsistent data\n",
    "In order to do so, we firstly observe an overview of our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "c3TuA6Cx_x5G",
    "outputId": "8b9139e8-da40-4805-efdf-01dec510440f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.397761</td>\n",
       "      <td>6.521791</td>\n",
       "      <td>15.751522</td>\n",
       "      <td>3.896747</td>\n",
       "      <td>1494.538370</td>\n",
       "      <td>1336.156208</td>\n",
       "      <td>9.159706</td>\n",
       "      <td>16.186079</td>\n",
       "      <td>1359.684715</td>\n",
       "      <td>142.320462</td>\n",
       "      <td>826.171319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37.434848</td>\n",
       "      <td>3.404917</td>\n",
       "      <td>8.782405</td>\n",
       "      <td>1.975532</td>\n",
       "      <td>507.725006</td>\n",
       "      <td>497.734898</td>\n",
       "      <td>34.840084</td>\n",
       "      <td>8.961210</td>\n",
       "      <td>498.663054</td>\n",
       "      <td>75.566948</td>\n",
       "      <td>610.236930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>922.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1917.000000</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1758.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>947.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>965.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>4983.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARRIVAL_DELAY         MONTH           DAY   DAY_OF_WEEK  \\\n",
       "count   14295.000000  14295.000000  14295.000000  14295.000000   \n",
       "mean        4.397761      6.521791     15.751522      3.896747   \n",
       "std        37.434848      3.404917      8.782405      1.975532   \n",
       "min       -62.000000      1.000000      1.000000      1.000000   \n",
       "25%       -13.000000      4.000000      8.000000      2.000000   \n",
       "50%        -5.000000      7.000000     16.000000      4.000000   \n",
       "75%         8.000000      9.000000     23.000000      6.000000   \n",
       "max       947.000000     12.000000     31.000000      7.000000   \n",
       "\n",
       "       SCHEDULED_ARRIVAL  DEPARTURE_TIME  DEPARTURE_DELAY      TAXI_OUT  \\\n",
       "count       14295.000000    14295.000000     14295.000000  14295.000000   \n",
       "mean         1494.538370     1336.156208         9.159706     16.186079   \n",
       "std           507.725006      497.734898        34.840084      8.961210   \n",
       "min             1.000000        1.000000       -36.000000      3.000000   \n",
       "25%          1110.000000      922.000000        -5.000000     11.000000   \n",
       "50%          1521.000000     1331.000000        -2.000000     14.000000   \n",
       "75%          1917.000000     1743.000000         8.000000     19.000000   \n",
       "max          2359.000000     2400.000000       965.000000    145.000000   \n",
       "\n",
       "         WHEELS_OFF  SCHEDULED_TIME      DISTANCE  \n",
       "count  14295.000000    14295.000000  14295.000000  \n",
       "mean    1359.684715      142.320462    826.171319  \n",
       "std      498.663054       75.566948    610.236930  \n",
       "min        1.000000       20.000000     31.000000  \n",
       "25%      936.000000       86.000000    373.000000  \n",
       "50%     1345.000000      123.000000    650.000000  \n",
       "75%     1758.000000      174.000000   1067.000000  \n",
       "max     2359.000000      680.000000   4983.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will plot a few histograms of the variables in our dataset to get a deeper understanding of their nature. First of all we are going to implement a function that is going to allow us to plot the histogram and personalize a few parameters such as the title, labels for the axis and the options to apply logarithms to any of the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_of_column(data, title, x_title, y_title, x_log, y_log):\n",
    "    # Assign default title if none is provided\n",
    "    if not title:\n",
    "        title = \"Histogram\"\n",
    "\n",
    "    # Assign default title to x axis if none is provided\n",
    "    if not x_title:\n",
    "        x_title = \"Value\"\n",
    "    \n",
    "    # Assing default title to y axis if none is provided\n",
    "    if not y_title:\n",
    "        y_title = \"Count\"\n",
    "    \n",
    "    # Apply logarithm to x axis if required\n",
    "    if x_log:\n",
    "        # Set up histogram with logarithmic x-axis\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data, bins=10**np.linspace(np.log10(0.1), np.log10(data.max()), 50))\n",
    "\n",
    "        # Set x-axis to logarithmic scale\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "        # Add \"log\" to x axis title\n",
    "        x_title = \"Log \" + x_title\n",
    "    else:\n",
    "        # Create plot with histogram of DAY column\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(data)\n",
    "\n",
    "\n",
    "    # Apply logarithm to y-axis if required\n",
    "    if y_log:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        # Add \"log\" to y axis title\n",
    "        y_title = \"Log \" + y_title\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a function that allows us to easily plot any column from the dataset that we are interested in, we will start by studying the nature of the columns that refer to delays on the flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of the arrival_delay column\n",
    "plot_histogram_of_column(flights[\"ARRIVAL_DELAY\"], \"Distribution of arrival delay\", None, None, True, False)\n",
    "\n",
    "# Histogram of DEPARTURE_DELAY column\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_DELAY\"], \"Distribution of departure delay\", None, None, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that both histograms share a similar shape, which means that these two variables are strongly correlated. However, we expected to get this result since those flights that suffer departure with any form of delay are extremely likely to also arrive with a similar delay.\n",
    "\n",
    "Another interesting result extracted from the data is that there are quite a lot of flights that depart before their scheduled departure time. Moreover, if we compare closely both histograms, we will notice that the amount of flights that arrive before their arrival time is smaller than the number of flights that departure ahead of their departure time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to analyse how the variables containing data related with the date of the flight are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"MONTH\"], \"Distribution of month\", None, None, False, False)\n",
    "\n",
    "# Create plot with histogram of DAY column\n",
    "plot_histogram_of_column(flights[\"DAY\"], \"Distribution of day\", None, None, False, True)\n",
    "\n",
    "# Histogram of DAY_OF_WEEK colum\n",
    "plot_histogram_of_column(flights[\"DAY_OF_WEEK\"], \"Distribution of day of week\", None, None, False, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we'll plot some other columns that could be interesting to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"DISTANCE\"], \"Distribution of distance\", \"Distance\", None, False, True)\n",
    "\n",
    "plot_histogram_of_column(flights[\"DEPARTURE_TIME\"], \"Distribution of departure time\", \"Time\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distance plot, we observe that the distance of the vast majority of flights range between a few hundred and a three thousand miles. The longest flights in the dataset are around 5000 miles. \n",
    "\n",
    "On the other hand, looking at the distribution of the departure time variabel, we see that fligh departure are almos uniformly distributed from 6 am to 8 pm. From 9 pm to 5 am, a much smaller number of departure take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWIhprxNkiCK",
    "outputId": "43978d06-cd15-4dda-b194-33eb6e73d502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DTW' 'SEA' 'DAL' 'HNL' 'ATL' 'TPA' 'PHX' 'LAX' 11292 'BWI' 'SJU' 'SMF'\n",
      " 'BNA' 'CLT' 'CAE' 'MSP' 'DFW' 'SAN' 'OGG' 'OAK' 14107 'DCA' 'BOS' 'HRL'\n",
      " 'MCI' 'SNA' 'ORD' 'MIA' 'RNO' 12451 'SFO' 'SAT' 'IAH' 'SLC' 'LAS' 'DEN'\n",
      " 'PIT' 'ACV' 'DRO' 'GSP' 'RSW' 'ITO' 12266 'LSE' 'OKC' 'DAY' 'EWR' 10299\n",
      " 'PHL' 'HOU' 11298 'MFE' 'FLL' 'MSY' 'IND' 'BZN' 'PSP' 'ASE' 'MEM' 'RDU'\n",
      " 'SBA' 15016 'SJC' 11066 'JAC' 'MCO' 'ROC' 11697 'BOI' 'CLE' 13830 10257\n",
      " 14908 'SDF' 12278 'MKE' 'JFK' 12889 10397 'RDM' 13851 'BRW' 'ALB' 'OMA'\n",
      " 'CRP' 'RIC' 'DHN' 11618 'BUR' 'CVG' 'MDW' 14893 'JAX' 13485 'ABQ' 12402\n",
      " 'CMH' 12264 10721 'PVD' 11433 'KTN' 10154 'LGA' 'PDX' 'LRD' 'SYR' 'PBI'\n",
      " 'ANC' 'BUF' 'RST' 'TUL' 'TUS' 'STL' 'TLH' 'BDL' 'AUS' 'MSN' 11278 13204\n",
      " 14771 'CHS' '13232' 'DSM' '12478' 14869 'AMA' 'LGB' 'GRR' 'IAD' 10874\n",
      " 'ONT' 'LIH' 'PIB' 15370 10980 13303 'GSO' 'MYR' 12478 11057 'BIS' 13930\n",
      " 'CDV' 14679 10713 'MOB' 14576 'KOA' 'TRI' 'CHO' 14843 'LEX' 11447 'MGM'\n",
      " 15919 14057 'ELP' 'IDA' 'MVY' 'EWN' 13487 10140 'COS' 11259 'ISP' 13495\n",
      " 13232 'CLL' '12391' 'LAN' 'SIT' 'SGF' 'EYW' 'ECP' 14524 15304 10792 'MHT'\n",
      " 'HPN' 12892 'LNK' 'MAF' 'FAR' 'ACT' 'PHF' '12953' 'AVL' 'ATW' 14635 'PNS'\n",
      " 13342 'CIU' 'TYR' 12953 'FCA' 13158 'DLH' 15024 'MLU' 14100 'CMI' 'EUG'\n",
      " 'CRW' 10423 12945 'GFK' 'CID' 'AZO' 'MHK' 'GRK' 10693 'LBB' 11630 'LIT'\n",
      " 'VPS' 'FAI' '11298' 12191 12173 14122 'COU' 'SRQ' 'SHV' 'BFL' '13891'\n",
      " 'BTR' 'SPS' 'MDT' 10821 'ORF' 'FNT' 'JAN' 'JMS' 'AEX' 'JNU' 16218 'BGM'\n",
      " 'MLI' '14986' 'FSD' 11042 'HDN' 'AVP' 'ICT' 11274 14831 'FAY' 'SBN' 'STT'\n",
      " 'EKO' 'ABR' 'SAV' '11042' 'BHM' 14027 14633 11884 'CAK' 'EGE' 'SUX' 13871\n",
      " 'GPT' 'ACK' 'PIA' 'YUM' '14107' 'AGS' 'GGG' 'GTF' 12896 'ILM' 'LAW' 14783\n",
      " 'BTV' 'LWS' 'MRY' 14747 '13851' 10529 'AKN' '14674' 'XNA' 12758 'ELM'\n",
      " 11140 14193 'FLG' 14683 14828 'VLD' 'GRB' 11775 'BIL' 'TTN' 'TWF' '12266'\n",
      " 'OTH' 'OAJ' 'FAT' 'GTR' 'GEG' 'ROA' 11537 10781 '12889' 'TYS' 11193\n",
      " '12892' 'EVV' 10994 13198 'SWF' 11481 'HSV' 12884 'PWM' 10158 'FWA' 'DIK'\n",
      " 'SGU' 12898 '10397' 'BQN' '10721' 'JLN' 'SBP' 'PLN' 11995 11097 'ABI'\n",
      " '13930' 'BTM' 11540 12217 12339 'MBS' 11637 '14831' 15376 '13244' 'BPT'\n",
      " 'BLI' 'GJT' 14492 'BRD' 'CHA' 14321 11267 'BMI' 12389 'HIB' 12992 'HLN'\n",
      " '14783' 13377 'MLB' 'BET' 'ISN' 'DBQ' 'GUC' 'TOL' 11638 '13487' '10821'\n",
      " 12448 'STC' '14747' 'ABE' 10561 11823 'VEL' 'SAF' 14730 'RAP' 11977 'LAR'\n",
      " 13296 'PBG' 'LFT' 10208 10800 'TVC' 14570 '10785' '13830' 11778 '11865'\n",
      " '13495' 14689 11315 13577 'CWA' 'APN' 13931 13244 'ACY' 15041 'FSM' 'INL'\n",
      " '11292' 12343 '15304' 'BRO' 'HYS' 'MOT' 14487 'ROW' 'GRI' 'SJT' 10685\n",
      " 12951 'OME' 'BGR' 'WYS' 12954 'CEC' 'PSC' 'PIH' 11986 15412 10747 '15016'\n",
      " 15249 'SPI' 'CPR' 10185 'MFR' 'COD' 'SUN' 14307 'DAB' 11982 13476 13891\n",
      " 'IAG' 'BJI' 'WRG' 'MEI' 'GUM' 11111 'ABY' 13433 13796 11973 '10792' 'PUB'\n",
      " 'LCH' '14771' 'GNV' 'MTJ' 11624 'TXK' 'GCK' 'GCC' 'PAH' 'CSG' '15376'\n",
      " 'RHI' 'IMT' 14489 10434 'PSE' '14307' 'HOB' '10279' 'MSO' '12191' 13029\n",
      " 'YAK' '10713' 11413 12197 '11433' 15411 'SCE' 10157 'BQK' '11618' 'CDC'\n",
      " 15096 '10431' 11109 15323 13061]\n"
     ]
    }
   ],
   "source": [
    "print(flights[\"DESTINATION_AIRPORT\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "      <td>14295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.397761</td>\n",
       "      <td>6.521791</td>\n",
       "      <td>15.751522</td>\n",
       "      <td>3.896747</td>\n",
       "      <td>1494.538370</td>\n",
       "      <td>1336.153340</td>\n",
       "      <td>9.159706</td>\n",
       "      <td>16.186079</td>\n",
       "      <td>1359.684715</td>\n",
       "      <td>142.320462</td>\n",
       "      <td>826.171319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37.434848</td>\n",
       "      <td>3.404917</td>\n",
       "      <td>8.782405</td>\n",
       "      <td>1.975532</td>\n",
       "      <td>507.725006</td>\n",
       "      <td>497.728885</td>\n",
       "      <td>34.840084</td>\n",
       "      <td>8.961210</td>\n",
       "      <td>498.663054</td>\n",
       "      <td>75.566948</td>\n",
       "      <td>610.236930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>922.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1917.000000</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1758.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>947.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>965.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>4983.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARRIVAL_DELAY         MONTH           DAY   DAY_OF_WEEK  \\\n",
       "count   14295.000000  14295.000000  14295.000000  14295.000000   \n",
       "mean        4.397761      6.521791     15.751522      3.896747   \n",
       "std        37.434848      3.404917      8.782405      1.975532   \n",
       "min       -62.000000      1.000000      1.000000      1.000000   \n",
       "25%       -13.000000      4.000000      8.000000      2.000000   \n",
       "50%        -5.000000      7.000000     16.000000      4.000000   \n",
       "75%         8.000000      9.000000     23.000000      6.000000   \n",
       "max       947.000000     12.000000     31.000000      7.000000   \n",
       "\n",
       "       SCHEDULED_ARRIVAL  DEPARTURE_TIME  DEPARTURE_DELAY      TAXI_OUT  \\\n",
       "count       14295.000000    14295.000000     14295.000000  14295.000000   \n",
       "mean         1494.538370     1336.153340         9.159706     16.186079   \n",
       "std           507.725006      497.728885        34.840084      8.961210   \n",
       "min             1.000000        1.000000       -36.000000      3.000000   \n",
       "25%          1110.000000      922.000000        -5.000000     11.000000   \n",
       "50%          1521.000000     1331.000000        -2.000000     14.000000   \n",
       "75%          1917.000000     1743.000000         8.000000     19.000000   \n",
       "max          2359.000000     2359.000000       965.000000    145.000000   \n",
       "\n",
       "         WHEELS_OFF  SCHEDULED_TIME      DISTANCE  \n",
       "count  14295.000000    14295.000000  14295.000000  \n",
       "mean    1359.684715      142.320462    826.171319  \n",
       "std      498.663054       75.566948    610.236930  \n",
       "min        1.000000       20.000000     31.000000  \n",
       "25%      936.000000       86.000000    373.000000  \n",
       "50%     1345.000000      123.000000    650.000000  \n",
       "75%     1758.000000      174.000000   1067.000000  \n",
       "max     2359.000000      680.000000   4983.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.loc[flights['WHEELS_OFF'] == 2400, 'WHEELS_OFF'] = 2359\n",
    "flights.loc[flights['DEPARTURE_TIME'] == 2400, 'DEPARTURE_TIME'] = 2359\n",
    "\n",
    "\n",
    "\n",
    "flights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZraBMwF8Miy"
   },
   "source": [
    "As we can infer from the output of the `decribe()` method, there are no values that are out of range or are non-sense in comparison with other values of the same column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the rows that contain integer values\n",
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: isinstance(x, int))]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: isinstance(x, int))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[~flights['ORIGIN_AIRPORT'].apply(lambda x: str(x).isnumeric())]\n",
    "flights = flights[~flights['DESTINATION_AIRPORT'].apply(lambda x: str(x).isnumeric())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZwbFfyp9k-l"
   },
   "source": [
    "### 1.4 Handle categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by looking at our columns to determine what type of data each one of them contains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4Mmph7r97oo",
    "outputId": "06cdb8d8-d82b-4c83-d198-60705beff9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARRIVAL_DELAY float64\n",
      "MONTH int64\n",
      "DAY int64\n",
      "DAY_OF_WEEK int64\n",
      "AIRLINE object\n",
      "ORIGIN_AIRPORT object\n",
      "DESTINATION_AIRPORT object\n",
      "SCHEDULED_ARRIVAL int64\n",
      "DEPARTURE_TIME float64\n",
      "DEPARTURE_DELAY float64\n",
      "TAXI_OUT float64\n",
      "WHEELS_OFF float64\n",
      "SCHEDULED_TIME float64\n",
      "DISTANCE int64\n"
     ]
    }
   ],
   "source": [
    "#Check nature of the columns \n",
    "for col in flights.columns:\n",
    "  print(col, flights[col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of the nature of each column we must determine which columns we are going to modify. Those columns are going to be the ones containing categorical variables. We are going to modify:\n",
    "- Month\n",
    "- Arrival_delay\n",
    "- Day\n",
    "- Day_of_week\n",
    "- Origin_airport\n",
    "- Destination_airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiHh3pHHfwVV"
   },
   "source": [
    "The next thing we want to do after observing the types of every feature is to transform some variables to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation we have chosen for the MONTH column is grouping the month by quarters. This will allow us to reduce the number of categories from 12 to 3. \n",
    "\n",
    "Additionally, when dividing the year in quarters we get a partition that closely matches the different travel seasons. Summer season matches almost perfectly with the second quarter while the first and third quarter match with winter season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "7qiBl0YzhBmX",
    "outputId": "946e44b0-5f07-4969-a137-28ebc8d1e52b"
   },
   "outputs": [],
   "source": [
    "# MONTH treatment: group months in quarters\n",
    "flights['Q_YEAR'] = flights['MONTH'].apply(lambda x: (x-1)//4 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrival_delay column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column which's data we need to process is the ARRIVAL_DELAY column. This column is the one we want to predict and the aim of our model is to be able to predict if a flight will arrive to its destination with some type of delay. Therefore, we want to convert this column into a column with binary values.\n",
    "\n",
    "We will create a new column in our dataset (i.e. DELAYED) which will have a 1 if the flight is delayed and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELAY TREATMENT\n",
    "flights['DELAYED'] = flights['ARRIVAL_DELAY'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAY column has values that range from 1 to 31, which it's unfeasible to work with. We have decided to divide the month in two fortnights and classify each day with the value of the fortnight they belong to. \n",
    "- From days 1 - 15 they will belong to the first fortnight. They will have a value of 1.\n",
    "- From days 15 - 31 they will belong to the second fortnight. The will have a value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uONZFls9ueiW",
    "outputId": "cadec565-7aa7-405b-8a98-f24f4f4e9988"
   },
   "outputs": [],
   "source": [
    "# DAY treatment \n",
    "\n",
    "flights['FORTNIGHT'] = pd.cut(flights['DAY'], bins=[0, 15, 31], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that this is an appropiate transformation because, usually, people earn they salaries at the end of the month. Therefore, people will have more money available during the first fortnight and this could mean that the number of passengers increase causing to be more flights during the first fortnight. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day_of_week column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same arguments we used for the DAY column apply to DAY_OF_WEEK. The values in this column range from 1 (i.e.: Monday) to 7 (i.e.: Sunday). Instead of working with each of this values, we are going to separate them into working days and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_OF_WEEK 1 is in-week days, 2 is weekend\n",
    "\n",
    "flights['WEEK_INFO'] = pd.cut(flights['DAY_OF_WEEK'], bins=[1, 5, 7], labels=[1, 2], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airline column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to treat the AIRLINES column. Originally, our dataframe had around 15 different airlines and we considered that it was going to be unfeasible to work with all of them. Therefore, we have decided to divide them intro three major groups:\n",
    "- Major airlines\n",
    "- Low cost airlines\n",
    "- Regional airlines\n",
    "\n",
    "We believe that this is a valid division because the nature of the airline could possibly affect the amount of delayed flights they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRLINE\n",
    "\n",
    "# Define the airlines categories\n",
    "major_airlines = ['DL', 'AA', 'UA', 'US', 'AS']\n",
    "low_cost_airlines = ['WN', 'NK', 'F9', 'B6', 'VX']\n",
    "regional_airlines = ['EV', 'OO', 'MQ', 'HA']\n",
    "\n",
    "# create a new column with the airline category\n",
    "flights['AC'] = 'Other'\n",
    "flights.loc[flights['AIRLINE'].isin(major_airlines), 'AC'] = 'Major'\n",
    "flights.loc[flights['AIRLINE'].isin(low_cost_airlines), 'AC'] = 'Low-Cost'\n",
    "flights.loc[flights['AIRLINE'].isin(regional_airlines), 'AC'] = 'Regional'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Origin_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original dataset there are over 300 different airports. Although it would be great to be able to keep all of them as individual categories, we have also considered it unfeasible. So, in order to reduce the number of categories, we are going to divide the USA territory in four quadrants:\n",
    "- Upper left\n",
    "- Upper right\n",
    "- Bottom right\n",
    "- Bottom left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before being able to classify the airports as we just mentioned, we first need to get its coordinates. This can be achieved by extractinc the necessary data (i.e. longitude and latitude) from the *airports* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index of the dataset to the IATA_CODE (code that identifies each airport)\n",
    "airports = airports.set_index('IATA_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LONGITUDE_O column to each row in flights with the longitude of the origin airport\n",
    "flights['LONGITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "\n",
    "# Add LATITUDE_O column to each row in flights with the latitude of the origin airport\n",
    "flights['LATITUDE_O'] = flights['ORIGIN_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have detected that some rows have a *NaN* value in either their longitude or latitude. Since this happens rarely, we can delete those columns without loosing to many rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999086410354016\n"
     ]
    }
   ],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_O\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false/len(flights[\"LONGITUDE_O\"]))\n",
    "flights = flights.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have removed al conflictive rows, we can proceed with the classification of the airports by its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty column with airport location (i.e.: GR_O)\n",
    "flights['GR_O'] = ''\n",
    "\n",
    "# Classify the airports\n",
    "# Upper right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_RIGHT'\n",
    "\n",
    "# Upper left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] >= 37), 'GR_O'] = 'UPPER_LEFT'\n",
    "\n",
    "# Bottom right\n",
    "flights.loc[(flights['LONGITUDE_O'] >= -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_RIGHT'\n",
    "\n",
    "# Bottom left\n",
    "flights.loc[(flights['LONGITUDE_O'] < -100) & (flights['LATITUDE_O'] < 37), 'GR_O'] = 'BOTTOM_LEFT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check that all airports have been given a location and that there are no empty quadrants, which would indicate that the classification is not correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UPPER_RIGHT', 'BOTTOM_LEFT', 'UPPER_LEFT', 'BOTTOM_RIGHT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights[\"GR_O\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy8g04lNBpmY"
   },
   "source": [
    "We observe a huge amount of features which may overfit the model in case of using them all. Therefore we will try to choose an optimal subset of explanatory variables that are going to be able to give our model the enough information in order to do good predictions. \n",
    "\n",
    "**Observation: We cannot follow like this since we need to consider some categorical variables. REMEMBER to one_shot_encode them before going on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Destination_airport column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the same transformations we just applied for the destination airport column. We first add the coordinates of the airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['LONGITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LONGITUDE'])\n",
    "flights['LATITUDE_D'] = flights['DESTINATION_AIRPORT'].apply(lambda x: airports.loc[x]['LATITUDE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13113\n",
      "13123\n"
     ]
    }
   ],
   "source": [
    "# we observe some nans are generated and we remove them \n",
    "counter = flights[\"LONGITUDE_D\"].isna() == False\n",
    "count_false = sum(counter)\n",
    "print(count_false)\n",
    "print(len(flights[\"LONGITUDE_D\"]))\n",
    "\n",
    "flights = flights.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And classify the destination airports based on their coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['GR_D'] = ''\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] >= 37), 'GR_D'] = 'UPPER_LEFT'\n",
    "flights.loc[(flights['LONGITUDE_D'] >= -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_RIGHT'\n",
    "flights.loc[(flights['LONGITUDE_D'] < -100) & (flights['LATITUDE_D'] < 37), 'GR_D'] = 'BOTTOM_LEFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UPPER_RIGHT', 'UPPER_LEFT', 'BOTTOM_RIGHT', 'BOTTOM_LEFT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights[\"GR_D\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of the airports classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our airports classification, we will plot them over a USA map and assing a color to each quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gmplot\n",
    "\n",
    "# # Initialize the map with the first airport as the center point\n",
    "# gmap = gmplot.GoogleMapPlotter(flights.iloc[0]['LATITUDE_O'], flights.iloc[0]['LONGITUDE_O'], zoom=3)\n",
    "\n",
    "# # Define the color mapping for different values of GR_D\n",
    "# color_map = {\n",
    "#     'UPPER_RIGHT': 'red',\n",
    "#     'UPPER_LEFT': 'green',\n",
    "#     'BOTTOM_RIGHT': 'blue',\n",
    "#     'BOTTOM_LEFT': 'purple'\n",
    "# }\n",
    "\n",
    "# # Plot each airport with its corresponding color\n",
    "# for i, row in flights.iterrows():\n",
    "#     color = color_map.get(row['GR_D'], 'gray')\n",
    "#     gmap.marker(row['LATITUDE_O'], row['LONGITUDE_O'], color=color)\n",
    "\n",
    "# # Draw the map and save it to an HTML file\n",
    "# gmap.draw('airports_map.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify scheduled_time and departure_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset has a few columns with values ranging from 0000 to 2359. This values are found in columns scheduled_time and departure_time and they represent an hour and minutes. The format is hhmm (i.e.: 1915 is 19:15).\n",
    "\n",
    "Once again, having 2359 categories is unfeasible. The solution we have come up with is dividing the day between daytime and nightime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to classify the time segment based on the input time\n",
    "def classify_time(time):\n",
    "    sunrise = 600.0   # Define the time of sunrise as 6:00 am (in decimal format)\n",
    "    sunset = 1800.0   # Define the time of sunset as 6:00 pm (in decimal format)\n",
    "    \n",
    "    if time >= sunrise and time < sunset:\n",
    "        return 'Daytime'\n",
    "    else:\n",
    "        return 'Nighttime'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined the function that will allow us to apply the classification, we apply it to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change formatting of SCHEDULED_ARRIVAL and DEPARTURE_TIME\n",
    "flights['ArrivalDayNight'] = flights['SCHEDULED_ARRIVAL'].apply(classify_time)\n",
    "flights['DepartureDayNight'] = flights['DEPARTURE_TIME'].apply(classify_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing and processing with all categorical data, we are left with a few unwanted columns, those ones we have used to create new columns. We will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ARRIVAL_DELAY', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE',\n",
       "       'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_ARRIVAL',\n",
       "       'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF',\n",
       "       'SCHEDULED_TIME', 'DISTANCE', 'Q_YEAR', 'DELAYED', 'FORTNIGHT',\n",
       "       'WEEK_INFO', 'AC', 'LONGITUDE_O', 'LATITUDE_O', 'GR_O', 'LONGITUDE_D',\n",
       "       'LATITUDE_D', 'GR_D', 'ArrivalDayNight', 'DepartureDayNight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()\n",
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights[['DELAYED','DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE', 'Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O',\n",
    "       'GR_D', 'ArrivalDayNight', 'DepartureDayNight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DELAYED', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF',\n",
       "       'SCHEDULED_TIME', 'DISTANCE', 'Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC',\n",
       "       'GR_O', 'GR_D', 'ArrivalDayNight', 'DepartureDayNight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "In order to do the process correctly, normalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n",
    "\n",
    "This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance. In our case, That is what we are going to observe right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X as the features (input variables)\n",
    "X = flights.drop(\"DELAYED\", axis=1)  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Define y as the target variable\n",
    "y = flights[\"DELAYED\"]  # Replace \"target_variable_column\" with the actual name of the target column\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>],\n",
       "       [<AxesSubplot: title={'center': 'WHEELS_OFF'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_TIME'}>],\n",
       "       [<AxesSubplot: title={'center': 'DISTANCE'}>,\n",
       "        <AxesSubplot: title={'center': 'Q_YEAR'}>]], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we observe that the distribution of the features are the same than that of the whole dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have processed all our data, it migh be interesting to, once again, visualize the distribution from some of the new columns we have added to the dataset and study if we can extract any knowledge from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIhCAYAAAC48qAWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABIoklEQVR4nO3de1xVVf7/8fdJbkJwvAKSeI1M85o2iF008ZpoZY6VM3y1MbM0jdRHk/ltpGa+ak6alXnJ8jLeK6XLOENqGmmiqUnmJccmNSsQSwS8oeL6/TE/9nTkIiBLQF/Px+M8Hp21P2fvtRf7rHyzz1m4jDFGAAAAAIAydV15dwAAAAAArkaELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AVs2fP18ul8t5+Pn5KTQ0VHfffbcmTpyo9PT0fK+Jj4+Xy+Uq0XFOnTql+Ph4ffrppyV6XUHHatCggWJiYkq0n0tZsmSJpk2bVuA2l8ul+Pj4Mj1eWfvkk0/Url07BQQEyOVy6f333y+0dseOHerYsaPcbrdcLpemTZumTz/9VC6Xy+PnU5qfc56862rbtm2XrJ0xY4bmz59fquMUJO9c3nvvvTLbZ2WyZ88excfH6+DBg+XdlQIVdP0VpKg5I+/a/Pnnn+12FsBVz6u8OwDg2jBv3jzdfPPNOnfunNLT07Vx40a99NJLevnll7V8+XJ16dLFqX300UfVo0ePEu3/1KlTeuGFFyRJnTp1KvbrSnOs0liyZIl27dqluLi4fNuSk5NVt25d630oLWOM+vfvr5tuukkffvihAgIC1KRJk0Lr//CHP+jkyZNatmyZqlevrgYNGuibb77JV3elxn7GjBmqVauWBg0aZP1Y14I9e/bohRdeUKdOndSgQYPy7k4+BV1/BSntnAEAJUHYAnBFNG/eXO3atXOeP/DAA3r66ad1xx13qG/fvtq/f79CQkIkSXXr1rUePk6dOiV/f/8rcqxLad++fbke/1J++uknHTt2TPfff7+io6MvWb9r1y4NGTJEPXv2dNoKClsVYexRfOfOnSv1ncjiyHtPXq6Crj9cWm5urs6fPy9fX9/y7gpwVeFjhADKTb169TRlyhRlZ2dr9uzZTntBHy9bt26dOnXqpJo1a6pq1aqqV6+eHnjgAZ06dUoHDx5U7dq1JUkvvPCC85HFvDsZefv78ssv1a9fP1WvXl2NGzcu9Fh5EhIS1LJlS/n5+alRo0Z67bXXPLbnfZTt4o9TXfyRuU6dOmnVqlU6dOiQx0cq8xT0McJdu3bp3nvvVfXq1eXn56fWrVtrwYIFBR5n6dKlGjdunMLCwhQUFKQuXbpo3759hQ/8r2zcuFHR0dEKDAyUv7+/OnTooFWrVjnb4+PjnUD0xz/+US6Xq9A7BXnjcf78ec2cOTPfeV6soLHPycnR6NGjFRoaKn9/f911113avn27GjRoUOCdqezsbD3xxBOqVauWatasqb59++qnn35ytjdo0EC7d+9WUlKS05+8/l+4cEF/+ctf1KRJE1WtWlXVqlVTy5Yt9eqrrxZr7M6cOaNRo0YpNDRUVatWVceOHbVjxw5n+8KFC+VyuZScnJzvtS+++KK8vb09+lqQVatWqXXr1vL19VXDhg318ssv5xu3gwcPyuVyFfhRyYuvrW+//VaPPPKIIiIi5O/vrxtuuEG9e/fW119/7fG6vGtr4cKFGj16tG644Qb5+vrqrbfe0m9/+1tJ0t133+2M6a+PvXbtWkVHRysoKEj+/v66/fbb9cknn3jsv6j3ZGEu9Z4oyfV3qTkjz5EjR/Twww/L7XYrJCREf/jDH5SZmelRY4zRjBkz1Lp1a1WtWlXVq1dXv3799N133xV5Phs2bHDevxf729/+JpfLpa1btzpt27ZtU58+fVSjRg35+fmpTZs2eueddzxed/ToUQ0bNkzNmjXT9ddfr+DgYHXu3FkbNmzId/4ul0uTJ0/WX/7yFzVs2FC+vr5av359kX0GUHKELQDl6p577lGVKlX02WefFVpz8OBB9erVSz4+Ppo7d64SExM1adIkBQQE6OzZs6pTp44SExMlSYMHD1ZycrKSk5P1/PPPe+ynb9++uvHGG/Xuu+9q1qxZRfYrJSVFcXFxevrpp5WQkKAOHTroqaee0ssvv1zic5wxY4Zuv/12hYaGOn0r6B/gefbt26cOHTpo9+7deu2117Ry5Uo1a9ZMgwYN0uTJk/PVP/fcczp06JDeeustvfnmm9q/f7969+6t3NzcIvuVlJSkzp07KzMzU2+//baWLl2qwMBA9e7dW8uXL5f0n4/6rVy5UpI0YsQIJScnKyEhocD99erVyzmvfv36XfI8C/LII49o2rRpeuSRR/TBBx/ogQce0P3336/jx48XWP/oo4/K29tbS5Ys0eTJk/Xpp5/q97//vbM9ISFBjRo1Ups2bZz+5PV/8uTJio+P18MPP6xVq1Zp+fLlGjx4cKHHuthzzz2n7777Tm+99Zbeeust/fTTT+rUqZPzj+wHH3xQoaGheuONNzxed/78ec2ePVv333+/wsLCCt3/J598onvvvVeBgYFatmyZ/vrXv+qdd97RvHnzitW/gvz000+qWbOmJk2apMTERL3xxhvy8vJSZGRkgQF97Nix+v777zVr1ix99NFHuv/++zVhwgRJ0htvvOGMaa9evSRJixYtUrdu3RQUFKQFCxbonXfeUY0aNdS9e/d8gUsq/nuyOO+Jklx/xZ0zHnjgAd10001asWKFnn32WS1ZskRPP/20R83QoUMVFxenLl266P3339eMGTO0e/dudejQQUeOHCn0nO688061adMm3/UhSdOnT9dtt92m2267TZK0fv163X777Tp+/LhmzZqlDz74QK1bt9aDDz7oEXSPHTsmSRo/frxWrVqlefPmqVGjRurUqVOB30177bXXtG7dOr388sv65z//qZtvvrnQ/gIoJQMAFs2bN89IMlu3bi20JiQkxDRt2tR5Pn78ePPr6em9994zkkxKSkqh+zh69KiRZMaPH59vW97+/vSnPxW67dfq169vXC5XvuN17drVBAUFmZMnT3qc24EDBzzq1q9fbySZ9evXO229evUy9evXL7DvF/f7oYceMr6+vub777/3qOvZs6fx9/c3x48f9zjOPffc41H3zjvvGEkmOTm5wOPlad++vQkODjbZ2dlO2/nz503z5s1N3bp1zYULF4wxxhw4cMBIMn/961+L3N+vz2f48OEebQWNycVjv3v3biPJ/PGPf/R47dKlS40kM3DgQKctb+yHDRvmUTt58mQjyaSmpjptt9xyi+nYsWO+fsbExJjWrVsX65wKOpdbb73VGSNjjDl48KDx9vY2jz76qMc5+vj4mCNHjjhty5cvN5JMUlJSkceJjIw0YWFh5vTp005bVlaWqVGjhse45f185s2bl28fhb0n8pw/f96cPXvWREREmKeffjrfOd511135XvPuu+/m+1kaY8zJkydNjRo1TO/evT3ac3NzTatWrcxvfvMbp62o92RBivueMKbg668gxZkzJk+e7NE+bNgw4+fn5/zck5OTjSQzZcoUj7rDhw+bqlWrmmeeeabIPuRdxzt27HDavvjiCyPJLFiwwGm7+eabTZs2bcy5c+c8Xh8TE2Pq1KljcnNzC9z/+fPnzblz50x0dLS5//77nfa8a6Zx48bm7NmzRfYRwOXhzhaAcmeMKXJ769at5ePjo8cee0wLFiy45MdzCvPAAw8Uu/aWW25Rq1atPNoGDBigrKwsffnll6U6fnGtW7dO0dHRCg8P92gfNGiQTp06le+39X369PF43rJlS0nSoUOHCj3GyZMntWXLFvXr10/XX3+9016lShXFxsbqhx9+KPZHEctKUlKSJKl///4e7f369ZOXV8FfMS7Nuef5zW9+o6+++krDhg3Txx9/rKysrBL1d8CAAR4fU6tfv746dOjg8VGsJ554QpI0Z84cp2369Olq0aKF7rrrrkL3ffLkSW3dulV9+/aVn5+f055357G0zp8/rwkTJqhZs2by8fGRl5eXfHx8tH//fu3duzdffUneM5s2bdKxY8c0cOBAnT9/3nlcuHBBPXr00NatW3Xy5MlS7b+k74myUtD1debMGWcV1b///e9yuVz6/e9/73HOoaGhatWq1SVXR3344YcVHBzscXfr9ddfV+3atfXggw9K+s9HP7/55hv97ne/kySP49xzzz1KTU31eK/OmjVLt956q/z8/OTl5SVvb2998sknBf58+/TpI29v71KNDYDiIWwBKFcnT57UL7/8UuTHqRo3bqy1a9cqODhYw4cPV+PGjdW4ceNif7cmT506dYpdGxoaWmjbL7/8UqLjltQvv/xSYF/zxuji49esWdPjed4X3E+fPl3oMTIyMmSMKdFxbMs7Xt5CKXm8vLzynWOe0px7nrFjx+rll1/W5s2b1bNnT9WsWVPR0dHFWk5eKvwa+fW4hYSE6MEHH9Ts2bOVm5urnTt3asOGDXryySeL3HdGRoYuXLhQ5HVYGqNGjdLzzz+v++67Tx999JG2bNmirVu3qlWrVgWOWUneM3kfmevXr5+8vb09Hi+99JKMMc7H3Eq6/5K+J8rKpa6vI0eOyBijkJCQfOe8efPmSy4d7+vrq6FDh2rJkiU6fvy4jh49qnfeeUePPvqoc6y8cR0zZky+YwwbNkySnONMnTpVTzzxhCIjI7VixQpt3rxZW7duVY8ePS775wugdFiNEEC5WrVqlXJzcy+59PKdd96pO++8U7m5udq2bZtef/11xcXFKSQkRA899FCxjlWSldTS0tIKbcv7B1jeHYecnByPusv92zw1a9ZUampqvva8xRRq1ap1WfuXpOrVq+u6666zfpySyBvXI0eO6IYbbnDaz58/b+Uf015eXho1apRGjRql48ePa+3atXruuefUvXt3HT58+JIr4xV2jVz8D/SnnnpKCxcu1AcffKDExERVq1bNuUtRmOrVq8vlchV5HeYp7DosaMwWLVqk//mf/3G+d5Xn559/VrVq1fLVl+Q9k3e9vP7664WusHlxkC7u/q/Ee6I0atWqJZfLpQ0bNhS4il9xVvZ74oknNGnSJM2dO1dnzpzR+fPn9fjjj3scQ/rPLwf69u1b4D7y/hTDokWL1KlTJ82cOdNje3Z2doGvs7m6JID/4M4WgHLz/fffa8yYMXK73Ro6dGixXlOlShVFRkY6H7vJ+0hfSe5oFMfu3bv11VdfebQtWbJEgYGBuvXWWyXJWdVu586dHnUffvhhvv35+voWu2/R0dFat25dvpXq/va3v8nf379MlooPCAhQZGSkVq5c6dGvCxcuaNGiRapbt65uuummyz5OSeR9rC5vcY487733ns6fP1/q/RZn7KtVq6Z+/fpp+PDhOnbsWLH+YO/SpUs9PgJ76NAhbdq0Kd8vDtq2basOHTropZde0uLFizVo0CAFBAQUue+AgAD95je/0cqVK3XmzBmnPTs7Wx999JFHbUhIiPz8/PJdhx988EG+/bpcrnwBYNWqVfrxxx+L7M+vFfZeu/3221WtWjXt2bNH7dq1K/Dh4+NT7OP8mo33RFnMGTExMTLG6McffyzwfFu0aHHJfdSpU0e//e1vNWPGDM2aNUu9e/dWvXr1nO1NmjRRRESEvvrqq0LHNTAwUFLBP9+dO3da+5glgEvjzhaAK2LXrl3O9wzS09O1YcMGzZs3T1WqVFFCQoKzDHNBZs2apXXr1qlXr16qV6+ezpw5o7lz50qS88eQAwMDVb9+fX3wwQeKjo5WjRo1VKtWrVL/0dWwsDD16dNH8fHxqlOnjhYtWqQ1a9bopZdecu543HbbbWrSpInGjBmj8+fPq3r16kpISNDGjRvz7a9FixZauXKlZs6cqbZt2+q6667z+LtjvzZ+/Hj9/e9/1913360//elPqlGjhhYvXqxVq1Zp8uTJcrvdpTqni02cOFFdu3bV3XffrTFjxsjHx0czZszQrl27tHTp0iv+W+9bbrlFDz/8sKZMmaIqVaqoc+fO2r17t6ZMmSK3263rrivd7wdbtGihZcuWafny5WrUqJH8/PzUokUL9e7d2/n7b7Vr19ahQ4c0bdo01a9fXxEREZfcb3p6uu6//34NGTJEmZmZGj9+vPz8/DR27Nh8tU899ZQefPBBuVwu56Nfl/LnP/9ZPXr0UNeuXTV69Gjl5ubqpZdeUkBAgMfH8fK+MzR37lw1btxYrVq10hdffKElS5bk22dMTIzmz5+vm2++WS1bttT27dv117/+tUR/76x58+aSpDfffFOBgYHy8/NTw4YNVbNmTb3++usaOHCgjh07pn79+ik4OFhHjx7VV199paNHj+a741JcNt4TZTFn3H777Xrsscf0yCOPaNu2bbrrrrsUEBCg1NRUbdy4US1atHC+t1eUp556SpGRkZJU4GqTs2fPVs+ePdW9e3cNGjRIN9xwg44dO6a9e/fqyy+/1LvvvivpPz/fP//5zxo/frw6duyoffv26cUXX1TDhg0v6xcWAC5Dea7OAeDql7faVt7Dx8fHBAcHm44dO5oJEyaY9PT0fK+5eJW65ORkc//995v69esbX19fU7NmTdOxY0fz4Ycferxu7dq1pk2bNsbX19dj9bq8/R09evSSxzLmP6sR9urVy7z33nvmlltuMT4+PqZBgwZm6tSp+V7/r3/9y3Tr1s0EBQWZ2rVrmxEjRphVq1blW63t2LFjpl+/fqZatWrG5XJ5HFMFrIj29ddfm969exu32218fHxMq1at8q02l7di3LvvvuvRXtTqdBfbsGGD6dy5swkICDBVq1Y17du3Nx999FGB+7O9GqExxpw5c8aMGjXKBAcHGz8/P9O+fXuTnJxs3G63x2p5ha1yWdBxDh48aLp162YCAwONJGdVyClTppgOHTqYWrVqGR8fH1OvXj0zePBgc/DgwSLPL+8YCxcuNCNHjjS1a9c2vr6+5s477zTbtm0r8DU5OTnG19fX9OjRo8h9X+zDDz80LVu2dPo3adKkAsctMzPTPProoyYkJMQEBASY3r17m4MHD+a7tjIyMszgwYNNcHCw8ff3N3fccYfZsGGD6dixo8eKjYVdW3mmTZtmGjZsaKpUqZLvWktKSjK9evUyNWrUMN7e3uaGG24wvXr18thXUe/JwhTnPWFM8VcjNKbkc0ZhK5DOnTvXREZGOu+jxo0bm//5n/8p9HooSIMGDTxWZb3YV199Zfr372+Cg4ONt7e3CQ0NNZ07dzazZs1yanJycsyYMWPMDTfcYPz8/Mytt95q3n//fTNw4ECP1VBL+p4GUHouYy6xDBgAAOVo06ZNuv3227V48WINGDCgvLtTKh999JH69OmjVatW6Z577rmsfcXHx+uFF1645CqeqDx27typVq1a6Y033ij2nU8AlQMfIwQAVBhr1qxRcnKy2rZtq6pVq+qrr77SpEmTFBERUejiABXZnj17dOjQIY0ePVqtW7dWz549y7tLqED+/e9/69ChQ3ruuedUp04dDRo0qLy7BKCMEbYAABVGUFCQVq9erWnTpik7O1u1atVSz549NXHiRI+/N1VZDBs2TJ9//rluvfVWLViwgNXf4OHPf/6zFi5cqKZNm+rdd9+95AqYACofPkYIAAAAABaw9DsAAAAAWEDYAgAAAAALCFsAAAAAYAELZBTThQsX9NNPPykwMJAvOAMAAADXMGOMsrOzFRYWpuuuK/z+FWGrmH766SeFh4eXdzcAAAAAVBCHDx9W3bp1C91O2CqmwMBASf8Z0KCgoHLuDQAAAIDykpWVpfDwcCcjFIawVUx5Hx0MCgoibAEAAAC45NeLWCADAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwoFzDVnx8vFwul8cjNDTU2W6MUXx8vMLCwlS1alV16tRJu3fv9thHTk6ORowYoVq1aikgIEB9+vTRDz/84FGTkZGh2NhYud1uud1uxcbG6vjx41fiFAEAAABco8r9ztYtt9yi1NRU5/H111872yZPnqypU6dq+vTp2rp1q0JDQ9W1a1dlZ2c7NXFxcUpISNCyZcu0ceNGnThxQjExMcrNzXVqBgwYoJSUFCUmJioxMVEpKSmKjY29oucJAAAA4NriVe4d8PLyuJuVxxijadOmady4cerbt68kacGCBQoJCdGSJUs0dOhQZWZm6u2339bChQvVpUsXSdKiRYsUHh6utWvXqnv37tq7d68SExO1efNmRUZGSpLmzJmjqKgo7du3T02aNLlyJwsAAADgmlHud7b279+vsLAwNWzYUA899JC+++47SdKBAweUlpambt26ObW+vr7q2LGjNm3aJEnavn27zp0751ETFham5s2bOzXJyclyu91O0JKk9u3by+12OzUFycnJUVZWlscDAAAAAIqrXMNWZGSk/va3v+njjz/WnDlzlJaWpg4dOuiXX35RWlqaJCkkJMTjNSEhIc62tLQ0+fj4qHr16kXWBAcH5zt2cHCwU1OQiRMnOt/xcrvdCg8Pv6xzBQAAAHBtKdew1bNnTz3wwANq0aKFunTpolWrVkn6z8cF87hcLo/XGGPytV3s4pqC6i+1n7FjxyozM9N5HD58uFjnBAAAAABSBfgY4a8FBASoRYsW2r9/v/M9rovvPqWnpzt3u0JDQ3X27FllZGQUWXPkyJF8xzp69Gi+u2a/5uvrq6CgII8HAAAAABRXhQpbOTk52rt3r+rUqaOGDRsqNDRUa9ascbafPXtWSUlJ6tChgySpbdu28vb29qhJTU3Vrl27nJqoqChlZmbqiy++cGq2bNmizMxMpwYAAAAAylq5rkY4ZswY9e7dW/Xq1VN6err+8pe/KCsrSwMHDpTL5VJcXJwmTJigiIgIRUREaMKECfL399eAAQMkSW63W4MHD9bo0aNVs2ZN1ahRQ2PGjHE+lihJTZs2VY8ePTRkyBDNnj1bkvTYY48pJiaGlQgBACXW4NlV5d2FCuvgpF7l3QUAqFDKNWz98MMPevjhh/Xzzz+rdu3aat++vTZv3qz69etLkp555hmdPn1aw4YNU0ZGhiIjI7V69WoFBgY6+3jllVfk5eWl/v376/Tp04qOjtb8+fNVpUoVp2bx4sUaOXKks2phnz59NH369Ct7sgAAAACuKS5jjCnvTlQGWVlZcrvdyszM5PtbAHAN485W4bizBeBaUdxsUKG+swUAAAAAVwvCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwAKv8u4ASqfBs6vKuwsV0sFJvcq7CwAAAIAk7mwBAAAAgBWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAggoTtiZOnCiXy6W4uDinzRij+Ph4hYWFqWrVqurUqZN2797t8bqcnByNGDFCtWrVUkBAgPr06aMffvjBoyYjI0OxsbFyu91yu92KjY3V8ePHr8BZAQAAALhWVYiwtXXrVr355ptq2bKlR/vkyZM1depUTZ8+XVu3blVoaKi6du2q7OxspyYuLk4JCQlatmyZNm7cqBMnTigmJka5ublOzYABA5SSkqLExEQlJiYqJSVFsbGxV+z8AAAAAFx7yj1snThxQr/73e80Z84cVa9e3Wk3xmjatGkaN26c+vbtq+bNm2vBggU6deqUlixZIknKzMzU22+/rSlTpqhLly5q06aNFi1apK+//lpr166VJO3du1eJiYl66623FBUVpaioKM2ZM0d///vftW/fvnI5ZwAAAABXv3IPW8OHD1evXr3UpUsXj/YDBw4oLS1N3bp1c9p8fX3VsWNHbdq0SZK0fft2nTt3zqMmLCxMzZs3d2qSk5PldrsVGRnp1LRv315ut9upKUhOTo6ysrI8HgAAAABQXF7lefBly5bpyy+/1NatW/NtS0tLkySFhIR4tIeEhOjQoUNOjY+Pj8cdsbyavNenpaUpODg43/6Dg4OdmoJMnDhRL7zwQslOCAAAAAD+v3K7s3X48GE99dRTWrRokfz8/Aqtc7lcHs+NMfnaLnZxTUH1l9rP2LFjlZmZ6TwOHz5c5DEBAAAA4NfKLWxt375d6enpatu2rby8vOTl5aWkpCS99tpr8vLycu5oXXz3KT093dkWGhqqs2fPKiMjo8iaI0eO5Dv+0aNH8901+zVfX18FBQV5PAAAAACguMotbEVHR+vrr79WSkqK82jXrp1+97vfKSUlRY0aNVJoaKjWrFnjvObs2bNKSkpShw4dJElt27aVt7e3R01qaqp27drl1ERFRSkzM1NffPGFU7NlyxZlZmY6NQAAAABQ1srtO1uBgYFq3ry5R1tAQIBq1qzptMfFxWnChAmKiIhQRESEJkyYIH9/fw0YMECS5Ha7NXjwYI0ePVo1a9ZUjRo1NGbMGLVo0cJZcKNp06bq0aOHhgwZotmzZ0uSHnvsMcXExKhJkyZX8IwBAAAAXEvKdYGMS3nmmWd0+vRpDRs2TBkZGYqMjNTq1asVGBjo1Lzyyivy8vJS//79dfr0aUVHR2v+/PmqUqWKU7N48WKNHDnSWbWwT58+mj59+hU/HwAAAADXDpcxxpR3JyqDrKwsud1uZWZmVojvbzV4dlV5d6FCOjipV3l3AcBVjvm3cMzBAK4Vxc0G5f53tgAAAADgakTYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALPAq7w4AAAAA+K8Gz64q7y5UWAcn9SrvLpQId7YAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAgnINWzNnzlTLli0VFBSkoKAgRUVF6Z///Kez3Rij+Ph4hYWFqWrVqurUqZN2797tsY+cnByNGDFCtWrVUkBAgPr06aMffvjBoyYjI0OxsbFyu91yu92KjY3V8ePHr8QpAgAAALhGlWvYqlu3riZNmqRt27Zp27Zt6ty5s+69914nUE2ePFlTp07V9OnTtXXrVoWGhqpr167Kzs529hEXF6eEhAQtW7ZMGzdu1IkTJxQTE6Pc3FynZsCAAUpJSVFiYqISExOVkpKi2NjYK36+AAAAAK4dXuV58N69e3s8/7//+z/NnDlTmzdvVrNmzTRt2jSNGzdOffv2lSQtWLBAISEhWrJkiYYOHarMzEy9/fbbWrhwobp06SJJWrRokcLDw7V27Vp1795de/fuVWJiojZv3qzIyEhJ0pw5cxQVFaV9+/apSZMmV/akAQAAAFwTKsx3tnJzc7Vs2TKdPHlSUVFROnDggNLS0tStWzenxtfXVx07dtSmTZskSdu3b9e5c+c8asLCwtS8eXOnJjk5WW632wlaktS+fXu53W6npiA5OTnKysryeAAAAABAcZV72Pr66691/fXXy9fXV48//rgSEhLUrFkzpaWlSZJCQkI86kNCQpxtaWlp8vHxUfXq1YusCQ4Oznfc4OBgp6YgEydOdL7j5Xa7FR4eflnnCQAAAODaUu5hq0mTJkpJSdHmzZv1xBNPaODAgdqzZ4+z3eVyedQbY/K1XezimoLqL7WfsWPHKjMz03kcPny4uKcEAAAAAOUftnx8fHTjjTeqXbt2mjhxolq1aqVXX31VoaGhkpTv7lN6erpztys0NFRnz55VRkZGkTVHjhzJd9yjR4/mu2v2a76+vs4qiXkPAAAAACiucg9bFzPGKCcnRw0bNlRoaKjWrFnjbDt79qySkpLUoUMHSVLbtm3l7e3tUZOamqpdu3Y5NVFRUcrMzNQXX3zh1GzZskWZmZlODQAAAACUtXJdjfC5555Tz549FR4eruzsbC1btkyffvqpEhMT5XK5FBcXpwkTJigiIkIRERGaMGGC/P39NWDAAEmS2+3W4MGDNXr0aNWsWVM1atTQmDFj1KJFC2d1wqZNm6pHjx4aMmSIZs+eLUl67LHHFBMTw0qEAAAAAKwp17B15MgRxcbGKjU1VW63Wy1btlRiYqK6du0qSXrmmWd0+vRpDRs2TBkZGYqMjNTq1asVGBjo7OOVV16Rl5eX+vfvr9OnTys6Olrz589XlSpVnJrFixdr5MiRzqqFffr00fTp06/syQIAAAC4priMMaa8O1EZZGVlye12KzMzs0J8f6vBs6vKuwsV0sFJvcq7CwCucsy/hWMOBsoG80zhKso8U9xsUOG+swUAAAAAVwPCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWFCqsNWoUSP98ssv+dqPHz+uRo0aXXanAAAAAKCyK1XYOnjwoHJzc/O15+Tk6Mcff7zsTgEAAABAZedVkuIPP/zQ+e+PP/5YbrfbeZ6bm6tPPvlEDRo0KLPOAQAAAEBlVaKwdd9990mSXC6XBg4c6LHN29tbDRo00JQpU8qscwAAAABQWZUobF24cEGS1LBhQ23dulW1atWy0ikAAAAAqOxKFLbyHDhwoKz7AQAAAABXlVKFLUn65JNP9Mknnyg9Pd2545Vn7ty5l90xAAAAAKjMShW2XnjhBb344otq166d6tSpI5fLVdb9AgAAAIBKrVRha9asWZo/f75iY2PLuj8AAAAAcFUo1d/ZOnv2rDp06FDWfQEAAACAq0apwtajjz6qJUuWlHVfAAAAAOCqUaqPEZ45c0Zvvvmm1q5dq5YtW8rb29tj+9SpU8ukcwAAAABQWZUqbO3cuVOtW7eWJO3atctjG4tlAAAAAEApw9b69evLuh8AAAAAcFUp1Xe2AAAAAABFK9WdrbvvvrvIjwuuW7eu1B0CAAAAgKtBqcJW3ve18pw7d04pKSnatWuXBg4cWBb9AgAAAIBKrVRh65VXXimwPT4+XidOnLisDgEAAADA1aBMv7P1+9//XnPnzi3LXQIAAABApVSmYSs5OVl+fn5luUsAAAAAqJRK9THCvn37ejw3xig1NVXbtm3T888/XyYdAwAAAIDKrFRhy+12ezy/7rrr1KRJE7344ovq1q1bmXQMAAAAACqzUoWtefPmlXU/AAAAAOCqUqqwlWf79u3au3evXC6XmjVrpjZt2pRVvwAAAACgUitV2EpPT9dDDz2kTz/9VNWqVZMxRpmZmbr77ru1bNky1a5du6z7CQAAAACVSqlWIxwxYoSysrK0e/duHTt2TBkZGdq1a5eysrI0cuTIsu4jAAAAAFQ6pbqzlZiYqLVr16pp06ZOW7NmzfTGG2+wQAYAAAAAqJR3ti5cuCBvb+987d7e3rpw4cJldwoAAAAAKrtSha3OnTvrqaee0k8//eS0/fjjj3r66acVHR1dZp0DAAAAgMqqVGFr+vTpys7OVoMGDdS4cWPdeOONatiwobKzs/X666+XdR8BAAAAoNIp1Xe2wsPD9eWXX2rNmjX65ptvZIxRs2bN1KVLl7LuHwAAAABUSiW6s7Vu3To1a9ZMWVlZkqSuXbtqxIgRGjlypG677Tbdcsst2rBhg5WOAgAAAEBlUqKwNW3aNA0ZMkRBQUH5trndbg0dOlRTp04ts84BAAAAQGVVorD11VdfqUePHoVu79atm7Zv337ZnQIAAACAyq5EYevIkSMFLvmex8vLS0ePHr3sTgEAAABAZVeisHXDDTfo66+/LnT7zp07VadOncvuFAAAAABUdiUKW/fcc4/+9Kc/6cyZM/m2nT59WuPHj1dMTEyZdQ4AAAAAKqsSLf3+v//7v1q5cqVuuukmPfnkk2rSpIlcLpf27t2rN954Q7m5uRo3bpytvgIAAABApVGisBUSEqJNmzbpiSee0NixY2WMkSS5XC51795dM2bMUEhIiJWOAgAAAEBlUuI/aly/fn394x//UEZGhr799lsZYxQREaHq1avb6B8AAAAAVEolDlt5qlevrttuu60s+wIAAAAAV40SLZABAAAAACgewhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWFCuYWvixIm67bbbFBgYqODgYN13333at2+fR40xRvHx8QoLC1PVqlXVqVMn7d6926MmJydHI0aMUK1atRQQEKA+ffrohx9+8KjJyMhQbGys3G633G63YmNjdfz4cdunCAAAAOAaVa5hKykpScOHD9fmzZu1Zs0anT9/Xt26ddPJkyedmsmTJ2vq1KmaPn26tm7dqtDQUHXt2lXZ2dlOTVxcnBISErRs2TJt3LhRJ06cUExMjHJzc52aAQMGKCUlRYmJiUpMTFRKSopiY2Ov6PkCAAAAuHZ4lefBExMTPZ7PmzdPwcHB2r59u+666y4ZYzRt2jSNGzdOffv2lSQtWLBAISEhWrJkiYYOHarMzEy9/fbbWrhwobp06SJJWrRokcLDw7V27Vp1795de/fuVWJiojZv3qzIyEhJ0pw5cxQVFaV9+/apSZMmV/bEAQAAAFz1KtR3tjIzMyVJNWrUkCQdOHBAaWlp6tatm1Pj6+urjh07atOmTZKk7du369y5cx41YWFhat68uVOTnJwst9vtBC1Jat++vdxut1NzsZycHGVlZXk8AAAAAKC4KkzYMsZo1KhRuuOOO9S8eXNJUlpamiQpJCTEozYkJMTZlpaWJh8fH1WvXr3ImuDg4HzHDA4OdmouNnHiROf7XW63W+Hh4Zd3ggAAAACuKRUmbD355JPauXOnli5dmm+by+XyeG6Mydd2sYtrCqovaj9jx45VZmam8zh8+HBxTgMAAAAAJFWQsDVixAh9+OGHWr9+verWreu0h4aGSlK+u0/p6enO3a7Q0FCdPXtWGRkZRdYcOXIk33GPHj2a765ZHl9fXwUFBXk8AAAAAKC4yjVsGWP05JNPauXKlVq3bp0aNmzosb1hw4YKDQ3VmjVrnLazZ88qKSlJHTp0kCS1bdtW3t7eHjWpqanatWuXUxMVFaXMzEx98cUXTs2WLVuUmZnp1AAAAABAWSrX1QiHDx+uJUuW6IMPPlBgYKBzB8vtdqtq1apyuVyKi4vThAkTFBERoYiICE2YMEH+/v4aMGCAUzt48GCNHj1aNWvWVI0aNTRmzBi1aNHCWZ2wadOm6tGjh4YMGaLZs2dLkh577DHFxMSwEiEAAAAAK8o1bM2cOVOS1KlTJ4/2efPmadCgQZKkZ555RqdPn9awYcOUkZGhyMhIrV69WoGBgU79K6+8Ii8vL/Xv31+nT59WdHS05s+frypVqjg1ixcv1siRI51VC/v06aPp06fbPUEAAAAA1yyXMcaUdycqg6ysLLndbmVmZlaI7281eHZVeXehQjo4qVd5dwHAVY75t3DMwUDZYJ4pXEWZZ4qbDSrEAhkAAAAAcLUhbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWFCuYeuzzz5T7969FRYWJpfLpffff99juzFG8fHxCgsLU9WqVdWpUyft3r3boyYnJ0cjRoxQrVq1FBAQoD59+uiHH37wqMnIyFBsbKzcbrfcbrdiY2N1/Phxy2cHAAAA4FpWrmHr5MmTatWqlaZPn17g9smTJ2vq1KmaPn26tm7dqtDQUHXt2lXZ2dlOTVxcnBISErRs2TJt3LhRJ06cUExMjHJzc52aAQMGKCUlRYmJiUpMTFRKSopiY2Otnx8AAACAa5dXeR68Z8+e6tmzZ4HbjDGaNm2axo0bp759+0qSFixYoJCQEC1ZskRDhw5VZmam3n77bS1cuFBdunSRJC1atEjh4eFau3atunfvrr179yoxMVGbN29WZGSkJGnOnDmKiorSvn371KRJkytzsgAAAACuKRX2O1sHDhxQWlqaunXr5rT5+vqqY8eO2rRpkyRp+/btOnfunEdNWFiYmjdv7tQkJyfL7XY7QUuS2rdvL7fb7dQUJCcnR1lZWR4PAAAAACiuChu20tLSJEkhISEe7SEhIc62tLQ0+fj4qHr16kXWBAcH59t/cHCwU1OQiRMnOt/xcrvdCg8Pv6zzAQAAAHBtqbBhK4/L5fJ4bozJ13axi2sKqr/UfsaOHavMzEzncfjw4RL2HAAAAMC1rMKGrdDQUEnKd/cpPT3dudsVGhqqs2fPKiMjo8iaI0eO5Nv/0aNH8901+zVfX18FBQV5PAAAAACguCps2GrYsKFCQ0O1Zs0ap+3s2bNKSkpShw4dJElt27aVt7e3R01qaqp27drl1ERFRSkzM1NffPGFU7NlyxZlZmY6NQAAAABQ1sp1NcITJ07o22+/dZ4fOHBAKSkpqlGjhurVq6e4uDhNmDBBERERioiI0IQJE+Tv768BAwZIktxutwYPHqzRo0erZs2aqlGjhsaMGaMWLVo4qxM2bdpUPXr00JAhQzR79mxJ0mOPPaaYmBhWIgQAAABgTbmGrW3btunuu+92no8aNUqSNHDgQM2fP1/PPPOMTp8+rWHDhikjI0ORkZFavXq1AgMDnde88sor8vLyUv/+/XX69GlFR0dr/vz5qlKlilOzePFijRw50lm1sE+fPoX+bS8AAAAAKAsuY4wp705UBllZWXK73crMzKwQ399q8Oyq8u5ChXRwUq/y7gKAqxzzb+GYg4GywTxTuIoyzxQ3G1TY72wBAAAAQGVG2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwALCFgAAAABYQNgCAAAAAAsIWwAAAABgAWELAAAAACwgbAEAAACABYQtAAAAALCAsAUAAAAAFhC2AAAAAMACwhYAAAAAWEDYAgAAAAALCFsAAAAAYAFhCwAAAAAsIGwBAAAAgAWELQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsOCaClszZsxQw4YN5efnp7Zt22rDhg3l3SUAAAAAV6lrJmwtX75ccXFxGjdunHbs2KE777xTPXv21Pfff1/eXQMAAABwFbpmwtbUqVM1ePBgPfroo2ratKmmTZum8PBwzZw5s7y7BgAAAOAq5FXeHbgSzp49q+3bt+vZZ5/1aO/WrZs2bdpU4GtycnKUk5PjPM/MzJQkZWVl2etoCVzIOVXeXaiQKsrPB8DVi/m3cMzBQNlgnilcRZln8vphjCmy7poIWz///LNyc3MVEhLi0R4SEqK0tLQCXzNx4kS98MIL+drDw8Ot9BFlwz2tvHsAANcu5mAAtlW0eSY7O1tut7vQ7ddE2Mrjcrk8nhtj8rXlGTt2rEaNGuU8v3Dhgo4dO6aaNWsW+porJSsrS+Hh4Tp8+LCCgoLKtS9XI8bXLsbXLsbXLsbXLsbXLsbXLsbXvoo0xsYYZWdnKywsrMi6ayJs1apVS1WqVMl3Fys9PT3f3a48vr6+8vX19WirVq2arS6WSlBQULlfaFczxtcuxtcuxtcuxtcuxtcuxtcuxte+ijLGRd3RynNNLJDh4+Ojtm3bas2aNR7ta9asUYcOHcqpVwAAAACuZtfEnS1JGjVqlGJjY9WuXTtFRUXpzTff1Pfff6/HH3+8vLsGAAAA4Cp0zYStBx98UL/88otefPFFpaamqnnz5vrHP/6h+vXrl3fXSszX11fjx4/P9zFHlA3G1y7G1y7G1y7G1y7G1y7G1y7G177KOMYuc6n1CgEAAAAAJXZNfGcLAAAAAK40whYAAAAAWEDYAgAAAAALCFsAAAAAYAFhq5x99tln6t27t8LCwuRyufT+++9f8jVJSUlq27at/Pz81KhRI82aNStfzYoVK9SsWTP5+vqqWbNmSkhIsND7iq+k47ty5Up17dpVtWvXVlBQkKKiovTxxx971MyfP18ulyvf48yZMxbPpGIq6fh++umnBY7dN99841HH9fsfJR3fQYMGFTi+t9xyi1PD9ftfEydO1G233abAwEAFBwfrvvvu0759+y75Oubg4inN+DIHF19pxpc5uPhKM77MwcU3c+ZMtWzZ0vnjxFFRUfrnP/9Z5Gsq69xL2CpnJ0+eVKtWrTR9+vRi1R84cED33HOP7rzzTu3YsUPPPfecRo4cqRUrVjg1ycnJevDBBxUbG6uvvvpKsbGx6t+/v7Zs2WLrNCqsko7vZ599pq5du+of//iHtm/frrvvvlu9e/fWjh07POqCgoKUmprq8fDz87NxChVaScc3z759+zzGLiIiwtnG9ftfJR3fV1991WNcDx8+rBo1aui3v/2tRx3X738kJSVp+PDh2rx5s9asWaPz58+rW7duOnnyZKGvYQ4uvtKML3Nw8ZVmfPMwB19aacaXObj46tatq0mTJmnbtm3atm2bOnfurHvvvVe7d+8usL5Sz70GFYYkk5CQUGTNM888Y26++WaPtqFDh5r27ds7z/v372969OjhUdO9e3fz0EMPlVlfK6PijG9BmjVrZl544QXn+bx584zb7S67jl0lijO+69evN5JMRkZGoTVcvwUrzfWbkJBgXC6XOXjwoNPG9Vu49PR0I8kkJSUVWsMcXHrFGd+CMAcXT3HGlzm49Epz/TIHl0z16tXNW2+9VeC2yjz3cmerkklOTla3bt082rp3765t27bp3LlzRdZs2rTpivXzanHhwgVlZ2erRo0aHu0nTpxQ/fr1VbduXcXExOT7rSuK1qZNG9WpU0fR0dFav369xzau37Lz9ttvq0uXLvn+eDvXb8EyMzMlKd/7/deYg0uvOON7Mebg4ivJ+DIHl1xprl/m4OLJzc3VsmXLdPLkSUVFRRVYU5nnXsJWJZOWlqaQkBCPtpCQEJ0/f14///xzkTVpaWlXrJ9XiylTpujkyZPq37+/03bzzTdr/vz5+vDDD7V06VL5+fnp9ttv1/79+8uxp5VDnTp19Oabb2rFihVauXKlmjRpoujoaH322WdODddv2UhNTdU///lPPfroox7tXL8FM8Zo1KhRuuOOO9S8efNC65iDS6e443sx5uDiKe74MgeXTmmuX+bgS/v66691/fXXy9fXV48//rgSEhLUrFmzAmsr89zrVa5HR6m4XC6P58aYfO0F1VzchqItXbpU8fHx+uCDDxQcHOy0t2/fXu3bt3ee33777br11lv1+uuv67XXXiuPrlYaTZo0UZMmTZznUVFROnz4sF5++WXdddddTjvX7+WbP3++qlWrpvvuu8+jneu3YE8++aR27typjRs3XrKWObjkSjK+eZiDi6+448scXDqluX6Zgy+tSZMmSklJ0fHjx7VixQoNHDhQSUlJhQauyjr3cmerkgkNDc2X0NPT0+Xl5aWaNWsWWXNx2kfhli9frsGDB+udd95Rly5diqy97rrrdNttt12Tv5UqC+3bt/cYO67fy2eM0dy5cxUbGysfH58ia7l+pREjRujDDz/U+vXrVbdu3SJrmYNLriTjm4c5uPhKM76/xhxctNKML3Nw8fj4+OjGG29Uu3btNHHiRLVq1UqvvvpqgbWVee4lbFUyUVFRWrNmjUfb6tWr1a5dO3l7exdZ06FDhyvWz8ps6dKlGjRokJYsWaJevXpdst4Yo5SUFNWpU+cK9O7qs2PHDo+x4/q9fElJSfr22281ePDgS9Zey9evMUZPPvmkVq5cqXXr1qlhw4aXfA1zcPGVZnwl5uDiKu34Xow5uGCXM77MwaVjjFFOTk6B2yr13HsFF+NAAbKzs82OHTvMjh07jCQzdepUs2PHDnPo0CFjjDHPPvusiY2Ndeq/++474+/vb55++mmzZ88e8/bbbxtvb2/z3nvvOTWff/65qVKlipk0aZLZu3evmTRpkvHy8jKbN2++4udX3ko6vkuWLDFeXl7mjTfeMKmpqc7j+PHjTk18fLxJTEw0//73v82OHTvMI488Yry8vMyWLVuu+PmVt5KO7yuvvGISEhLMv/71L7Nr1y7z7LPPGklmxYoVTg3X73+VdHzz/P73vzeRkZEF7pPr97+eeOIJ43a7zaeffurxfj916pRTwxxceqUZX+bg4ivN+DIHF19pxjcPc/CljR071nz22WfmwIEDZufOnea5554z1113nVm9erUx5uqaewlb5SxvGdaLHwMHDjTGGDNw4EDTsWNHj9d8+umnpk2bNsbHx8c0aNDAzJw5M99+3333XdOkSRPj7e1tbr75Zo+J9FpS0vHt2LFjkfXGGBMXF2fq1atnfHx8TO3atU23bt3Mpk2bruyJVRAlHd+XXnrJNG7c2Pj5+Znq1aubO+64w6xatSrffrl+/6M088Px48dN1apVzZtvvlngPrl+/6ugsZVk5s2b59QwB5deacaXObj4SjO+zMHFV9r5gTm4eP7whz+Y+vXrO+MQHR3tBC1jrq6512XM//92GQAAAACgzPCdLQAAAACwgLAFAAAAABYQtgAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQC4qhw+fFiDBw9WWFiYfHx8VL9+fT311FP65ZdfrsjxO3XqpLi4uCtyLABAxUbYAgBcNb777ju1a9dO//rXv7R06VJ9++23mjVrlj755BNFRUXp2LFj1o597ty5Mt3f2bNny3R/AIArj7AFALhqDB8+XD4+Plq9erU6duyoevXqqWfPnlq7dq1+/PFHjRs3TpLkcrn0/vvve7y2WrVqmj9/vvP8j3/8o2666Sb5+/urUaNGev755z0CVXx8vFq3bq25c+eqUaNG8vX11cCBA5WUlKRXX31VLpdLLpdLBw8elCTt2bNH99xzj66//nqFhIQoNjZWP//8s7O/Tp066cknn9SoUaNUq1Ytde3a1do4AQCuDMIWAOCqcOzYMX388ccaNmyYqlat6rEtNDRUv/vd77R8+XIZY4q1v8DAQM2fP1979uzRq6++qjlz5uiVV17xqPn222/1zjvvaMWKFUpJSdFrr72mqKgoDRkyRKmpqUpNTVV4eLhSU1PVsWNHtW7dWtu2bVNiYqKOHDmi/v37e+xvwYIF8vLy0ueff67Zs2df3oAAAMqdV3l3AACAsrB//34ZY9S0adMCtzdt2lQZGRk6evRosfb3v//7v85/N2jQQKNHj9by5cv1zDPPOO1nz57VwoULVbt2bafNx8dH/v7+Cg0NddpmzpypW2+9VRMmTHDa5s6dq/DwcP3rX//STTfdJEm68cYbNXny5OKdMACgwiNsAQCuCXl3tHx8fIpV/95772natGn69ttvdeLECZ0/f15BQUEeNfXr1/cIWoXZvn271q9fr+uvvz7ftn//+99O2GrXrl2x+gYAqBz4GCEA4Kpw4403yuVyac+ePQVu/+abb1S7dm1Vq1ZNLpcr38cJf/19rM2bN+uhhx5Sz5499fe//107duzQuHHj8i1aERAQUKy+XbhwQb1791ZKSorHY//+/brrrrtKvD8AQOXAnS0AwFWhZs2a6tq1q2bMmKGnn37a43tbaWlpWrx4sYYPHy5Jql27tlJTU53t+/fv16lTp5znn3/+uerXr+8sqCFJhw4dKlY/fHx8lJub69F26623asWKFWrQoIG8vPhfLwBcK7izBQC4akyfPl05OTnq3r27PvvsMx0+fFiJiYnq2rWrbrrpJv3pT3+SJHXu3FnTp0/Xl19+qW3btunxxx+Xt7e3s58bb7xR33//vZYtW6Z///vfeu2115SQkFCsPjRo0EBbtmzRwYMH9fPPP+vChQsaPny4jh07pocfflhffPGFvvvuO61evVp/+MMf8gUzAMDVg7AFALhqREREaOvWrWrUqJH69++v+vXrq2fPnrrpppv0+eefO9+ZmjJlisLDw3XXXXdpwIABGjNmjPz9/Z393HvvvXr66af15JNPqnXr1tq0aZOef/75YvVhzJgxqlKlipo1a6batWvr+++/V1hYmD7//HPl5uaqe/fuat68uZ566im53W5ddx3/KwaAq5XLFHcNXAAAKqHx48dr6tSpWr16taKiosq7OwCAawhhCwBw1Zs3b54yMzM1cuRI7iQBAK4YwhYAAAAAWMCv9wAAAADAAsIWAAAAAFhA2AIAAAAACwhbAAAAAGABYQsAAAAALCBsAQAAAIAFhC0AAAAAsICwBQAAAAAWELYAAAAAwIL/B9wfyXDt/wauAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"Q_YEAR\"], \"Distribution of flights by quarter of the year\", \"Quarter\", None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the difference between the three quarters it is not huge, it is clear that, as we expected, the second quarter (i.e.: May to August) is the one with the greater amount of flights as it matches with the summer season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIhCAYAAAC48qAWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABJZ0lEQVR4nO3de1hVdd7//9eWk0CwRRS2JIkVmYaH0kJoJjE8JjFZ3drQkJbH8RSZU5l3iX1noJjxMDNMZmXamIfuSspJbxIzKRPSLCrNrBkPaYJa4QaVQHH9/ujHutsCCshygzwf17Wvq/1Z773We+29tPVynWyGYRgCAAAAADSqVu5uAAAAAAAuRYQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0A+P8tXbpUNpvNfLVu3VoOh0P9+/dXenq6jhw5Uu0zqampstls9VrOyZMnlZqaqk2bNtXrczUtKyIiQgkJCfWaz/msWLFCCxYsqHGazWZTampqoy6vsb377rvq06eP/P39ZbPZ9Oabb9Za++mnn6pfv36y2+2y2WxasGCBNm3aJJvN5vL7NOR3rlK1XX388cfnrX322We1dOnSBi2nJlXr8vrrrzfaPGvz6quv6rrrrpOvr69sNpsKCgoaZb7r1q274G2u6jfYt29fvT9bn+/wXH92ALRMhC0AOMuSJUuUl5ennJwc/eMf/1CvXr30zDPPqGvXrtqwYYNL7dixY5WXl1ev+Z88eVJz5sypd9hqyLIa4lw7jHl5eRo7dqzlPTSUYRgaMWKEvLy8tGbNGuXl5alfv3611j/wwAMqLCzUqlWrlJeXp3vuuafGuov13Td22LpYjh49quTkZF111VXKzs5WXl6errnmmkaZ97p16zRnzpwLmsewYcOUl5enDh06NEpPtSFsATibp7sbAICmJioqSn369DHf33XXXXrooYf0q1/9Snfeeae++eYbhYaGSpI6duyojh07WtrPyZMn5efnd1GWdT59+/Z16/LP59ChQ/rxxx81fPhwxcfHn7d+x44dGjdunIYOHWqOffXVV9XqmsJ335R9/fXXOnXqlH73u9+dM9zWR9V23xjat2+v9u3bN8q8AKA+OLIFAHVwxRVXaO7cuSotLdWiRYvM8ZpOL9u4caPi4uIUHBwsX19fXXHFFbrrrrt08uRJ7du3z9zpmzNnjnnK4ujRo13m98knn+juu+9WUFCQrrrqqlqXVSUrK0s9evRQ69atdeWVV+pvf/uby/TaTqM6+5S5uLg4rV27Vvv373c5pbJKTacR7tixQ7/5zW8UFBSk1q1bq1evXnr55ZdrXM7KlSs1a9YshYWFKTAwUAMGDNDu3btr/+J/YfPmzYqPj1dAQID8/PwUGxurtWvXmtNTU1PNQPToo4/KZrMpIiKixnlVfR+nT5/WwoULq63n2Wr67svLy/Xwww/L4XDIz89Pt9xyi7Zv366IiAjz9/yl0tJS/f73v1e7du0UHBysO++8U4cOHTKnR0REaOfOncrNzTX7qer/zJkz+uMf/6guXbrI19dXbdq0UY8ePfTXv/61Tt/dTz/9pOnTp8vhcMjX11f9+vXTp59+ak5ftmyZbDZbjUfvnnrqKXl5ebn0+kujR4/Wr371K0nSyJEjZbPZFBcXZ05fs2aNYmJi5Ofnp4CAAA0cOLDacmrb7kePHq1//OMfkuSyPVZtxzabTVOmTNGyZcvUtWtX+fn5qWfPnnr77bdd5l/T9m8YhtLS0tSpUye1bt1affr0UU5OjuLi4lz6r3Lq1Klzbrvn+7MDoGUibAFAHd12223y8PDQ+++/X2vNvn37NGzYMHl7e+ull15Sdna2nn76afn7+6uiokIdOnRQdna2JGnMmDHKy8tTXl6ennjiCZf53Hnnnbr66qv12muv6bnnnjtnXwUFBUpJSdFDDz2krKwsxcbG6sEHH9Rf/vKXeq/js88+q5tvvlkOh8Ps7Vynz+3evVuxsbHauXOn/va3v2n16tXq1q2bRo8erYyMjGr1jz/+uPbv368XX3xRzz//vL755hvdfvvtqqysPGdfubm5uvXWW+V0OrV48WKtXLlSAQEBuv322/Xqq69K+vlUv9WrV0uSpk6dqry8PGVlZdU4v6rTyiTp7rvvPu961uT+++/XggULdP/99+utt97SXXfdpeHDh+vYsWM11o8dO1ZeXl5asWKFMjIytGnTJv3ud78zp2dlZenKK6/U9ddfb/ZT1X9GRoZSU1P129/+VmvXrtWrr76qMWPG1Lqssz3++OPas2ePXnzxRb344os6dOiQ4uLitGfPHkk/hySHw2EGmyqnT5/WokWLNHz4cIWFhdU47yeeeML8XFpamvLy8vTss89K+vm0ut/85jcKDAzUypUrtXjxYhUXFysuLk6bN2+uNq+zt/snnnhCd999tyS5bI+/PB1w7dq1yszM1FNPPaU33nhDbdu21fDhw811q82sWbM0a9YsDRkyRG+99ZYmTpyosWPH6uuvv671OzzXtlvfPzsAWggDAGAYhmEsWbLEkGRs27at1prQ0FCja9eu5vvZs2cbv/yr9PXXXzckGQUFBbXO4+jRo4YkY/bs2dWmVc3vySefrHXaL3Xq1Mmw2WzVljdw4EAjMDDQOHHihMu67d2716XuvffeMyQZ7733njk2bNgwo1OnTjX2fnbf99xzj+Hj42N8++23LnVDhw41/Pz8jGPHjrks57bbbnOp+5//+R9DkpGXl1fj8qr07dvXCAkJMUpLS82x06dPG1FRUUbHjh2NM2fOGIZhGHv37jUkGX/+85/POb9frs/kyZNdxmr6Ts7+7nfu3GlIMh599FGXz65cudKQZIwaNcocq/ruJ02a5FKbkZFhSDIKCwvNseuuu87o169ftT4TEhKMXr161WmdalqXG264wfyODMMw9u3bZ3h5eRljx451WUdvb2/j8OHD5tirr75qSDJyc3PrtJzXXnvNHKusrDTCwsKM7t27G5WVleZ4aWmpERISYsTGxrosu7btfvLkydW2+yqSjNDQUKOkpMQcKyoqMlq1amWkp6ebY2dv/z/++KPh4+NjjBw50mV+eXl5hiSX36A+2+65/uwAaJk4sgUA9WAYxjmn9+rVS97e3ho/frxefvnl8/7rem3uuuuuOtded9116tmzp8tYUlKSSkpK9MknnzRo+XW1ceNGxcfHKzw83GV89OjROnnyZLV/2U9MTHR536NHD0nS/v37a13GiRMn9NFHH+nuu+/WZZddZo57eHgoOTlZBw8erPOpiI0lNzdXkjRixAiX8bvvvluenjVfDt2Qda9y00036bPPPtOkSZP0zjvvqKSkpF79JiUluZzS1qlTJ8XGxuq9994zx37/+99Lkl544QVzLDMzU927d9ctt9xSr+VJPx/1PHTokJKTk9Wq1f/tblx22WW66667lJ+fr5MnT7p8pj7bfZX+/fsrICDAfB8aGqqQkJBzfq/5+fkqLy+v9vv17du31lNPL+T3A9ByEbYAoI5OnDihH374odbTqSTpqquu0oYNGxQSEqLJkyfrqquu0lVXXVXna2uq1OeuaQ6Ho9axH374oV7Lra8ffvihxl6rvqOzlx8cHOzy3sfHR5JUVlZW6zKKi4tlGEa9lmO1quVV3SiliqenZ7V1rNKQda8yc+ZM/eUvf1F+fr6GDh2q4OBgxcfH1+l28lLt28gvv7fQ0FCNHDlSixYtUmVlpT7//HN98MEHmjJlSp2Wcbaqedf2u505c0bFxcUu4w25W2BN37ePj885v9fafr/axmpaTn1+PwAtF2ELAOpo7dq1qqysrPHi+V/69a9/rX/9619yOp3Kz89XTEyMUlJStGrVqjovqz4X1hcVFdU6VrWD2Lp1a0k/39Thl77//vs6L6cmwcHBKiwsrDZedTOFdu3aXdD8JSkoKEitWrWyfDn1UfW9Hj582GX89OnTlgQ/T09PTZ8+XZ988ol+/PFHrVy5UgcOHNDgwYOrHR2qSW3byNkB4sEHH9SBAwf01ltvKTMzU23atNG9997boJ6r5l3b79aqVSsFBQW5jF+sG0rU9vtJNX9XANBQhC0AqINvv/1WM2bMkN1u14QJE+r0GQ8PD0VHR5s3D6g6pa+x/0V8586d+uyzz1zGVqxYoYCAAN1www2SZJ4a9fnnn7vUrVmzptr8zndU4Jfi4+O1cePGaneq++c//yk/P79GuVW8v7+/oqOjtXr1ape+zpw5o1deeUUdO3ZstGc61VXVaXVVN+eo8vrrr+v06dMNnm9dvvs2bdro7rvv1uTJk/Xjjz/W6UG9K1eudDkFdv/+/dqyZUu1fzjo3bu3YmNj9cwzz2j58uUaPXq0/P39G7Iq6tKliy6//HKtWLHCZdknTpzQG2+8Yd6h8HysOIIUHR0tHx+far9ffn7+BZ0WWJ8/OwBaBp6zBQBn2bFjh06fPq3Tp0/ryJEj+uCDD7RkyRJ5eHgoKyvrnM/ree6557Rx40YNGzZMV1xxhX766Se99NJLkqQBAwZIkgICAtSpUye99dZbio+PV9u2bdWuXbtarxU5n7CwMCUmJio1NVUdOnTQK6+8opycHD3zzDPmzuyNN96oLl26aMaMGTp9+rSCgoKUlZVV4x3hunfvrtWrV2vhwoXq3bu3WrVq5fLcsV+aPXu23n77bfXv319PPvmk2rZtq+XLl2vt2rXKyMiQ3W5v0DqdLT09XQMHDlT//v01Y8YMeXt769lnn9WOHTu0cuXKi36L7euuu06//e1vNXfuXHl4eOjWW2/Vzp07NXfuXNntdpdrlOqje/fuWrVqlV599VVdeeWVat26tbp3767bb7/dfP5b+/bttX//fi1YsECdOnVSZGTkeed75MgRDR8+XOPGjZPT6dTs2bPVunVrzZw5s1rtgw8+aN7CfdKkSQ1aD0lq1aqVMjIydO+99yohIUETJkxQeXm5/vznP+vYsWN6+umn6zSf7t27S5KeeeYZDR06VB4eHurRo4e8vb0b3Fvbtm01ffp0paenKygoSMOHD9fBgwc1Z84cdejQ4YJ+v7r+2QHQMhC2AOAs999/vyTJ29tbbdq0UdeuXfXoo49q7Nix530waq9evbR+/XrNnj1bRUVFuuyyyxQVFaU1a9Zo0KBBZt3ixYv1hz/8QYmJiSovL9eoUaO0dOnSBvXbq1cv3X///Zo9e7a++eYbhYWFad68eXrooYfMGg8PD/3rX//SlClTNHHiRPn4+Oiee+5RZmamhg0b5jK/Bx98UDt37tTjjz8up9MpwzBqvTFIly5dtGXLFj3++OOaPHmyysrK1LVrVy1ZsqTGZ001VL9+/bRx40bNnj1bo0eP1pkzZ9SzZ0+tWbNGCQkJjbac+liyZIk6dOigxYsXa/78+erVq5f+53/+R0OGDFGbNm0aNM85c+aosLBQ48aNU2lpqTp16qR9+/apf//+euONN/Tiiy+qpKREDodDAwcO1BNPPCEvL6/zzjctLU3btm3T/fffr5KSEt10001atWqV+Qy3X7rjjjvk4+Oj/v371ynInUtSUpL8/f2Vnp6ukSNHysPDQ3379tV7772n2NjYOs/jww8/1LPPPqunnnpKhmFo7969Df7HiSp/+tOf5O/vr+eee05LlizRtddeq4ULF2rWrFkN/v3q82cHQMtgM/hbAACARrFlyxbdfPPNWr58uZKSktzdToP861//UmJiotauXavbbrvN3e1cVHv37tW1116r2bNn6/HHH3d3OwAuAYQtAAAaICcnR3l5eerdu7d8fX312Wef6emnn5bdbtfnn39u3pSkufjyyy+1f/9+Pfjgg/L399cnn3xy0U/PvJg+++wzrVy5UrGxsQoMDNTu3buVkZGhkpIS7dixo9a7EgJAfXAaIQAADRAYGKj169drwYIFKi0tVbt27TR06FClp6c3u6AlSZMmTdKHH36oG264QS+//PIlHbSkn2+88vHHH2vx4sU6duyY7Ha74uLi9Kc//YmgBaDRcGQLAAAAACzArd8BAAAAwAJuDVsRERGy2WzVXpMnT5YkGYah1NRUhYWFydfXV3Fxcdq5c6fLPMrLyzV16lS1a9dO/v7+SkxM1MGDB11qiouLlZycLLvdLrvdruTkZB07duxirSYAAACAFsitYWvbtm0qLCw0Xzk5OZKk//qv/5IkZWRkaN68ecrMzNS2bdvMW92Wlpaa80hJSVFWVpZWrVqlzZs36/jx40pISFBlZaVZk5SUpIKCAmVnZys7O1sFBQVKTk6+uCsLAAAAoEVpUtdspaSk6O2339Y333wj6ecHdaakpOjRRx+V9PNRrNDQUD3zzDOaMGGCnE6n2rdvr2XLlmnkyJGSpEOHDik8PFzr1q3T4MGDtWvXLnXr1k35+fmKjo6W9PMT4mNiYvTVV1+pS5cudertzJkzOnTokAICAi75i4YBAAAA1M4wDJWWliosLOycD0JvMncjrKio0CuvvKLp06fLZrNpz549KioqcnkIqI+Pj/r166ctW7ZowoQJ2r59u06dOuVSExYWpqioKG3ZskWDBw9WXl6e7Ha7GbQkqW/fvrLb7dqyZUutYau8vFzl5eXm+++++07dunWzYM0BAAAANEcHDhxQx44da53eZMLWm2++qWPHjmn06NGSpKKiIkmqdvvV0NBQ7d+/36zx9vZWUFBQtZqqzxcVFSkkJKTa8kJCQsyamqSnp2vOnDnVxg8cOKDAwMC6rxgAAACAS0pJSYnCw8MVEBBwzromE7YWL16soUOHKiwszGX87FP2DMM472l8Z9fUVH+++cycOVPTp08331d9oYGBgYQtAAAAAOfNJU3i1u/79+/Xhg0bNHbsWHPM4XBIUrWjT0eOHDGPdjkcDlVUVKi4uPicNYcPH662zKNHj57zoYU+Pj5msCJgAQAAAKivJhG2lixZopCQEA0bNswc69y5sxwOh3mHQunn67pyc3MVGxsrSerdu7e8vLxcagoLC7Vjxw6zJiYmRk6nU1u3bjVrPvroIzmdTrMGAAAAABqb208jPHPmjJYsWaJRo0bJ0/P/2rHZbEpJSVFaWpoiIyMVGRmptLQ0+fn5KSkpSZJkt9s1ZswYPfzwwwoODlbbtm01Y8YMde/eXQMGDJAkde3aVUOGDNG4ceO0aNEiSdL48eOVkJBQ5zsRAgAAAEB9uT1sbdiwQd9++60eeOCBatMeeeQRlZWVadKkSSouLlZ0dLTWr1/vciHa/Pnz5enpqREjRqisrEzx8fFaunSpPDw8zJrly5dr2rRp5l0LExMTlZmZaf3KAQAAAGixmtRztpqykpIS2e12OZ1Ort8CAAAAWrC6ZoMmcc0WAAAAAFxqCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAFPdzcAAAAA4P9EPLbW3S00WfueHubuFuqFI1sAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABTzd3QAaJuKxte5uoUna9/Qwd7cAAAAASOLIFgAAAABYgrAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAF3B62vvvuO/3ud79TcHCw/Pz81KtXL23fvt2cbhiGUlNTFRYWJl9fX8XFxWnnzp0u8ygvL9fUqVPVrl07+fv7KzExUQcPHnSpKS4uVnJysux2u+x2u5KTk3Xs2LGLsYoAAAAAWiC3hq3i4mLdfPPN8vLy0v/+7//qyy+/1Ny5c9WmTRuzJiMjQ/PmzVNmZqa2bdsmh8OhgQMHqrS01KxJSUlRVlaWVq1apc2bN+v48eNKSEhQZWWlWZOUlKSCggJlZ2crOztbBQUFSk5OvpirCwAAAKAF8XTnwp955hmFh4dryZIl5lhERIT534ZhaMGCBZo1a5buvPNOSdLLL7+s0NBQrVixQhMmTJDT6dTixYu1bNkyDRgwQJL0yiuvKDw8XBs2bNDgwYO1a9cuZWdnKz8/X9HR0ZKkF154QTExMdq9e7e6dOly8VYaAAAAQIvg1iNba9asUZ8+ffRf//VfCgkJ0fXXX68XXnjBnL53714VFRVp0KBB5piPj4/69eunLVu2SJK2b9+uU6dOudSEhYUpKirKrMnLy5PdbjeDliT17dtXdrvdrDlbeXm5SkpKXF4AAAAAUFduDVt79uzRwoULFRkZqXfeeUcTJ07UtGnT9M9//lOSVFRUJEkKDQ11+VxoaKg5raioSN7e3goKCjpnTUhISLXlh4SEmDVnS09PN6/vstvtCg8Pv7CVBQAAANCiuDVsnTlzRjfccIPS0tJ0/fXXa8KECRo3bpwWLlzoUmez2VzeG4ZRbexsZ9fUVH+u+cycOVNOp9N8HThwoK6rBQAAAADuDVsdOnRQt27dXMa6du2qb7/9VpLkcDgkqdrRpyNHjphHuxwOhyoqKlRcXHzOmsOHD1db/tGjR6sdNavi4+OjwMBAlxcAAAAA1JVbw9bNN9+s3bt3u4x9/fXX6tSpkySpc+fOcjgcysnJMadXVFQoNzdXsbGxkqTevXvLy8vLpaawsFA7duwwa2JiYuR0OrV161az5qOPPpLT6TRrAAAAAKAxufVuhA899JBiY2OVlpamESNGaOvWrXr++ef1/PPPS/r51L+UlBSlpaUpMjJSkZGRSktLk5+fn5KSkiRJdrtdY8aM0cMPP6zg4GC1bdtWM2bMUPfu3c27E3bt2lVDhgzRuHHjtGjRIknS+PHjlZCQwJ0IAQAAAFjCrWHrxhtvVFZWlmbOnKmnnnpKnTt31oIFC3TvvfeaNY888ojKyso0adIkFRcXKzo6WuvXr1dAQIBZM3/+fHl6emrEiBEqKytTfHy8li5dKg8PD7Nm+fLlmjZtmnnXwsTERGVmZl68lQUAAADQotgMwzDc3URzUFJSIrvdLqfT2SSu34p4bK27W2iS9j09zN0tAAAAXBD282rXVPb16poN3HrNFgAAAABcqghbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFjArWErNTVVNpvN5eVwOMzphmEoNTVVYWFh8vX1VVxcnHbu3Okyj/Lyck2dOlXt2rWTv7+/EhMTdfDgQZea4uJiJScny263y263Kzk5WceOHbsYqwgAAACghXL7ka3rrrtOhYWF5uuLL74wp2VkZGjevHnKzMzUtm3b5HA4NHDgQJWWlpo1KSkpysrK0qpVq7R582YdP35cCQkJqqysNGuSkpJUUFCg7OxsZWdnq6CgQMnJyRd1PQEAAAC0LJ5ub8DT0+VoVhXDMLRgwQLNmjVLd955pyTp5ZdfVmhoqFasWKEJEybI6XRq8eLFWrZsmQYMGCBJeuWVVxQeHq4NGzZo8ODB2rVrl7Kzs5Wfn6/o6GhJ0gsvvKCYmBjt3r1bXbp0uXgrCwAAAKDFcPuRrW+++UZhYWHq3Lmz7rnnHu3Zs0eStHfvXhUVFWnQoEFmrY+Pj/r166ctW7ZIkrZv365Tp0651ISFhSkqKsqsycvLk91uN4OWJPXt21d2u92sqUl5eblKSkpcXgAAAABQV24NW9HR0frnP/+pd955Ry+88IKKiooUGxurH374QUVFRZKk0NBQl8+Ehoaa04qKiuTt7a2goKBz1oSEhFRbdkhIiFlTk/T0dPMaL7vdrvDw8AtaVwAAAAAti1vD1tChQ3XXXXepe/fuGjBggNauXSvp59MFq9hsNpfPGIZRbexsZ9fUVH+++cycOVNOp9N8HThwoE7rBAAAAABSEziN8Jf8/f3VvXt3ffPNN+Z1XGcffTpy5Ih5tMvhcKiiokLFxcXnrDl8+HC1ZR09erTaUbNf8vHxUWBgoMsLAAAAAOqqSYWt8vJy7dq1Sx06dFDnzp3lcDiUk5NjTq+oqFBubq5iY2MlSb1795aXl5dLTWFhoXbs2GHWxMTEyOl0auvWrWbNRx99JKfTadYAAAAAQGNz690IZ8yYodtvv11XXHGFjhw5oj/+8Y8qKSnRqFGjZLPZlJKSorS0NEVGRioyMlJpaWny8/NTUlKSJMlut2vMmDF6+OGHFRwcrLZt22rGjBnmaYmS1LVrVw0ZMkTjxo3TokWLJEnjx49XQkICdyIEAAAAYBm3hq2DBw/qt7/9rb7//nu1b99effv2VX5+vjp16iRJeuSRR1RWVqZJkyapuLhY0dHRWr9+vQICAsx5zJ8/X56enhoxYoTKysoUHx+vpUuXysPDw6xZvny5pk2bZt61MDExUZmZmRd3ZQEAAAC0KDbDMAx3N9EclJSUyG63y+l0NonrtyIeW+vuFpqkfU8Pc3cLAAAAF4T9vNo1lX29umaDJnXNFgAAAABcKghbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFigyYSt9PR02Ww2paSkmGOGYSg1NVVhYWHy9fVVXFycdu7c6fK58vJyTZ06Ve3atZO/v78SExN18OBBl5ri4mIlJyfLbrfLbrcrOTlZx44duwhrBQAAAKClahJha9u2bXr++efVo0cPl/GMjAzNmzdPmZmZ2rZtmxwOhwYOHKjS0lKzJiUlRVlZWVq1apU2b96s48ePKyEhQZWVlWZNUlKSCgoKlJ2drezsbBUUFCg5OfmirR8AAACAlsftYev48eO699579cILLygoKMgcNwxDCxYs0KxZs3TnnXcqKipKL7/8sk6ePKkVK1ZIkpxOpxYvXqy5c+dqwIABuv766/XKK6/oiy++0IYNGyRJu3btUnZ2tl588UXFxMQoJiZGL7zwgt5++23t3r271r7Ky8tVUlLi8gIAAACAunJ72Jo8ebKGDRumAQMGuIzv3btXRUVFGjRokDnm4+Ojfv36acuWLZKk7du369SpUy41YWFhioqKMmvy8vJkt9sVHR1t1vTt21d2u92sqUl6erp52qHdbld4eHijrC8AAACAlsGtYWvVqlX65JNPlJ6eXm1aUVGRJCk0NNRlPDQ01JxWVFQkb29vlyNiNdWEhIRUm39ISIhZU5OZM2fK6XSarwMHDtRv5QAAAAC0aJ7uWvCBAwf04IMPav369WrdunWtdTabzeW9YRjVxs52dk1N9eebj4+Pj3x8fM65HAAAAACojduObG3fvl1HjhxR79695enpKU9PT+Xm5upvf/ubPD09zSNaZx99OnLkiDnN4XCooqJCxcXF56w5fPhwteUfPXq02lEzAAAAAGgsbgtb8fHx+uKLL1RQUGC++vTpo3vvvVcFBQW68sor5XA4lJOTY36moqJCubm5io2NlST17t1bXl5eLjWFhYXasWOHWRMTEyOn06mtW7eaNR999JGcTqdZAwAAAACNzW2nEQYEBCgqKsplzN/fX8HBweZ4SkqK0tLSFBkZqcjISKWlpcnPz09JSUmSJLvdrjFjxujhhx9WcHCw2rZtqxkzZqh79+7mDTe6du2qIUOGaNy4cVq0aJEkafz48UpISFCXLl0u4hoDAAAAaEncFrbq4pFHHlFZWZkmTZqk4uJiRUdHa/369QoICDBr5s+fL09PT40YMUJlZWWKj4/X0qVL5eHhYdYsX75c06ZNM+9amJiYqMzMzIu+PgAAAABaDpthGIa7m2gOSkpKZLfb5XQ6FRgY6O52FPHYWne30CTte3qYu1sAAAC4IOzn1a6p7OvVNRu4/TlbAAAAAHApImwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYIEGha0rr7xSP/zwQ7XxY8eO6corr7zgpgAAAACguWtQ2Nq3b58qKyurjZeXl+u777674KYAAAAAoLnzrE/xmjVrzP9+5513ZLfbzfeVlZV69913FRER0WjNAQAAAEBzVa+wdccdd0iSbDabRo0a5TLNy8tLERERmjt3bqM1BwAAAADNVb3C1pkzZyRJnTt31rZt29SuXTtLmgIAAACA5q5eYavK3r17G7sPAAAAALikNChsSdK7776rd999V0eOHDGPeFV56aWXLrgxAAAAAGjOGhS25syZo6eeekp9+vRRhw4dZLPZGrsvAAAAAGjWGhS2nnvuOS1dulTJycmN3Q8AAAAAXBIa9JytiooKxcbGNnYvAAAAAHDJaFDYGjt2rFasWNHYvQAAAADAJaNBpxH+9NNPev7557Vhwwb16NFDXl5eLtPnzZvXKM0BAAAAQHPVoLD1+eefq1evXpKkHTt2uEzjZhkAAAAA0MCw9d577zV2HwAAAABwSWnQNVsAAAAAgHNr0JGt/v37n/N0wY0bNza4IQAAAAC4FDQobFVdr1Xl1KlTKigo0I4dOzRq1KjG6AsAAAAAmrUGha358+fXOJ6amqrjx49fUEMAAAAAcClo1Gu2fve73+mll15qzFkCAAAAQLPUqGErLy9PrVu3bsxZAgAAAECz1KDTCO+8806X94ZhqLCwUB9//LGeeOKJRmkMAAAAAJqzBoUtu93u8r5Vq1bq0qWLnnrqKQ0aNKhRGgMAAACA5qxBYWvJkiWN3QcAAAAAXFIaFLaqbN++Xbt27ZLNZlO3bt10/fXXN1ZfAAAAANCsNShsHTlyRPfcc482bdqkNm3ayDAMOZ1O9e/fX6tWrVL79u0bu08AAAAAaFYadDfCqVOnqqSkRDt37tSPP/6o4uJi7dixQyUlJZo2bVpj9wgAAAAAzU6DjmxlZ2drw4YN6tq1qznWrVs3/eMf/+AGGQAAAACgBh7ZOnPmjLy8vKqNe3l56cyZMxfcFAAAAAA0dw0KW7feeqsefPBBHTp0yBz77rvv9NBDDyk+Pr7RmgMAAACA5qpBYSszM1OlpaWKiIjQVVddpauvvlqdO3dWaWmp/v73vzd2jwAAAADQ7DTomq3w8HB98sknysnJ0VdffSXDMNStWzcNGDCgsfsDAAAAgGapXke2Nm7cqG7duqmkpESSNHDgQE2dOlXTpk3TjTfeqOuuu04ffPCBJY0CAAAAQHNSr7C1YMECjRs3ToGBgdWm2e12TZgwQfPmzWu05gAAAACguapX2Prss880ZMiQWqcPGjRI27dvv+CmAAAAAKC5q1fYOnz4cI23fK/i6empo0ePXnBTAAAAANDc1StsXX755friiy9qnf7555+rQ4cOF9wUAAAAADR39Qpbt912m5588kn99NNP1aaVlZVp9uzZSkhIaLTmAAAAAKC5qtet3//7v/9bq1ev1jXXXKMpU6aoS5custls2rVrl/7xj3+osrJSs2bNsqpXAAAAAGg26hW2QkNDtWXLFv3+97/XzJkzZRiGJMlms2nw4MF69tlnFRoaakmjAAAAANCc1Puhxp06ddK6detUXFysf//73zIMQ5GRkQoKCrKiPwAAAABoluodtqoEBQXpxhtvbMxeAAAAAOCSUa8bZAAAAAAA6oawBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFnBr2Fq4cKF69OihwMBABQYGKiYmRv/7v/9rTjcMQ6mpqQoLC5Ovr6/i4uK0c+dOl3mUl5dr6tSpateunfz9/ZWYmKiDBw+61BQXFys5OVl2u112u13Jyck6duzYxVhFAAAAAC2UW8NWx44d9fTTT+vjjz/Wxx9/rFtvvVW/+c1vzECVkZGhefPmKTMzU9u2bZPD4dDAgQNVWlpqziMlJUVZWVlatWqVNm/erOPHjyshIUGVlZVmTVJSkgoKCpSdna3s7GwVFBQoOTn5oq8vAAAAgJbDZhiG4e4mfqlt27b685//rAceeEBhYWFKSUnRo48+Kunno1ihoaF65plnNGHCBDmdTrVv317Lli3TyJEjJUmHDh1SeHi41q1bp8GDB2vXrl3q1q2b8vPzFR0dLUnKz89XTEyMvvrqK3Xp0qVOfZWUlMhut8vpdCowMNCala+HiMfWuruFJmnf08Pc3QIAAMAFYT+vdk1lX6+u2aDJXLNVWVmpVatW6cSJE4qJidHevXtVVFSkQYMGmTU+Pj7q16+ftmzZIknavn27Tp065VITFhamqKgosyYvL092u90MWpLUt29f2e12s6Ym5eXlKikpcXkBAAAAQF25PWx98cUXuuyyy+Tj46OJEycqKytL3bp1U1FRkSQpNDTUpT40NNScVlRUJG9vbwUFBZ2zJiQkpNpyQ0JCzJqapKenm9d42e12hYeHX9B6AgAAAGhZ3B62unTpooKCAuXn5+v3v/+9Ro0apS+//NKcbrPZXOoNw6g2draza2qqP998Zs6cKafTab4OHDhQ11UCAAAAAPeHLW9vb1199dXq06eP0tPT1bNnT/31r3+Vw+GQpGpHn44cOWIe7XI4HKqoqFBxcfE5aw4fPlxtuUePHq121OyXfHx8zLskVr0AAAAAoK7cHrbOZhiGysvL1blzZzkcDuXk5JjTKioqlJubq9jYWElS79695eXl5VJTWFioHTt2mDUxMTFyOp3aunWrWfPRRx/J6XSaNQAAAADQ2DzdufDHH39cQ4cOVXh4uEpLS7Vq1Spt2rRJ2dnZstlsSklJUVpamiIjIxUZGam0tDT5+fkpKSlJkmS32zVmzBg9/PDDCg4OVtu2bTVjxgx1795dAwYMkCR17dpVQ4YM0bhx47Ro0SJJ0vjx45WQkFDnOxECAAAAQH25NWwdPnxYycnJKiwslN1uV48ePZSdna2BAwdKkh555BGVlZVp0qRJKi4uVnR0tNavX6+AgABzHvPnz5enp6dGjBihsrIyxcfHa+nSpfLw8DBrli9frmnTppl3LUxMTFRmZubFXVkAAAAALUqTe85WU8VztpqHpvLsBQAAgIZiP692TWVfr9k9ZwsAAAAALiWELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAs4NawlZ6erhtvvFEBAQEKCQnRHXfcod27d7vUGIah1NRUhYWFydfXV3Fxcdq5c6dLTXl5uaZOnap27drJ399fiYmJOnjwoEtNcXGxkpOTZbfbZbfblZycrGPHjlm9igAAAABaKLeGrdzcXE2ePFn5+fnKycnR6dOnNWjQIJ04ccKsycjI0Lx585SZmalt27bJ4XBo4MCBKi0tNWtSUlKUlZWlVatWafPmzTp+/LgSEhJUWVlp1iQlJamgoEDZ2dnKzs5WQUGBkpOTL+r6AgAAAGg5bIZhGO5uosrRo0cVEhKi3Nxc3XLLLTIMQ2FhYUpJSdGjjz4q6eejWKGhoXrmmWc0YcIEOZ1OtW/fXsuWLdPIkSMlSYcOHVJ4eLjWrVunwYMHa9euXerWrZvy8/MVHR0tScrPz1dMTIy++uordenS5by9lZSUyG63y+l0KjAw0LovoY4iHlvr7haapH1PD3N3CwAAABeE/bzaNZV9vbpmgyZ1zZbT6ZQktW3bVpK0d+9eFRUVadCgQWaNj4+P+vXrpy1btkiStm/frlOnTrnUhIWFKSoqyqzJy8uT3W43g5Yk9e3bV3a73aw5W3l5uUpKSlxeAAAAAFBXTSZsGYah6dOn61e/+pWioqIkSUVFRZKk0NBQl9rQ0FBzWlFRkby9vRUUFHTOmpCQkGrLDAkJMWvOlp6ebl7fZbfbFR4efmErCAAAAKBFaTJha8qUKfr888+1cuXKatNsNpvLe8Mwqo2d7eyamurPNZ+ZM2fK6XSarwMHDtRlNQAAAABAUhMJW1OnTtWaNWv03nvvqWPHjua4w+GQpGpHn44cOWIe7XI4HKqoqFBxcfE5aw4fPlxtuUePHq121KyKj4+PAgMDXV4AAAAAUFduDVuGYWjKlClavXq1Nm7cqM6dO7tM79y5sxwOh3JycsyxiooK5ebmKjY2VpLUu3dveXl5udQUFhZqx44dZk1MTIycTqe2bt1q1nz00UdyOp1mDQAAAAA0Jk93Lnzy5MlasWKF3nrrLQUEBJhHsOx2u3x9fWWz2ZSSkqK0tDRFRkYqMjJSaWlp8vPzU1JSklk7ZswYPfzwwwoODlbbtm01Y8YMde/eXQMGDJAkde3aVUOGDNG4ceO0aNEiSdL48eOVkJBQpzsRAgAAAEB9uTVsLVy4UJIUFxfnMr5kyRKNHj1akvTII4+orKxMkyZNUnFxsaKjo7V+/XoFBASY9fPnz5enp6dGjBihsrIyxcfHa+nSpfLw8DBrli9frmnTppl3LUxMTFRmZqa1KwgAAACgxWpSz9lqynjOVvPQVJ69AAAA0FDs59WuqezrNcvnbAEAAADApYKwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAF3Bq23n//fd1+++0KCwuTzWbTm2++6TLdMAylpqYqLCxMvr6+iouL086dO11qysvLNXXqVLVr107+/v5KTEzUwYMHXWqKi4uVnJwsu90uu92u5ORkHTt2zOK1AwAAANCSuTVsnThxQj179lRmZmaN0zMyMjRv3jxlZmZq27ZtcjgcGjhwoEpLS82alJQUZWVladWqVdq8ebOOHz+uhIQEVVZWmjVJSUkqKChQdna2srOzVVBQoOTkZMvXDwAAAEDL5enOhQ8dOlRDhw6tcZphGFqwYIFmzZqlO++8U5L08ssvKzQ0VCtWrNCECRPkdDq1ePFiLVu2TAMGDJAkvfLKKwoPD9eGDRs0ePBg7dq1S9nZ2crPz1d0dLQk6YUXXlBMTIx2796tLl26XJyVBQAAANCiNNlrtvbu3auioiINGjTIHPPx8VG/fv20ZcsWSdL27dt16tQpl5qwsDBFRUWZNXl5ebLb7WbQkqS+ffvKbrebNTUpLy9XSUmJywsAAAAA6qrJhq2ioiJJUmhoqMt4aGioOa2oqEje3t4KCgo6Z01ISEi1+YeEhJg1NUlPTzev8bLb7QoPD7+g9QEAAADQsjTZsFXFZrO5vDcMo9rY2c6uqan+fPOZOXOmnE6n+Tpw4EA9OwcAAADQkjXZsOVwOCSp2tGnI0eOmEe7HA6HKioqVFxcfM6aw4cPV5v/0aNHqx01+yUfHx8FBga6vAAAAACgrpps2OrcubMcDodycnLMsYqKCuXm5io2NlaS1Lt3b3l5ebnUFBYWaseOHWZNTEyMnE6ntm7datZ89NFHcjqdZg0AAAAANDa33o3w+PHj+ve//22+37t3rwoKCtS2bVtdccUVSklJUVpamiIjIxUZGam0tDT5+fkpKSlJkmS32zVmzBg9/PDDCg4OVtu2bTVjxgx1797dvDth165dNWTIEI0bN06LFi2SJI0fP14JCQnciRAAAACAZdwatj7++GP179/ffD99+nRJ0qhRo7R06VI98sgjKisr06RJk1RcXKzo6GitX79eAQEB5mfmz58vT09PjRgxQmVlZYqPj9fSpUvl4eFh1ixfvlzTpk0z71qYmJhY67O9AAAAAKAx2AzDMNzdRHNQUlIiu90up9PZJK7finhsrbtbaJL2PT3M3S0AAABcEPbzatdU9vXqmg2a7DVbAAAAANCcEbYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsECLClvPPvusOnfurNatW6t379764IMP3N0SAAAAgEtUiwlbr776qlJSUjRr1ix9+umn+vWvf62hQ4fq22+/dXdrAAAAAC5BLSZszZs3T2PGjNHYsWPVtWtXLViwQOHh4Vq4cKG7WwMAAABwCfJ0dwMXQ0VFhbZv367HHnvMZXzQoEHasmVLjZ8pLy9XeXm5+d7pdEqSSkpKrGu0Hs6Un3R3C01SU/l9AAAAGor9vNo1lX29qj4MwzhnXYsIW99//70qKysVGhrqMh4aGqqioqIaP5Oenq45c+ZUGw8PD7ekRzQO+wJ3dwAAAACrNLV9vdLSUtnt9lqnt4iwVcVms7m8Nwyj2liVmTNnavr06eb7M2fO6Mcff1RwcHCtn7lYSkpKFB4ergMHDigwMNCtvaB5YJtBfbHNoL7YZlBfbDOor6a0zRiGodLSUoWFhZ2zrkWErXbt2snDw6PaUawjR45UO9pVxcfHRz4+Pi5jbdq0sarFBgkMDHT7hobmhW0G9cU2g/pim0F9sc2gvprKNnOuI1pVWsQNMry9vdW7d2/l5OS4jOfk5Cg2NtZNXQEAAAC4lLWII1uSNH36dCUnJ6tPnz6KiYnR888/r2+//VYTJ050d2sAAAAALkEtJmyNHDlSP/zwg5566ikVFhYqKipK69atU6dOndzdWr35+Pho9uzZ1U5zBGrDNoP6YptBfbHNoL7YZlBfzXGbsRnnu18hAAAAAKDeWsQ1WwAAAABwsRG2AAAAAMAChC0AAAAAsABhCwAAAAAsQNhqgt5//33dfvvtCgsLk81m05tvvnnez+Tm5qp3795q3bq1rrzySj333HPWN4omob7by+rVqzVw4EC1b99egYGBiomJ0TvvvHNxmkWT0JC/Y6p8+OGH8vT0VK9evSzrD01PQ7aZ8vJyzZo1S506dZKPj4+uuuoqvfTSS9Y3iyahIdvM8uXL1bNnT/n5+alDhw66//779cMPP1jfLNwuPT1dN954owICAhQSEqI77rhDu3fvPu/nmsP+L2GrCTpx4oR69uypzMzMOtXv3btXt912m37961/r008/1eOPP65p06bpjTfesLhTNAX13V7ef/99DRw4UOvWrdP27dvVv39/3X777fr0008t7hRNRX23mSpOp1P33Xef4uPjLeoMTVVDtpkRI0bo3Xff1eLFi7V7926tXLlS1157rYVdoimp7zazefNm3XfffRozZox27typ1157Tdu2bdPYsWMt7hRNQW5uriZPnqz8/Hzl5OTo9OnTGjRokE6cOFHrZ5rL/i+3fm/ibDabsrKydMcdd9Ra8+ijj2rNmjXatWuXOTZx4kR99tlnysvLuwhdoqmoy/ZSk+uuu04jR47Uk08+aU1jaLLqs83cc889ioyMlIeHh958800VFBRY3h+anrpsM9nZ2brnnnu0Z88etW3b9uI1hyapLtvMX/7yFy1cuFD/+c9/zLG///3vysjI0IEDBy5Cl2hKjh49qpCQEOXm5uqWW26psaa57P9yZOsSkJeXp0GDBrmMDR48WB9//LFOnTrlpq7QXJw5c0alpaXsEOGclixZov/85z+aPXu2u1tBM7BmzRr16dNHGRkZuvzyy3XNNddoxowZKisrc3draKJiY2N18OBBrVu3ToZh6PDhw3r99dc1bNgwd7cGN3A6nZJ0zn2T5rL/6+nuBnDhioqKFBoa6jIWGhqq06dP6/vvv1eHDh3c1Bmag7lz5+rEiRMaMWKEu1tBE/XNN9/oscce0wcffCBPT/63gfPbs2ePNm/erNatWysrK0vff/+9Jk2apB9//JHrtlCj2NhYLV++XCNHjtRPP/2k06dPKzExUX//+9/d3RouMsMwNH36dP3qV79SVFRUrXXNZf+XI1uXCJvN5vK+6uzQs8eBX1q5cqVSU1P16quvKiQkxN3toAmqrKxUUlKS5syZo2uuucbd7aCZOHPmjGw2m5YvX66bbrpJt912m+bNm6elS5dydAs1+vLLLzVt2jQ9+eST2r59u7Kzs7V3715NnDjR3a3hIpsyZYo+//xzrVy58ry1zWH/l3+ivAQ4HA4VFRW5jB05ckSenp4KDg52U1do6l599VWNGTNGr732mgYMGODudtBElZaW6uOPP9ann36qKVOmSPp5R9owDHl6emr9+vW69dZb3dwlmpoOHTro8ssvl91uN8e6du0qwzB08OBBRUZGurE7NEXp6em6+eab9Yc//EGS1KNHD/n7++vXv/61/vjHPzaZoxSw1tSpU7VmzRq9//776tix4zlrm8v+L2HrEhATE6N//etfLmPr169Xnz595OXl5aau0JStXLlSDzzwgFauXMn58DinwMBAffHFFy5jzz77rDZu3KjXX39dnTt3dlNnaMpuvvlmvfbaazp+/Lguu+wySdLXX3+tVq1anXcHCi3TyZMnq52m7OHhIen/jlbg0mUYhqZOnaqsrCxt2rSpTv9vaS77v5xG2AQdP35cBQUF5p2+9u7dq4KCAn377beSpJkzZ+q+++4z6ydOnKj9+/dr+vTp2rVrl1566SUtXrxYM2bMcEf7uMjqu72sXLlS9913n+bOnau+ffuqqKhIRUVF5sWouPTVZ5tp1aqVoqKiXF4hISFq3bq1oqKi5O/v767VwEVU379nkpKSFBwcrPvvv19ffvml3n//ff3hD3/QAw88IF9fX3esAi6y+m4zt99+u1avXq2FCxdqz549+vDDDzVt2jTddNNNCgsLc8cq4CKaPHmyXnnlFa1YsUIBAQHmvskvTztutvu/Bpqc9957z5BU7TVq1CjDMAxj1KhRRr9+/Vw+s2nTJuP66683vL29jYiICGPhwoUXv3G4RX23l379+p2zHpe+hvwd80uzZ882evbseVF6RdPQkG1m165dxoABAwxfX1+jY8eOxvTp042TJ09e/ObhFg3ZZv72t78Z3bp1M3x9fY0OHToY9957r3Hw4MGL3zwuupq2FUnGkiVLzJrmuv/Lc7YAAAAAwAKcRggAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQDABYqLi1NKSkq9PmOz2fTmm29a0g8AoGkgbAEAmqXRo0fLZrNVe/373/9u8DwbGoBWr16t//f//l+Dl1uTTZs2yWaz6dixY406XwDAxePp7gYAAGioIUOGaMmSJS5j7du3r/d8Kioq5O3t3eA+2rZt2+DPAgAuXRzZAgA0Wz4+PnI4HC4vDw8P5ebm6qabbpKPj486dOigxx57TKdPnzY/FxcXpylTpmj69Olq166dBg4cqIiICEnS8OHDZbPZzPepqanq1auXli1bpoiICNntdt1zzz0qLS11md8vTyMsLCzUsGHD5Ovrq86dO2vFihWKiIjQggULXPr//vvvNXz4cPn5+SkyMlJr1qyRJO3bt0/9+/eXJAUFBclms2n06NGN/v0BAKxF2AIAXFK+++473Xbbbbrxxhv12WefaeHChVq8eLH++Mc/utS9/PLL8vT01IcffqhFixZp27ZtkqQlS5aosLDQfC9J//nPf/Tmm2/q7bff1ttvv63c3Fw9/fTTtfZw33336dChQ9q0aZPeeOMNPf/88zpy5Ei1ujlz5mjEiBH6/PPPddttt+nee+/Vjz/+qPDwcL3xxhuSpN27d6uwsFB//etfG+PrAQBcRJxGCABott5++21ddtll5vuhQ4fqmmuuUXh4uDIzM2Wz2XTttdfq0KFDevTRR/Xkk0+qVauf/53x6quvVkZGRrV5tmnTRg6Hw2XszJkzWrp0qQICAiRJycnJevfdd/WnP/2p2ue/+uorbdiwQdu2bVOfPn0kSS+++KIiIyOr1Y4ePVq//e1vJUlpaWn6+9//rq1bt2rIkCHmqYkhISFq06ZNA74dAIC7EbYAAM1W//79tXDhQvO9v7+/Jk+erJiYGNlsNnP85ptv1vHjx3Xw4EFdccUVkmQGobqIiIgwg5YkdejQocYjVdLPR6I8PT11ww03mGNXX321goKCqtX26NHDpfeAgIBa5wsAaH4IWwCAZsvf319XX321y5hhGC5Bq2pMksu4v79/nZfj5eXl8t5ms+nMmTM11lYtqy7j9ZkvAKD54ZotAMAlpVu3btqyZYtLuNmyZYsCAgJ0+eWXn/OzXl5eqqysvKDlX3vttTp9+rQ+/fRTc+zf//53vW/hXnV3xAvtBwDgPoQtAMAlZdKkSTpw4ICmTp2qr776Sm+99ZZmz56t6dOnm9dr1SYiIkLvvvuuioqKVFxc3KDlX3vttRowYIDGjx+vrVu36tNPP9X48ePl6+tb7YjbuXTq1Ek2m01vv/22jh49quPHjzeoHwCA+xC2AACXlMsvv1zr1q3T1q1b1bNnT02cOFFjxozRf//3f5/3s3PnzlVOTo7Cw8N1/fXXN7iHf/7znwoNDdUtt9yi4cOHa9y4cQoICFDr1q3rtR5z5szRY489ptDQUE2ZMqXB/QAA3MNm1HZyOQAAaBQHDx5UeHi4NmzYoPj4eHe3AwC4SAhbAAA0so0bN+r48ePq3r27CgsL9cgjj+i7777T119/Xe2mGACASxd3IwQAoJGdOnVKjz/+uPbs2aOAgADFxsZq+fLlBC0AaGE4sgUAAAAAFuAGGQAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABf4//Zy60v+rza4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, False)\n",
    "plot_histogram_of_column(flights[\"FORTNIGHT\"], \"Distribution of flights by fortnight\", \"Fortnight\", None, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the difference is minimal. However, if we apply the logarithm to the y axis we can see that the second forntingh accumulates more flights than the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"WEEK_INFO\"], \"Flights distribution by working days and weekends\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it could be expected, weekdays accumulate almost three times more flights than weekends. However, we must keep in mind that the weekday category groups five days while the weekend only groups two days. Probably, the difference wouldn't be this big if we were plotting the average number of flights per day. However, this escapes from the scope of this project as it doesn't provide any additional information that could be helpful to predict flight delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"AC\"], \"Flights distribution by airline category\", \"Airline category\", None, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"GR_O\"], \"Flights distribution by quadrant of origin airport\", \"Quadrant\", None, False,False)\n",
    "\n",
    "plot_histogram_of_column(flights[\"GR_D\"], \"Flights distribution by quadrant of destination airport\", \"Quadrant\", None, False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the quadrants with the greater amount of flights are *UPPER_RIGHT* and *BOTTOM_RIGT*. This represents the east cost of the USA, where many of the biggest cities are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram_of_column(flights[\"DepartureDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)\n",
    "\n",
    "plot_histogram_of_column(flights[\"ArrivalDayNight\"], \"Flights distribution by departure time (day or night)\", None, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logically, most of the flights departure during daytime and this trend seems to hold for the arrival time. However, if we compare both histograms we will see that the amount of flights departing during daytime is considerably greater than the flights arriving during daytime. This mean that a considerable amount of flights arrive at their destination at night time even though day departed at daytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauzanization of continuos variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models we will apply assume that our data is distributed following a gaussian distribution, we need to make sure that our data is gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we are going to determine which columns of our dataset are the ones containing continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>],\n",
       "       [<AxesSubplot: title={'center': 'WHEELS_OFF'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_TIME'}>],\n",
       "       [<AxesSubplot: title={'center': 'DISTANCE'}>, <AxesSubplot: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Declare which variables are categorical and which ones are continuous. \n",
    "\n",
    "continuous = ['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
    "       'DISTANCE']\n",
    "\n",
    "flights[continuous].hist()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(data=flights[continuous], hue='DELAYED');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data doesn't follow a gaussian distribution. Before gaussianizing it, we will first remove any outliers that can be identified in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_outliers_zscore(data_series, threshold=3):\n",
    "    \"\"\"\n",
    "    Remove outliers from a pandas Series using the Z-score method.\n",
    "    :param data_series: a pandas Series containing the data\n",
    "    :param threshold: the number of standard deviations from the mean at which to consider a data point an outlier\n",
    "    :return: a new pandas Series with the outliers removed\n",
    "    \"\"\"\n",
    "    z_scores = np.abs((data_series - data_series.mean()) / data_series.std())\n",
    "    return data_series[z_scores <= threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13113"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove outliers (no outliers in the whole dataset, no need to modify X_train and X_test)\n",
    "flights_cont = remove_outliers_zscore(flights[continuous])\n",
    "len(flights_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that no outliers are removed. We can now proceed with the normalization of the continuous data defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we normalize continuous variables\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Initialize transformer with number of quantiles and output distribution\n",
    "transformer = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "# Apply transformation to continuous columns\n",
    "for col in continuous:\n",
    "    flights[col] = transformer.fit_transform(flights[col].values.reshape(-1, 1))\n",
    "    X_train[col] = transformer.fit_transform(X_train[col].values.reshape(-1, 1))\n",
    "    X_test[col] = transformer.fit_transform(X_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot the normalized data, which now clearly follow a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot: title={'center': 'DEPARTURE_DELAY'}>,\n",
       "        <AxesSubplot: title={'center': 'TAXI_OUT'}>],\n",
       "       [<AxesSubplot: title={'center': 'WHEELS_OFF'}>,\n",
       "        <AxesSubplot: title={'center': 'SCHEDULED_TIME'}>],\n",
       "       [<AxesSubplot: title={'center': 'DISTANCE'}>, <AxesSubplot: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGxCAYAAACTN+exAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABjZ0lEQVR4nO3deVxU9f4/8NcAw6YwCsiWCESmJmqGGy65oxaZmaXZ9WqZmVuSmrnc+wW7Ji65dDXXCC01LZebXr0oJWBeXLl6cymzxB1EUYHEWN+/P/zNuQwzIIMDzJHX8/GYh86Z9znnfc6c+fA+n7NpRERAREREpDI2NZ0AERERUWWwiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiLGwtWvXQqPRKC9HR0d4e3uje/fuiI6ORkZGhkF8VFSUQXzp14ULF5TY0p/pdDp069YNu3btMsrj5s2bcHBwgEajwbFjx0zmOmLECIPp2dvbIygoCFOmTEF2djYAICAgoNz89K+1a9fiwoUL0Gg0+Pjjj03O7+OPPzZapm7duhmtr6eeegqzZ89Gfn6+wfj66Zf1ioqKqsA3dF9iYqLRsjdo0ACdOnXCzJkzcfHiRaNxSn+3pV+JiYlKbEBAAMLDwyucz44dO6DRaODu7o68vDxl+IIFC6DRaLBjxw6T4/Xp0wdubm64du1ahedFVBUq0k6U/p38/e9/h0ajQXBwsMlpJiUlwcbGBjNmzDD67LfffkPdunUxaNAgZdiIESNQt27dSuV/6NAhvPLKK/Dx8YG9vT28vb0xaNAgHDx40ChW327fvHnT5LSCg4PRrVs3AMZtnCXaL/ofu5pO4FEVGxuLpk2boqCgABkZGThw4ADmzZuHjz/+GJs3b0avXr0M4uPi4qDT6Yym4+PjY/B+0KBBmDx5MoqLi3H+/HnMnj0bL7zwAnbu3Innn39eifvyyy+VIiAmJgZt2rQxmaeTkxP27dsHALhz5w62bNmChQsX4scff8TevXuxfft2gz+qn332GWJiYozyDQoKwt27d81cS/c9/vjj2LBhAwDgxo0b+Oyzz/DXv/4Vly5dwurVq43iJ0yYgKFDhxoNb9iwodnznjNnDrp3746ioiJkZmbi8OHD+Pzzz7F48WKsWbMGr7/+utE4+u+2tKeeesrs+evFxMQAAG7duoV//OMfGDx4MABg8uTJ2LFjB0aPHo3OnTvDzc1NGWf16tXYu3cvvvrqK/j6+lZ63kSWUPqP/d/+9jckJCQo7Yteyd/J559/DgA4ffo0Dh8+jPbt2xvEdu3aFe+++y7mz5+PAQMGoF27dgCA4uJiDB8+HM7OzlixYsVD57506VJERESgXbt2mD9/Pvz9/XHp0iV8+umn6Ny5Mz755BOMHz++UtNevny5slMIALt27cLs2bON2pHKtF8EQMiiYmNjBYAcPXrU6LOLFy+Kn5+fuLi4SHp6uoiIREZGCgC5cePGA6cNQMaNG2cw7NdffxUA0qtXL4PhwcHB4unpKW3bthWdTie5ublG0xs+fLjUqVPHaHj37t0FgJw/f97os/LyTU1NFQCyYMECk/kvWLBAAEhqaqoyrGvXrtK8eXODuIKCAmncuLHY29vLvXv3Kjx9cyQkJAgA+eabb4w+y8zMlNatW4udnZ38+OOPyvDyvtvS/P395fnnn69QLmlpaWJnZyc9evQQR0dH6d27t8Hnv/32m9StW1eGDBmiDLtw4YK4uLjIK6+8UqF5EFW3stoXvaNHjwoAef755wWAjBo1ymRcbm6uPPnkk9K0aVOlPZg3b54AkK1bt5o1T1MOHDggNjY2Eh4eLgUFBQafFRQUSHh4uNjY2MiBAweU4Q9qt5s3by5du3Y1+Zk57Qg9GA8nVaNGjRph4cKFyMnJwapVqywyzaCgIDRo0MDg8Mfhw4dx6tQpDBs2DKNGjUJWVha2bt1a4Wnqe22uX79ukRzNZWdnh6effhr5+fm4c+dOtc/fzc0Nq1atQmFhIRYvXlzl81u3bh0KCwvx3nvvYeDAgfj+++8Nvs/HH38cH3/8MTZt2oStW7dCRDBy5EjUqVPHInuhRDVB3/s4d+5cdOzYEZs2bUJubq5RnJOTE9auXYtffvkFM2bMwKlTp/B///d/eP311zFw4MCHziM6OhoajQYrVqyAnZ3hwQk7OzssX74cGo0Gc+fOfeh5keWxiKlmzz33HGxtbbF//36D4UVFRSgsLDR4FRUVPXB6t2/fRmZmJho0aKAM0zcOb775JoYMGQJnZ2dlWEWkpqbCzs4Ojz/+eIXHsbTU1FTUq1fPYLn0iouLjdZVYWGhRefftm1b+Pj4GH1PQOW/q7J8/vnn8PHxQb9+/fDmm2+iuLgYa9euNYgZPXo0+vbtizFjxmD27Nn4/vvvsWbNGri7u1d6vkQ15d69e/jqq6/Qtm1bBAcH480330ROTg6++eYbk/GhoaGYMmUKPvnkE/Tv3x/u7u5YunTpQ+dRVFSEhIQEtGnTpszDOX5+fggJCcG+ffse6ndOVYNFTDWrU6cOPDw8jE7E9Pb2hlarNXg1adLEaHwRQWFhIQoKCvDzzz/j9ddfR3FxsXLuRm5uLjZv3owOHTrgqaeegouLC1555RUkJSXht99+M5mT/g9xZmYmVq5ciW3btmHq1Knw9PS0/Aoogz6H9PR0REZG4tixY5g7dy5sbW2NYj/44AOjdaXVanHgwAGL5tSoUSOTJ8x26NDBaN4ODg6VmscPP/yAX375BcOHD4etrS169OiBwMBAxMbGQkQMYmNiYlBYWIj/+7//w8iRI806cZjImmzZsgVZWVkYOXIkAGDw4MGoW7duuTtbs2bNgouLC1JTU/H3v/8d9evXf+g8bt68idzcXAQGBpYbFxgYiNzcXGRmZj70PMmyeGJvDSj9xwkAvvvuO6MTex0dHY3ili9fjuXLlyvvdTodPvzwQ4wdOxYA8PXXXyM7OxtvvvmmEvPmm29i3bp1iI2NxezZsw2md/fuXWi1WoNhr732Gj766CPzF6ySTp8+bZTD9OnTMXr0aJPxEydOxJ/+9Cej4aZOtn0Ypr4nAPjiiy/QrFkzg2EajaZS8yjZa6afzogRIxAZGYnvv//e4ARwX19fjB49GnPnzsWHH35YqfkRWYOYmBg4OTlhyJAhAIC6devilVdeQWxsLM6dO4fGjRsbjRMbG4usrCzY2NggPj4eL7/8crXlq28LKvs7p6rDIqaa3b17F5mZmWjRooXB8FatWsHDw+OB47/66qt4//33odFo4OLigqCgIIPeipiYGDg6OqJv377K+SQtW7ZEQEAA1q5di1mzZhnEOzk5KYdM0tPTsXDhQnz11Vdo2bIlpk2bZtay6Y8nl9Xlqj/kU7pgCQoKwqZNmyAiuHjxImbPno3o6Gi0bNlSaeRKatiwYZlXW1nSpUuXTF7106xZM4vMX9993q5dOzRo0ED5vl566SVERUUhJibG6Co2fY+Pvb39Q8+fqCb8+uuv2L9/P15++WWIiLLdDxo0CLGxsfj8888RHR1tMM758+fx/vvv46WXXkLLli0xa9YsDBo0yOj3YS4PDw84OzsjNTW13LgLFy7A2dlZuTqwIm1d6XaOqgaLmGq2a9cuFBUVKfcQMFeDBg3K/AP6yy+/KIdUGjVqZDJmz549eO6555T3NjY2BtPr3bs3QkJCMGvWLLz++uvw8/OrcG4eHh6wtbXF1atXTX5+9epV2NraGp3H4ejoqOTQtm1bdO/eHc2bN0dERATCw8Mrfd+Hh3HkyBGkp6cr3d1V4auvvkJubi6OHDlismt8+/btuH37tkW6zYmsxeeffw4RwZYtW7Blyxajz9etW4fZs2crO1sigjfeeANOTk5YuXIl6tevj3/84x946623cPLkSbi4uFQ6F1tbW3Tv3h1xcXG4cuWKyfNirly5gpSUFPTr10/JycvLC8D9Nk3/fz0RQVpaWrXsaBHPialWly5dwpQpU6DT6co8VPIw9Icm1qxZg4SEBIPX7t27odVqlfsylMXBwQGffvop/vjjD6NDTw/i6OiITp06YceOHfjjjz8MPvvjjz+wY8cOdO7c2eRhspLc3d0xd+5cXL9+3SIn75nr1q1beOedd6DVavHee+9V2XxiYmLg4uKC77//3uj7WrBgAfLy8pT75xA9CoqKirBu3ToEBQUZbfMJCQmYPHky0tLS8K9//UsZ55NPPsH+/fuxYsUKeHp6QqvVYu3atbh27Rref//9h85p+vTpEBGMHTvWqGelqKgIY8aMgYhg+vTpyvAePXpAo9Fg8+bNRtOLi4tDdnb2Q/cSUcWwJ6aKnDp1SjlZNSMjAz/88ANiY2Nha2uL7du3G111k5KSYvJmd0899RRcXV0fOL/CwkLlXI233nrLZMwLL7yAHTt24MaNGyav+tHr2rUrnnvuOcTGxmLatGkPPOmtpLlz56J79+4IDQ1FREQEGjVqhEuXLmHJkiW4fv06Nm3aVKHp/PnPf8aiRYvw8ccfY9y4cQbr4NKlSzh06JDROA0aNEBQUFCFcwWAc+fO4dChQyguLlZudhcTE4Ps7Gx88cUXaN68udE4+u+2NP3l7nrp6ekm9zQDAgLg6OiII0eOYMyYMejRo4dRTKdOnbBw4ULExMRU+iZbRNbmX//6F65du4Z58+aZ7I0ODg7GsmXLEBMTg/DwcOWy6iFDhhjcmffpp5/GjBkzLHJYqVOnTliyZAkiIiLQuXNnjB8/Xmm3Pv30Uxw+fBhLlixBx44dlXGCgoIwfvx4LFiwAHfu3MFzzz0HJycnHD16FHPnzkWbNm1M3pCTqkCN3J3mEaa/kZH+ZW9vL56entK1a1eZM2eOZGRkGMTrb5pU1is+Pl6JhYmb3en94x//EACyZMmSMnOLi4sTALJw4UIRKf/GUCdPnhQbGxt54403TOZb3s35jh07Ji+99JJ4eHiIra2teHh4yEsvvSQpKSlGsaZudqe3a9cuASCzZs0Skf/d7K6s1+uvv15mTqXpb3anf9nZ2Ym7u7uEhobKjBkz5MKFC0bjlP5uS7/WrFmjxPr7+5cZN3z4cImIiBAAcuLEiTJznDZtmgAwWG/m3ByRqCaZal8GDBgg9vb2Ru1gSUOGDBE7OztJT0+X0NBQ8fb2lszMTKO4/Px8adWqlfj7+0t2dnaZ86yogwcPyqBBg8TLy0vs7OzE09NTBg4cKMnJySbji4uLZcWKFdKmTRtxdnYWe3t7ady4sXzwwQeSk5NT5nx4szvL0oiUcQkGERERkRXjOTFERESkSjwnhh4pIvLAu2ra2tryfg9Ej7ji4mIUFxeXG1P6MQOkPuyJoUfKunXrTN7Nt+QrKSmpptMkoir24YcfPrAtuHDhQk2nSQ/poc6JiY6OxowZMzBx4kQsWbIEwP094VmzZmH16tW4ffs22rdvj08//dTgKo+8vDxMmTIFX331Fe7du4eePXti+fLlBtfo3759G++++y527NgBAOjfvz+WLl2KevXqVTZdqgUyMzMfeOOqJk2aPNS9JYjI+l27ds3kY0NKatmyJW8cqXKVLmKOHj2KV199Fa6urujevbtSxMybNw8fffQR1q5diyeffBKzZ8/G/v37cfbsWeUPx5gxY7Bz506sXbsW7u7umDx5Mm7duoWUlBTlZkL9+vXDlStXsHr1agDA22+/jYCAAOzcudMCi01ERESqV5lLmnJycqRx48YSHx8vXbt2lYkTJ4rI/UvOvL29Ze7cuUrsH3/8ITqdTlauXCkiInfu3BGtViubNm1SYq5evSo2NjYSFxcnIiJnzpwRAHLo0CEl5uDBgwJAfv7558qkTERERI+YSp3VNG7cODz//PPo1auXwV1dU1NTkZ6ejrCwMGWYg4MDunbtiuTkZIwePRopKSkoKCgwiPH19UVwcDCSk5PRp08fHDx4EDqdDu3bt1diOnToAJ1Oh+TkZJNPd87Ly0NeXp7yvri4GLdu3YK7uztP4iSyMBFBTk4OfH19YWNTO0+tKy4uxrVr1+Di4sI2hsiCzGlfzC5iNm3ahP/85z84evSo0Wfp6ekAYPQsCS8vL1y8eFGJsbe3N3oejJeXlzJ+eno6PD09jabv6empxJQWHR2NWbNmmbs4RPQQLl++bPJ5M7XBtWvXzHq2GBGZpyLti1lFzOXLlzFx4kTs3bu33OfflN4rEZEH7qmUjjEVX950pk+fjkmTJinvs7Ky0KhRI6Smppp1EmdBQQESEhLQvXt31T2FlLnXjNqYe05ODgIDA6v0BGlrv3BAv+yXL1+u0KNBgPvre+/evQgLC1PltsLca4aa869M7tnZ2fDz86tQ+2JWEZOSkoKMjAyEhIQow4qKirB//34sW7YMZ8+eBXC/J8XHx0eJycjIUHpnvL29kZ+fb/R03oyMDOXZFN7e3rh+/brR/G/cuGHUy6Pn4OAABwcHo+Fubm4VbmCA+yvc2dkZ7u7uqtxYmHv1q42562Or6jDK0aNHsXr1arRs2dJg+Pz587Fo0SKDCwd69+5tcOFAREQEdu7ciU2bNikXDoSHhxtcODB06FBcuXIFcXFxAO5fODBs2DCzLhzQL7urq6tZRYyzszNcXV1Vu60w9+qn5vwfJveKtC9mHczu2bMnTp48iRMnTiivNm3a4PXXX8eJEyfw+OOPw9vbG/Hx8co4+fn5SEpKUgqUkJAQaLVag5i0tDScOnVKiQkNDUVWVhaOHDmixBw+fBhZWVkGD+EiokfP77//jtdffx1r1qwx2NERESxZsgQzZ87EwIEDERwcjHXr1iE3NxcbN24EcL8HNiYmBgsXLkSvXr3QunVrrF+/HidPnsR3330HAPjpp58QFxeHzz77DKGhoQgNDcWaNWvwz3/+U9kRIyJ1MKsnxsXFBcHBwQbD6tSpA3d3d2V4REQE5syZg8aNG6Nx48aYM2cOnJ2dlSd66nQ6jBw5EpMnT4a7uzvc3NwwZcoUtGjRQnkSabNmzdC3b1+MGjUKq1atAnB/Tyk8PNzkSb1E9OiwxgsHAOOLB7KzswHc39MsKCio0LLp4yoab02Ye81Rc/6Vyd2cWIvfc3nq1Km4d+8exo4dqxyz3rt3r8GxrcWLF8POzg6vvvqqcsx67dq1SlcvAGzYsAHvvvuu0hj1798fy5Yts3S69BACpu2y2LQcbAXz21lscqRS1nrhAFD2xQN79+6Fs7PzA5bMUMmeaLVh7jVHzfmbk3tubm6FYx+6iElMTDR4r9FoEBUVhaioqDLHcXR0xNKlS7F06dIyY9zc3LB+/fqHTY+IVMKaLxwAjC8e0J98GBYWZtY5MfHx8ejdu7cqz22o7tyDo/ZYZDoONoK/tSlW5XoHat92o+/lrAg+/YqsSnDUHuQVWeZk0Qtzn7fIdKh6WPOFA0DZFw/on8NjjsqMYy2qM3dLtQV6al7vgLrzNyd3c5axdt6lioisDi8cICJzsSeGiKwCLxwgInOxiCEi1eCFA0RUEosYIrJavHCAiMrDc2KIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKvNkdERHVCnzA7KOHPTFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJXMKmKio6PRtm1buLi4wNPTEwMGDMDZs2cNYkQEUVFR8PX1hZOTE7p164bTp08bxOTl5WHChAnw8PBAnTp10L9/f1y5csUg5vbt2xg2bBh0Oh10Oh2GDRuGO3fuVG4piYiI6JFjVhGTlJSEcePG4dChQ4iPj0dhYSHCwsJw9+5dJWb+/PlYtGgRli1bhqNHj8Lb2xu9e/dGTk6OEhMREYHt27dj06ZNOHDgAH7//XeEh4ejqKhIiRk6dChOnDiBuLg4xMXF4cSJExg2bJgFFpmIiIgeBXbmBMfFxRm8j42NhaenJ1JSUvDss89CRLBkyRLMnDkTAwcOBACsW7cOXl5e2LhxI0aPHo2srCzExMTgyy+/RK9evQAA69evh5+fH7777jv06dMHP/30E+Li4nDo0CG0b98eALBmzRqEhobi7NmzaNKkiSWWnYiIiFTMrCKmtKysLACAm5sbACA1NRXp6ekICwtTYhwcHNC1a1ckJydj9OjRSElJQUFBgUGMr68vgoODkZycjD59+uDgwYPQ6XRKAQMAHTp0gE6nQ3JysskiJi8vD3l5ecr77OxsAEBBQQEKCgoqvEz6WHPGsRbVnbuDrVhuWjZi8K8lVNd6qI3bTFUsa3R0NLZt24aff/4ZTk5O6NixI+bNm2fwexcRzJo1C6tXr8bt27fRvn17fPrpp2jevLkSk5eXhylTpuCrr77CvXv30LNnTyxfvhwNGzZUYm7fvo13330XO3bsAAD0798fS5cuRb169Sy+XERUdSpdxIgIJk2ahM6dOyM4OBgAkJ6eDgDw8vIyiPXy8sLFixeVGHt7e9SvX98oRj9+eno6PD09jebp6empxJQWHR2NWbNmGQ3fu3cvnJ2dzVw6ID4+3uxxrEV15T6/neWn+bc2xRab1u7duy02rYqoTdtMbm6uxXPQH65u27YtCgsLMXPmTISFheHMmTOoU6cOgP8drl67di2efPJJzJ49G71798bZs2fh4uIC4P7h6p07d2LTpk1wd3fH5MmTER4ejpSUFNja2gK4f7j6ypUrSu/y22+/jWHDhmHnzp0WXy4iqjqVLmLGjx+PH3/8EQcOHDD6TKPRGLwXEaNhpZWOMRVf3nSmT5+OSZMmKe+zs7Ph5+eHsLAwuLq6ljvvkgoKChAfH4/evXtDq9VWeDxrUN25B0ftsdi0HGwEf2tTjL8es0FecfnbSkWdiupjkek8SG3cZvQ9nZZk7YerLdHbWxt77R6GpXp71dzTW3JetWW7MSe2UkXMhAkTsGPHDuzfv9+gi9bb2xvA/Z4UHx8fZXhGRobSO+Pt7Y38/Hzcvn3boDcmIyMDHTt2VGKuX79uNN8bN24Y9fLoOTg4wMHBwWi4Vqut1B+Wyo5nDaor97wiyxQbBtMs1lhsutX9/dWmbaY6ltOaDlcDlu3trU29dg/D0r29au7pBWrPdmNOT69ZRYyIYMKECdi+fTsSExMRGBho8HlgYCC8vb0RHx+P1q1bAwDy8/ORlJSEefPmAQBCQkKg1WoRHx+PV199FQCQlpaGU6dOYf78+QCA0NBQZGVl4ciRI2jX7v5WfPjwYWRlZSmFDhE9uqztcDVgmd7e2thr9zAs1dur5p5eoPZtN+b09JpVxIwbNw4bN27Et99+CxcXF+UHr9Pp4OTkBI1Gg4iICMyZMweNGzdG48aNMWfOHDg7O2Po0KFK7MiRIzF58mS4u7vDzc0NU6ZMQYsWLZTu32bNmqFv374YNWoUVq1aBeD+Mevw8HBemURUC1jb4WrAsr29tanX7mFYurdXzT29+nnWhu3GnGU0q4hZsWIFAKBbt24Gw2NjYzFixAgAwNSpU3Hv3j2MHTtWuXpg7969ykl3ALB48WLY2dnh1VdfVa4eWLt2rXLSHQBs2LAB7777rtIt3L9/fyxbtsycdIlIhazxcDURWSezbnYnIiZf+gIGuL+HExUVhbS0NPzxxx9ISkpSuoP1HB0dsXTpUmRmZiI3Nxc7d+6En5+fQYybmxvWr1+P7OxsZGdnY/369bz8kegRJiIYP348tm3bhn379pV7uFpPf7haX6CUPFytpz9crY8pebhaj4eridTpoe4TQ0RkKTxcTUTmYhFDRFaBh6uJyFwsYojIKog8+B4e+sPVUVFRZcboD1cvXbq0zBj94WoiUjezzokhIiIishYsYoiIiEiVWMQQERGRKrGIISIiIlViEUNERESqxCKGiIiIVImXWNMjK2DaLotN68Lc5y02LSIisgz2xBAREZEqsSeGiIgswpK9n9bO0svK3t7KYU8MERERqRJ7YmqZ2rSnREREjzb2xBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEq2dV0AkRqEDBtV5mfOdgK5rcDgqP2IK9IU6HpXZj7vKVSIyKqtay+iFm+fDkWLFiAtLQ0NG/eHEuWLEGXLl1qOq1qU94fz9Iq88eUqDar7e0LUPE2hu1L1bLkjlJt2kmy6sNJmzdvRkREBGbOnInjx4+jS5cu6NevHy5dulTTqRGRyrF9IVI/qy5iFi1ahJEjR+Ktt95Cs2bNsGTJEvj5+WHFihU1nRoRqRzbFyL1s9rDSfn5+UhJScG0adMMhoeFhSE5OdkoPi8vD3l5ecr7rKwsAMCtW7dQUFBQ4fkWFBQgNzcXmZmZ0Gq1lczecuwK71Y8tliQm1sMuwIbFBWrq7u3tuWemZlZxVlVTGW395ycHACAiFRValXK3PYFsEwbY23tC1DxNqa2/Uatibn5W0v7AlRumzenfbHaIubmzZsoKiqCl5eXwXAvLy+kp6cbxUdHR2PWrFlGwwMDA6ssR2s0tKYTeAi1KXePhVWSRrXLycmBTqer6TTMZm77ArCNAWrXb9TamJN/bWpfrLaI0dNoDKtOETEaBgDTp0/HpEmTlPfFxcW4desW3N3dTcaXJTs7G35+frh8+TJcXV0rn3gNYO41ozbmLiLIycmBr69vFWZX9SravgCWaWNq47ZiDdScO6Du/CuTuznti9UWMR4eHrC1tTXaK8rIyDDaewIABwcHODg4GAyrV69epefv6uqquo1Fj7nXjNqWuxp7YPTMbV8Ay7YxtW1bsRZqzh1Qd/7m5l7R9sVqT+y1t7dHSEgI4uPjDYbHx8ejY8eONZQVET0K2L4QPRqsticGACZNmoRhw4ahTZs2CA0NxerVq3Hp0iW88847NZ0aEakc2xci9bPqImbw4MHIzMzEhx9+iLS0NAQHB2P37t3w9/evsnk6ODggMjLSqNtYDZh7zWDu6sT2xTzMveaoOf+qzl0jar1GkoiIiGo1qz0nhoiIiKg8LGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqlTri5iAgABoNBqDV+mHwpUmIoiKioKvry+cnJzQrVs3nD59ulry3bJlCzQaDTZv3owLFy5g5MiRCAwMhJOTExwcHKDRaPDPf/7TaLygoCA888wzAIC6desaLXOHDh0AAMeOHYNGo8HatWuVcdeuXWsUX/KVmJioxJpan/rXk08+icDAQDg6Oipxx44dK3NZk5KS0KJFC9ja2kKr1cLe3h5ubm5o0aIFRo0ahcuXL5u9/i5fvozx48cjKCgIjo6OqF+/Prp164YNGzYYPWzswoULZS5LmzZtlLgRI0aUu370r59//tnsfB9GVFSUUQ7e3t7ljpOUlISQkBA4Ojri8ccfx8qVK6sp20dXdbQxhw8fxksvvYRGjRrBwcEBXl5eCA0NxeTJkw3iiouL8eWXX6JXr17w8PCAVquFp6cnwsPDsXPnThQXFwP437bfrl07pX0JCgpCZGQk8vPz8fHHH0Oj0eDChQvKtB/0O9Ar/bvSarVwd3dH27Zt8d5775lcTn0bVFZ7ER4eDnd3d6V9CQkJgUajwfjx48tcZ0lJSXBxcSkz34CAACU2MTHR4DN7e3s0aNAAnTp1wsyZM3Hx4sUy52NKRduMESNGAAC6deuG4OBgk7mU1b588cUXJttoU+1CyVfJ7/RBrKGNser7xFSXDz/8EKNGjVLe161bt9z4+fPnY9GiRVi7di2efPJJzJ49G71798bZs2fh4uJSpbl269YNGo0GCQkJ0Ol0KC4uxqpVq+Dh4YE2bdpAo9Hgww8/RHh4uDLOlStXcP78eYPnvjRo0AA//vij8t7e3v6B846NjUXTpk2Nhj/11FMG7zt16oSPP/7YYNh3332HWbNmYcWKFejUqRMmTpyIixcvlvmwvdTUVPTt2xcajQYNGzZEly5dsGnTJowaNQo6nQ5ff/01zp8/Dz8/vwfmrffvf/8b4eHhqFu3Lt5//320bNkSWVlZ+Prrr/GnP/0JO3fuxMaNG2FjY1jb9+rVC2+88QaKioqwcuVK/Pbbb1i+fLlBjJOTE/bt2wcA+M9//oNx48Zh8+bNaNWqlXL77AYNGlQ4V0tp3rw5vvvuO+W9ra1tmbGpqal47rnnMGrUKKxfvx7//ve/MXbsWDRo0AAvv/xydaT7yKrKNmbXrl3o378/unXrhvnz58PHxwdpaWk4duwYNm3ahIUL7z8N8I8//sCAAQOwd+9eDBkyBCtWrIC3tzdu3LiBuLg4vPLKK9i8eTNefPFFZdoiglWrVuGJJ57AqVOnMGrUKNy9e7fMP1ROTk7o3r07bt26hb/85S/KcDs74z81EyZMwNChQ1FcXIw7d+7g+PHj+Pzzz7F06VJER0fj/ffff/CK/f+uXbuGW7duYd68eejUqRNWrVqF//znP8rTkEvTb+v169dH/fr18frrr2PBggX429/+hu7duwOAyfuazJkzB927d0dRUREyMzNx+PBhfP7551i8eDHWrFmD119/vUL5/vWvf8U777yDiIgI9O7dGwAwe/ZsNGnSBHfu3MFXX30FJyenctuMxx57DFlZWbh79y6Sk5PRuHFjg88///xzuLq6Ijs72+T4cXFxJm/t7+PjU6Fl0KvxNkZqOX9/f1m8eHGF44uLi8Xb21vmzp2rDPvjjz9Ep9PJypUrqyBDYy1atJAmTZoYDNu2bZtotVrp1KmTODg4GHz2xRdfCADZuXOniIjUqVNHvLy8TE776NGjAkBiY2OVYbGxsQJAjh49+sDc/P395fnnnzca3q5dO3nnnXeMpjl8+HCT05k6daq4u7sLADl//ryIiIwePVo6dOigxBQVFT0wH73bt2+Lp6en+Pv7S3p6utHnc+fOFQASHR2tDEtNTRUAsmDBAmVYRkaGAJCkpCRl2PDhw6VOnTrK+4SEBAEgt2/frnB+VSEyMlJatWpV4fipU6dK06ZNDYaVXudkvqpuY5599lkJCgqSgoICo89K/kbGjBkjAGTdunUm5/vLL7/If//7XxExve2LiMyfP18CAwNlwYIFAkBSU1OVz/S/g+HDh8uLL75Y5vKVNW0RkdzcXOnbt68AkN27dyvDH9QG6XQ6qVu3rsEwAPLMM8+YjNdv6127dpXmzZuLSPnbuv43/c033xh9lpmZKa1btxY7Ozv58ccfTS/0A+inHxMTY9S+iIhBnvpYPz8/6devnzRs2FBmzJhhEP/rr7+KRqORUaNGCQBJSEhQPouMjBQAcuPGjUrlWpI1tDG1/nASAMybNw/u7u54+umn8dFHHyE/P7/M2NTUVKSnpyMsLEwZ5uDggK5duyI5Obk60kX37t1x9uxZpKWlKcMSExPRtm1b+Pn5IS8vz2APJDExEba2tujSpYsyLDMzE56ennjyyScxatQoZGRkVFm++fn5SElJMVhneiV7g0o6ePAgHnvsMdjY2MDT0xMA0KdPHxw7dgwFBQUAYNRjUp7PPvsMGRkZmDt3rskH/E2dOhVNmzbFggULlOmbkpWVBQBwc3N74Dxbt24NHx8f9OzZEwkJCRXO1ZLOnTsHX19fBAYGYsiQITh//nyZsQcPHjT6jkqvc6qcqmxjMjMz4eHhYbK3Q/8bSU9Px2effYY+ffrgz3/+s8n5Nm7cGC1btix3ObKysiq07ScmJlaqfXFyckJMTAy0Wi0WLFhQoXHy8/ORlZUFJycno8/K6um15Lbu5uaGVatWobCwEIsXLzZr3NLu3r2rTPNBrl27hn379sHW1harV69WDgUC93th/Pz80KtXr4fKpyJquo2p9UXMxIkTsWnTJiQkJGD8+PFYsmQJxo4dW2a8/kdR+g+hl5dXmT8YS9N3d5Y8zpmQkICWLVti165dsLW1xQ8//GDw2TPPPKN0HTo5OeHpp5/G3r17MX/+fBw5cgTdu3fH3bt3UVRUVOZ8i4qKUFhYaPAyFS8iBjHp6ekoKipSipGSMjMzTc4rPT0dTZo0QXFxMQYOHIg9e/agbt26KCwsxM2bNyu0nkqKj4+Hra0tXnjhBZOfazQa9O/fH7du3UJKSorBZ8XFxSgsLERBQQEiIiLQqVMnNG/e3Gga+uVt0KABVqxYga+//hrbtm1DkyZN0LNnT+zfv9/svB9G+/bt8cUXX2DPnj1Ys2YN0tPT0bFjx3LXuanturLrnO6r6jYmNDQUhw8fxrvvvovDhw+b/GOQkJCAgoICDBgwwKzc9dt+YWEhzp49i7///e8YNWqUwR/M0sLCwvDFF18YtS95eXkVmqevry9CQkKQnJyMwsLCB8brt01ThzH0RUFppbf1wsJCuLu7K+1VYWFhuctYWtu2beHj4/PQv/F169ahc+fOyvkvpvj4+GD16tXw9PRESEgIOnXqhJs3byqH8IuKirBu3TqMGDGi3B29irbn5bGKNqbSfThWTN9dVt6rrG7JLVu2CAC5efOmyc///e9/CwC5du2awfC33npL+vTpU23529jYyNtvvy0iIjdv3hSNRiM+Pj4ycuRIadeunUyZMkVERC5duiQAZOrUqcr0/f39Hzh9U4eTTL1sbW0Nci9v2qNGjTKapr+/v8l10LhxY/noo49k9OjRYmNjIwBEo9EIAHn77bcNurEromnTpuLt7V1uzIoVKwSAbN68WUT+1+1t6hUfH6+MN3z4cJMxnTp1UmLCw8PlhRdeMCtnS/v999/Fy8tLFi5caPLzxo0by5w5cwyGHThwQABIWlpadaSoGtbUxty8eVM6d+6szFer1UrHjh0lOjpacnJyROR/h0vj4uIqlPu33377wBiYOJxUVpxGo5GtW7eKSPmHk/QGDx4sAOT69esiUv7hpKtXrwoAo983AKlXr57J6eu39a5du5aZ88iRI5X48g4n6bVv316cnJzK/Lw8+ul7eHjI5cuXjT4veThJr+Shezc3N/Hx8RERkV27dolGo5HU1FT55ptvyjycZOoVFBRUqfz1aqKNeSRP7B0/fjyGDBlSbkzJM89L0l+l8+uvv8Ld3d3oc/0Jbenp6QYnQGVkZJg8TFEZFcn/tddeU3pitm/fDhFB165dsXr1akybNk05yVR/GEPfe6PXuXNng67Pl156CQMGDEC7du3K7G7+4osv0KxZM4NhJa86KGvaBQUF6Ny5M9q3b28UW1a3qbe3N65fv46VK1di+vTp2L17N7Zu3Yrvv/8eq1evxvr167F792507drV5PiVIf//6qTSyzRx4kRcu3YNSUlJWL16NR577DE0adLEIMbJycloL6zkCZgdOnTA+vXrLZZrZdSpUwctWrTAuXPnTH7u7e1ttKefkZEBOzs7k7+F2sya2hh3d3f88MMPOHbsGL7//nscO3YMiYmJmD59OlatWoWjR4+anbv+NzBx4kT06dMH77zzDoKDgxEZGQkbGxusX78en3zyidF4pn4HAPDyyy+Xud2Zov8tVoSHhwcAmOxFcHZ2NjlOyW09KChI6SmbPn06Dhw4ADs7O7NPxDcn59L06zIqKgoNGzY0e/zu3btj27ZtyMzMRExMDLp3746AgIByr/787rvvjE7sdXR0NHveJdVEG/NIFjEeHh7Khm2u48ePAyj7DO3AwEB4e3sjPj4erVu3BnD/mGxSUhLmzZtXuYRLqUj+PXr0wKJFi5CSkoIpU6bA3d0d69evh42NDbp27YqFCxciKysLCQkJsLOzQ+fOnQ3G1+l0ymXCmZmZuHHjBtq2bWtUpJTUrFkzg0uLy1Jy2npt2rRBSkoKRo4caTC8rGPwoaGh2LlzJwDA398fY8aMwY8//oi7d+/ivffew2uvvYb3338fR44ceWA+ANCoUSOcO3cOd+/eRZ06dUzG6C8tLH3F09GjR3HhwgUcOHDA6AoAPRsbm3LXzfHjx80+69/S8vLy8NNPPxmcG1VSyXWut3fvXrRp0wZarbY6UlQNa2xj2rRpo2yDBQUF+OCDD7B48WLMnz9fmU5qair69OnzwNz1vwUXFxdMnDgRHTt2xPr165VDNiUPZZdk6neQmZmJ69evm7X9X7x4EQ4ODspOjv58H1OFir29PXQ6ncnzjMq6ikq/rXt6esLR0RFt2rRBTEwM2rZtqxSZ5rp06RJ8fX3NGkdEMGHCBOXwf2V3hAsLC2FjY4PFixdj586dBrfIKEurVq0qvQ2XpUbamEr13zwikpOTZdGiRXL8+HE5f/68bN68WXx9faV///4GcU2aNJFt27Yp7+fOnSs6nU62bdsmJ0+elNdee018fHwkOzu72nLfuXOnABAvLy+pU6eOjBkzRtLS0iQtLU3Onj0rNjY2smPHDgkICBBHR0cl/5ycHHF1dZWOHTtKamqqJCQkSGhoqDz22GOSnZ1dZVcnbdq0SbRarcTExMiZM2ekd+/eAkB27NghIiLTpk2TYcOGKfHnz58XZ2dnee+99+TMmTMSExMjWq1WtmzZIiIiTz/9tFldt/qrKb766iuTnxcXF0vTpk3Fzc1N8vPzReR/3d6Ojo6SmJiorN+0tDTJzc1Vxg0ODhY7Ozvl/eLFi2X79u3yyy+/yKlTp2TatGkCQOlOry6TJ0+WxMREOX/+vBw6dEjCw8PFxcVFLly4ICLmr3MyX022MXfu3BEA0q9fP0lLSxOtVlvhQ976bd/d3V169OghV65cMdj+S16dpM9df3XS5MmTJTk52WT7UnLaZR1OunLlitjZ2UnPnj2VYXv37i33N+Tn5ycajUZpXyIiIgSA/PnPfxaRsrf1hg0byhNPPPHAbf1Bh5MOHz5sdAiqIsaMGSM6nU4WL14sAGTNmjVG7cu0adPEy8tLOZykb18ee+wxefbZZ5X2pXfv3mJjYyP16tWTe/fuiYiUezjJElcnWUMbU6uLmJSUFGnfvr3odDpxdHSUJk2aSGRkpNy9e9cgrvQf9eLiYomMjBRvb29xcHCQZ599Vk6ePFmtuWdlZSnniph6PfPMM/Lyyy8bneOSm5srjo6OYm9vL1qtVho1aiTDhw+XS5cuiUjVXWItIvLpp5+Kv7+/2NvbK+fO6Kc5fPhw6dq1q0H81q1bpXXr1mJvby8BAQGyYsUKEblfiLm7u5t1/FZ/iXVAQIBynL0k/TkDJS9rLe+cmJLrJygoSGxsbJT38+bNk6CgIHF0dJT69etL586dZdeuXRXO1VIGDx4sPj4+otVqxdfXVwYOHCinT59WPje1zhMTE02uc6qc6mhjSp87o3fw4EGDP6wPusT6119/NbrEuqxXySJGn7u+iAkLC5MGDRqYbF9KTvtBl1jv2bNHGZ6dnS1169aVV1991Wic06dPi0ajkeeee05pX5555hkBIOPGjRORsrf1unXrikajeeC2XpFLrLVarZw6darMaZhSkfZl+PDhotPplCJG377g/5//pG9fjh8/Li+++KIsWrRIGbeqixhraGM0Ig9xII9qVLt27XDs2DHY2Njg1q1bcHV1VT6bNGkSlixZAhFBfHy8waV2AQEBCA4ONnln32PHjqFt27aIjY1V7ha5du1avPHGG2Xe7C4oKEg5fhwQEICGDRsa3ewOuH+ZqL5bWz/NefPm4fHHHzeKfe655zB16lT8+9//xuDBg/H000/DyckJqampWLZsGVJSUvD555/jjTfeqPD6Kn2zu1atWiE7OxubN2/Ghg0bMHjwYIOb3V24cAGBgYFYsGABpkyZUuZ0R4wYgS1btuD333+vcC5EltKyZUs0bNgQL7zwApo2bYri4mKcOHECCxcuRE5ODpKTk9GiRQuDm9299tpreOmll+Dl5YWbN28iPj4esbGx2LRpE1588cUHbvsff/wx3n//faSmpirn/owYMQJff/21cj5eaa1bt4aDg4My7ZI3u8vKylJudnfx4kXMmzfP4OacALBo0SJMnjwZgwYNwuDBg1G/fn2cPHkSc+bMQZ06dZCSkmJwjp1Go0Hfvn2NDmED92/Q+dRTT6Fbt264fPkyNmzYYDJn/aGlxMREdO/eXbnZXXFxsXKzu5iYGGRnZyMmJuaB5xqVRT/9b775BoMGDTL6vFu3brh58yZOnTqlDCuvHdfbsmULXnnlFSQkJKBbt24A7p93M2vWrDJvdvfUU08Z/C2xeg9VAlGNmjp1qgCQNm3aGH32j3/8QwCIvb290V5feb0l5fXElPVas2aNwbTLinvssccqPM3U1FQ5dOiQjBs3Tlq1aiVubm5ia2srDRo0kL59+xrcCMscly5dknHjxsnjjz8u9vb2otPp5Nlnn5X169dLcXGxQWxFrqIQMb7ZHVF12rx5swwdOlQaN24sdevWVXpAhg0bJmfOnDGILSwslHXr1kmPHj3Ezc1N7OzspEGDBtKvXz/ZuHGjcnO8B237Zd3srrzf9Llz5wymrX/Z2tpK/fr1JSQkRCIiIgz25Ev7+uuvpXPnzuLi4iJ2dnbSqFEjGTNmjMkbWJaXS2RkpIhIuVcnAVBuIKjvidG/7OzsxN3dXUJDQ2XGjBnK4ZPKetDhqgddnVQWc69OAgyvvFQD9sQQERGRKtX6m90RERGROj2Sl1hT7SEiD7zLpK2trcn72RARVYUH3WnYxsbGrMemUNm4FknVkpKSoNVqy32tW7euptMkolriwoULD2yTPvzww5pO85HBc2JI1XJycnD27NlyYwIDA3nHWSKqFvn5+WU+2FbP19fX7BvjkWksYoiIiEiVHtlzYoqLi3Ht2jW4uLjwfAgiCxMR5OTkwNfXt9Ye22cbQ1Q1zGlfHtki5tq1a0bPwCEiy7p8+XKlHlj3KGAbQ1S1KtK+PLJFjP4JwpcvXzbr7oMFBQXYu3cvwsLCVPfQO+ZeM2pj7tnZ2fDz8zN4UndtU5k2pjZuK9ZAzbkD6s6/Mrmb0748skWMvnvX1dXV7CLG2dkZrq6uqtxYmHv1q8251+bDKJVpY2rztlKT1Jw7oO78Hyb3irQvtfNgNhEREakeixgiIiJSpUf2cBJVvYBpuyw2LQdbwfx2QHDUHuQVWeYQxYW5z1tkOkRUMyzVxujbF3r0sIghIqJagTtJjx4eTiIiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlcwqYqKjo9G2bVu4uLjA09MTAwYMwNmzZw1iRARRUVHw9fWFk5MTunXrhtOnTxvE5OXlYcKECfDw8ECdOnXQv39/XLlyxSDm9u3bGDZsGHQ6HXQ6HYYNG4Y7d+5UbimJiIjokWNWEZOUlIRx48bh0KFDiI+PR2FhIcLCwnD37l0lZv78+Vi0aBGWLVuGo0ePwtvbG71790ZOTo4SExERge3bt2PTpk04cOAAfv/9d4SHh6OoqEiJGTp0KE6cOIG4uDjExcXhxIkTGDZsmAUWmYisEXeSiMhcZhUxcXFxGDFiBJo3b45WrVohNjYWly5dQkpKCoD7DcySJUswc+ZMDBw4EMHBwVi3bh1yc3OxceNGAEBWVhZiYmKwcOFC9OrVC61bt8b69etx8uRJfPfddwCAn376CXFxcfjss88QGhqK0NBQrFmzBv/85z+NGjUiejRwJ4mIzGX3MCNnZWUBANzc3AAAqampSE9PR1hYmBLj4OCArl27Ijk5GaNHj0ZKSgoKCgoMYnx9fREcHIzk5GT06dMHBw8ehE6nQ/v27ZWYDh06QKfTITk5GU2aNDHKJS8vD3l5ecr77OxsAEBBQQEKCgoqvEz6WHPGsRbVnbuDrVhuWjZi8K8lVNd6qI3bTFUsa1xcnMH72NhYeHp6IiUlBc8++6zRThIArFu3Dl5eXti4cSNGjx6t7CR9+eWX6NWrFwBg/fr18PPzw3fffYc+ffooO0mHDh1S2pg1a9YgNDQUZ8+eNdm+AJZpY2rjtvIwLNXGqLl9KTmv2rLdmBNb6SJGRDBp0iR07twZwcHBAID09HQAgJeXl0Gsl5cXLl68qMTY29ujfv36RjH68dPT0+Hp6Wk0T09PTyWmtOjoaMyaNcto+N69e+Hs7Gzm0gHx8fFmj2Mtqiv3+e0sP82/tSm22LR2795tsWlVRG3aZnJzc6sok/+xpp0kwLJtTG3aVh6GpdsYNbcvQO3ZbsxpXypdxIwfPx4//vgjDhw4YPSZRqMxeC8iRsNKKx1jKr686UyfPh2TJk1S3mdnZ8PPzw9hYWFwdXUtd94lFRQUID4+Hr1794ZWq63weNagunMPjtpjsWk52Aj+1qYYfz1mg7zi8reVijoV1cci03mQ2rjN6Hshqoq17SQBlmljauO28jAs1caouX0Bat92Y077UqkiZsKECdixYwf279+Phg0bKsO9vb0B3G8kfHx8lOEZGRlKw+Pt7Y38/Hzcvn3boKHJyMhAx44dlZjr168bzffGjRtGDZieg4MDHBwcjIZrtdpKfemVHc8aVFfueUWWaQwMplmssdh0q/v7q03bTFUvp7XtJAGWbWNq07byMCzdxqi5fdHPszZsN+Yso1kn9ooIxo8fj23btmHfvn0IDAw0+DwwMBDe3t4G3Ub5+flISkpSCpSQkBBotVqDmLS0NJw6dUqJCQ0NRVZWFo4cOaLEHD58GFlZWUoMET2a9DtJCQkJZe4klVTWTlJ5MebuJBGRdTKriBk3bhzWr1+PjRs3wsXFBenp6UhPT8e9e/cA3N+7iYiIwJw5c7B9+3acOnUKI0aMgLOzM4YOHQoA0Ol0GDlyJCZPnozvv/8ex48fx5/+9Ce0aNFCORGvWbNm6Nu3L0aNGoVDhw7h0KFDGDVqFMLDw8s8Xk1E6sadJCIyl1mHk1asWAEA6Natm8Hw2NhYjBgxAgAwdepU3Lt3D2PHjsXt27fRvn177N27Fy4uLkr84sWLYWdnh1dffRX37t1Dz549sXbtWtja2ioxGzZswLvvvqucoNe/f38sW7asMstIRCowbtw4bNy4Ed9++62ykwTc3/FxcnIy2Elq3LgxGjdujDlz5pS5k+Tu7g43NzdMmTKlzJ2kVatWAQDefvtt7iQRqZBZRYzIgy9P02g0iIqKQlRUVJkxjo6OWLp0KZYuXVpmjJubG9avX29OekSkYtxJIiJzPdR9YoiILIU7SURkLj4AkoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsSb3dEjK2DaLotN68Lc5y02LSIisgwWMbWMJf+wExER1SQWMURERGay9A4he3srh0UMERFZBHt6qbrxxF4iIiJSJRYxREREpEosYoiIiEiVWMQQERGRKrGIISIiIlViEUNERESqxCKGiIiIVIlFDBEREakSixgiIiJSJRYxREREpEosYoiIiEiVWMQQERGRKrGIISIiIlViEUNERESqxCKGiIiIVIlFDBEREakSixgiIiJSJRYxREREpEosYoiIiEiVWMQQERGRKrGIISIiIlViEUNERESqxCKGiIiIVIlFDBEREakSixgiIiJSJRYxREREpEosYoiIiEiV7Go6ASIiqjkB03ZVKM7BVjC/HRActQd5RZoqzoqoYljEEBER1bDyiklzC8gLc5+3ZGpWzeqLmOXLl2PBggVIS0tD8+bNsWTJEnTp0qWm06o2Fd1LArinVJUs2cAAtauRsWa1vX0hUjurPidm8+bNiIiIwMyZM3H8+HF06dIF/fr1w6VLl2o6NSJSObYvROpn1UXMokWLMHLkSLz11lto1qwZlixZAj8/P6xYsaKmUyMilWP7QqR+Vns4KT8/HykpKZg2bZrB8LCwMCQnJxvF5+XlIS8vT3mflZUFALh16xYKCgoqPN+CggLk5uYiMzMTWq22Urm3j/6+UuOZYs4XZFcsyM0thl2BDYqK1XU4qbblnpmZWcVZVUxlt/ecnBwAgIhUVWpVytz2BbBMG2OJ9sXS7ArvViyulv1GrYm5+VtL+wJUbps3p32x2iLm5s2bKCoqgpeXl8FwLy8vpKenG8VHR0dj1qxZRsMDAwOrLEdrNLSmE3gItSl3j4VVkka1y8nJgU6nq+k0zGZu+wKwjQFq12/U2piTf21qX6y2iNHTaAyrThExGgYA06dPx6RJk5T3xcXFuHXrFtzd3U3GlyU7Oxt+fn64fPkyXF1dK594DWDuNaM25i4iyMnJga+vbxVmV/Uq2r4AlmljauO2Yg3UnDug7vwrk7s57YvVFjEeHh6wtbU12ivKyMgw2nsCAAcHBzg4OBgMq1evXqXn7+rqqrqNRY+514zalrsae2D0zG1fAMu2MbVtW7EWas4dUHf+5uZe0fbFak/stbe3R0hICOLj4w2Gx8fHo2PHjjWUFRE9Cti+ED0arLYnBgAmTZqEYcOGoU2bNggNDcXq1atx6dIlvPPOOzWdGhGpHNsXIvWz6iJm8ODByMzMxIcffoi0tDQEBwdj9+7d8Pf3r7J5Ojg4IDIy0qjbWA2Ye81g7urE9sU8zL3mqDn/qs5dI2q9RpKIiIhqNas9J4aIiIioPCxiiIiISJVYxBAREZEqsYghIiIiVar1RUxAQAA0Go3Bq/TzVEoTEURFRcHX1xdOTk7o1q0bTp8+XU0Z/8+FCxcwcuRIBAYGwsnJCUFBQYiMjER+fr7J+LVr1xotq/7l5+eHjIwMg/ioqChoNBrcvHlTGSYi2LRpE7p06QJPT084OjqiYcOG6NOnDz777DMAwIgRI8qcT8lXv379lOnevHkTDg4O0Gg0OHbsGAAgKSkJISEhcHR0xOOPP47Q0FBoNBo0b94cRUVFRsun0Wgwfvx4o+HXr1/HtGnT0KJFC9StWxeOjo5o3LgxJk6ciHPnzhktb1kvd3d3DBgwAGfPni33e0lMTDQ5/s8//1zueJZmanm8vb3LHaf0Ol+5cmU1ZfvoUmsbY277omfq99+hQ4cqz3f58uUIDAyEo6MjQkJC8MMPP5Qbbw3benR0NNq2bQsXFxd4enqqqn0BrKSNkVrO399fPvzwQ0lLS1NeOTk55Y4zd+5ccXFxka1bt8rJkydl8ODB4uPjI9nZ2dWU9X3/+te/ZMSIEbJnzx757bff5NtvvxVPT0+ZPHmyyfjY2FgBIJ06dZIOHTrI9u3bZc2aNTJq1ChxdXUVNzc3iY+PV+IjIyMFgNy4cUMZ9sEHHwgAGTVqlHz77beyb98+iY2NlWHDhkl4eLiIiPz6669y8OBB5fXpp58KALGxsZFp06bJV199Ja+++qo4OTnJxYsXRURk0aJFAkAAyDvvvCPnz58XZ2dnmThxopw5c0bWrFkjGo1Gifnss8+Mlg+AjBs3zmDY4cOHpUGDBuLh4SFRUVGyZ88eSUhIkJUrV0rnzp2lXr16RssbFxen5N6+fXv5y1/+Ihs2bJAjR47I888/L40aNZLff/+9zO8lISFBAMjZs2cNtqvCwsIKfKuWExkZKc2bNzfIISMjo8x4U+tcq9XKli1bqjHrR49a2xhz2xe94cOHS9++fQ2WNzMzs0pz3bRpk2i1WlmzZo2cOXNGJk6cKHXq1FHal9KsZVvv06ePxMbGyqlTp+TEiROqal9ErKONYRHj7y+LFy+ucHxxcbF4e3vL3LlzlWF//PGH6HQ6WblyZRVkaJ758+dLYGCgyc/0Rczzzz8vL774osFnFy9eFD8/P3FxcZH09HQRMS5icnNzxcHBQf785z+bnH5RUZHJ4fofXe/evQ2GN23aVKZNmyYiIsHBweLp6Slt27YVnU4n7733njRt2tQg/sknnxQbGxvp0qWLPPbYY5Kbm2vweekiJisrS7y9vcXPz08uX75sMrdvvvlG+b+poq20jIwMASBJSUllxuiX9/bt22XGVIfIyEhp1apVheOnTp1qtM5Hjx4tHTp0sHBmtcuj1MaU177oDR8+3Kh9qWrt2rWTd955x2BYyfalNGvd1tXUvohYRxtT6w8nAcC8efPg7u6Op59+Gh999FG53aWpqalIT09HWFiYMszBwQFdu3ZFcnJydaRbrqysLLi5uT0wLjExEZ6ennjyyScxatQoODo6YuHChcjJycGqVatMjnP37l3k5eXBx8fH5Oc2NqY3p4KCAgBAq1atDIaHhYUhOTkZhw8fxqlTpzBs2DCMGjUKWVlZ2L17t8E6BoDHHnsMxcXF+Oijj3D16lV88skn5S7jmjVrkJ6ejvnz56Nhw4YmYwYNGlTuNErLysoCgAqt49atW8PHxwc9e/ZEQkKCWfOxlHPnzsHX1xeBgYEYMmQIzp8/X2bswYMHjdZ5nz59cOzYMeU7pMp5VNqYyrYvpQ9VW1J+fj5SUlKMtl19+2KKtW7ramtfgJpvY2p9ETNx4kRs2rQJCQkJGD9+PJYsWYKxY8eWGa9/YFzph8R5eXkZPUyuuv32229YunTpA2+b3rFjR2zYsAH79u3DwoULcfToUfTo0QM9e/aEra0t9u/fb3I8Dw8PPPHEE1i+fDkWLVqEn3/+GVKBeyXqf5ilH5anX2cxMTEAgDfffBNDhgyBs7MzLl26ZLSOHR0dAQBPPPEEXnrpJcybNw+3bt0qc7579+6Fra0tXnjhhQfmWFJRUREKCwsNXkVFRRARTJo0CZ07d0ZwcHCZ4/v4+GD16tXYunUrtm3bhiZNmqBnz55lrteq0r59e3zxxRfYs2ePUtB17NgRmZmZJuPT09NNbteFhYUG50WReR6VNqai7Uu/fv1Mti95eXlVktfNmzdRVFRk1vqyxm1dbe0LYCVtTKX7cKyY/rBAea+jR4+aHHfLli0CQG7evGny83//+98CQK5du2Yw/K233pI+ffrUWP5Xr16VJ554QkaOHFnmdPWHk0qPe+3aNdFqtbJ161bx8vKSZs2aGeRR8vDKkSNHpFGjRkoeLi4uEh4eLl988YUUFxebnO8333wjAGT27NkGw2fPni2NGzcWV1dXg+7E4cOHCwCjY+/9+vUTAJKWliY///yz2NraGsSg1OGkpk2bire3d5nro7Ty1ntQUJCMHTtW/P39yzw0VZ7w8HB54YUXzB7Pkn7//Xfx8vKShQsXmvy8cePGMmfOHINhBw4cUNY5/Y+a25iqal/KUrJ9qQpXr14VAJKcnGwwfPbs2dKkSROT41jjtq729kWkZtoYq352UmWNHz8eQ4YMKTcmICDA5HD9WfS//vor3N3djT7Xn3mdnp5ucFglIyPDqMKsLHPzv3btGrp37648xM5cPj4+8Pf3x7lz5x7Ys9K2bVv8+uuv2LdvH/bv349jx47h+++/xz//+U98/fXX2LFjBzQajcE4+keq37lzx2B4RkYGNBoNsrOz8eabbyrD33zzTaxbt85oz+KPP/4AALi7u8Pb2xsjR47EsmXL8O6776JRo0ZmL3dZvvvuO6PHwC9atAg7duzA/v37yzw0VZ4OHTpg/fr1lkqxUurUqYMWLVoYXJFVkre3t9Gea0ZGBuzs7Ez+FmozNbcxNdm+VAUPDw/Y2tqa3HbLWl/Wtq1PmDBB9e0LUDNtzCNZxHh4eMDDw6NS4x4/fhwAyjzvIzAwEN7e3oiPj0fr1q0B3D8mm5SUhHnz5lUu4VLMyf/q1avo3r07QkJCEBsbW+Z5KeXJzMzE5cuX4ebmhszMTLRo0aLceK1Wiz59+qBPnz7K+IMGDcI///lP/Otf/8Jzzz1nFA8AP/74o8Hw+Ph45ObmwtHREX379lWKnJYtW0Kn0+H48eMoKiqCra2tsqw2NjbK9KKiorB+/Xr89a9/xbp164zybNSoEc6dO4e7d++iTp06FV4frVq1Uta/iGDChAlISkpCYmIiAgMDKzydko4fP17mNlVd8vLy8NNPP6FLly4mPw8NDcXOnTsNhu3duxdt2rRR1jndp+Y2pqbal6ra/u3t7RESEoL4+Hi89NJLyvD4+Hi8+OKLJsexlm1d375s375d9e0LUENtTKX6bx4RycnJsmjRIjl+/LicP39eNm/eLL6+vtK/f3+DuCZNmsi2bduU93PnzhWdTifbtm2TkydPymuvvVYjl1jru3h79OghV65cMbjMzVT++sNJr7/+uiQnJ0tqaqokJCRIaGioPPbYY7J27VoBIH/7299EpGJX6+j94x//EAAyb948o8/0Z9Pb2tpKTEyMnDlzRiIiIsTJyemB3doDBgyQM2fOSExMjGg0GnF0dDSY9owZM8TGxkb++9//Gh1OWrhwoQCQr776qkLr09TyjhkzRnQ6nSQmJhqs35JXRk2bNk2GDRumvF+8eLFs375dfvnlFzl16pRMmzZNAFRZd3pZJk+eLImJiXL+/Hk5dOiQhIeHi4uLi1y4cMFk3vrLH9977z1lnfMS64ej5jbG3PZFRCQnJ0cmT55ssn2pytz1l1iXbF/q1Klj9du6mtsXEetoY2p1EZOSkiLt27cXnU4njo6O0qRJE4mMjJS7d+8axAGQ2NhY5X1xcbFERkaKt7e3ODg4yLPPPisnT56s5uz/d46LqVdJ+vz18e3bt5cGDRqIVquVRo0ayfDhw+XgwYPi5+cnOp1Ouc6/9B/1/Pz8Mo/jR0dHCwD58ssvjT7TFzEjR44Uf39/sbe3l2eeeUZee+01ASBr1qyRPn36SKtWrSQhIUESEhJk9+7dYmdnJ/Xq1RN7e3sJCAiQDh06SJ06dQymnZWVJR4eHsr5MiWLmDt37iiXWF+5csVk3iV/+KaKmLLWb8ntYfjw4dK1a1fl/bx58yQoKEgcHR2lfv360rlzZ9m1a5fJ+Vcl/b1FtFqt+Pr6ysCBA+X06dNl5i0ikpiYKK1bt1bW+YoVK6o560eLmtsYc9sXkfu3YQgLCzNqXy5dulTl+X766acG7UvJy5StdVtXc/siYh1tjEakApeX0CNh7dq1eOONNxAbG4umTZuisLAQGRkZ+OGHHxAbGwtbW1ts2bIF3bt3B3D/cM2sWbNw48YNeHh44ObNmwgICMArr7yCXr16wc/PD7///jsSExPxySefICgoCMeOHYOzs7PBfBMTE9G9e3d88803yiXNhYWF8PPzQ/369XHmzBmT+b788svYuXMnrl69igYNGmDEiBHYsmULfv/9d4O4JUuW4L333gMAjBs3DsuWLVM+O3LkCMLDwwHcPxcgNDQU9vb2OHfuHNavX4///ve/uH37tsHyxsXFGZ0TAwBPPfUUXF1dK7PqiYioCjyS58RQ+d544w0A948l16tXD82aNcMHH3yAt956Cw0aNChzPFdXV8yaNQvff/89ZsyYgevXr0Oj0SAwMBARERH44IMPjAqYsuzatQvp6enl3n797bffxrZt2/Dll19i0qRJZcaNHTsWf//735Gammr0Wbt27XDy5EksXrwYX3/9NebNm4eioiL4+fmhZ8+eBgWPXt++fU3OJz4+Hr169arA0hERUXVgTwwRERGpUq2/2R0RERGpE4sYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVemTvE1NcXIxr167BxcXF6IGERPRwRAQ5OTnw9fWt1PN0iIgs4ZEtYq5duwY/P7+aToPokXb58uVKPXWXiMgSHtkixsXFBcD9RtacW8UXFBRg7969CAsLU92Te5l7zaiNuWdnZ8PPz0/5nRER1YRHtojRH0JydXU1u4hxdnaGq6urKv8gMffqV5tz56FaIqpJPJhNREREqsQihoiIiFTpkT2cRFUvYNoui03LwVYwvx0QHLUHeUWWOURxYe7zFpkOERFZJ/bEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlVjEEBERkSqxiCEiIiJVYhFDREREqsQihoiIiFSJRQwRERGpEosYIiIiUiUWMURERKRKLGKIiIhIlcwqYqKjo9G2bVu4uLjA09MTAwYMwNmzZw1iRARRUVHw9fWFk5MTunXrhtOnTxvE5OXlYcKECfDw8ECdOnXQv39/XLlyxSDm9u3bGDZsGHQ6HXQ6HYYNG4Y7d+5UbimJiIjokWNWEZOUlIRx48bh0KFDiI+PR2FhIcLCwnD37l0lZv78+Vi0aBGWLVuGo0ePwtvbG71790ZOTo4SExERge3bt2PTpk04cOAAfv/9d4SHh6OoqEiJGTp0KE6cOIG4uDjExcXhxIkTGDZsmAUWmYiIiB4FduYEx8XFGbyPjY2Fp6cnUlJS8Oyzz0JEsGTJEsycORMDBw4EAKxbtw5eXl7YuHEjRo8ejaysLMTExODLL79Er169AADr16+Hn58fvvvuO/Tp0wc//fQT4uLicOjQIbRv3x4AsGbNGoSGhuLs2bNo0qSJUW55eXnIy8tT3mdnZwMACgoKUFBQUOFl1MeaM461qO7cHWzFctOyEYN/LaG61kNt3GbUuKxE9Ogxq4gpLSsrCwDg5uYGAEhNTUV6ejrCwsKUGAcHB3Tt2hXJyckYPXo0UlJSUFBQYBDj6+uL4OBgJCcno0+fPjh48CB0Op1SwABAhw4doNPpkJycbLKIiY6OxqxZs4yG7927F87OzmYvW3x8vNnjWIvqyn1+O8tP829tii02rd27d1tsWhVRm7aZ3NzcKsqEiKjiKl3EiAgmTZqEzp07Izg4GACQnp4OAPDy8jKI9fLywsWLF5UYe3t71K9f3yhGP356ejo8PT2N5unp6anElDZ9+nRMmjRJeZ+dnQ0/Pz+EhYXB1dW1wstVUFCA+Ph49O7dG1qttsLjWYPqzj04ao/FpuVgI/hbm2L89ZgN8oo1Fpnmqag+FpnOg9TGbUbf00lEVJMqXcSMHz8eP/74Iw4cOGD0mUZj+EdIRIyGlVY6xlR8edNxcHCAg4OD0XCtVlupPyyVHc8aVFfueUWWKTYMplmssdh0q/v7q03bjFqXk4geLZW6xHrChAnYsWMHEhIS0LBhQ2W4t7c3ABj1lmRkZCi9M97e3sjPz8ft27fLjbl+/brRfG/cuGHUy0NERES1k1lFjIhg/Pjx2LZtG/bt24fAwECDzwMDA+Ht7W1wfD0/Px9JSUno2LEjACAkJARardYgJi0tDadOnVJiQkNDkZWVhSNHjigxhw8fRlZWlhJDREREtZtZh5PGjRuHjRs34ttvv4WLi4vS46LT6eDk5ASNRoOIiAjMmTMHjRs3RuPGjTFnzhw4Oztj6NChSuzIkSMxefJkuLu7w83NDVOmTEGLFi2Uq5WaNWuGvn37YtSoUVi1ahUA4O2330Z4eLjJk3qJiIio9jGriFmxYgUAoFu3bgbDY2NjMWLECADA1KlTce/ePYwdOxa3b99G+/btsXfvXri4uCjxixcvhp2dHV599VXcu3cPPXv2xNq1a2Fra6vEbNiwAe+++65yFVP//v2xbNmyyiwjERERPYLMKmJEHnwPD41Gg6ioKERFRZUZ4+joiKVLl2Lp0qVlxri5uWH9+vXmpEdERES1CJ+dRERERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUyazHDpD6BUzbVdMpVBtLLuuFuc9bbFpERGQZ7IkhIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlFjFERESkSixiiIiISJVYxBAREZEqsYghIiIiVWIRQ0RERKrEIoaIiIhUiUUMERERqRKLGCIiIlIlu5pOgMoXMG1XhWMdbAXz2wHBUXuQV6Spwqxqn/K+h8qs9wtzn7dUakREtZbV98QsX74cgYGBcHR0REhICH744YeaTomIiIisgFUXMZs3b0ZERARmzpyJ48ePo0uXLujXrx8uXbpU06kRERFRDbPqImbRokUYOXIk3nrrLTRr1gxLliyBn58fVqxYUdOpERERUQ2z2nNi8vPzkZKSgmnTphkMDwsLQ3JyslF8Xl4e8vLylPdZWVkAgFu3bqGgoKDC8y0oKEBubi4yMzOh1Wormb3l2BXerXhssSA3txh2BTYoKlbXOTG1LffMzMwqzqpiKru95+TkAABEpKpSIyJ6IKstYm7evImioiJ4eXkZDPfy8kJ6erpRfHR0NGbNmmU0PDAwsMpytEZDazqBh1CbcvdYWCVpVLucnBzodLqaToOIaimrLWL0NBrDPVsRMRoGANOnT8ekSZOU98XFxbh16xbc3d1NxpclOzsbfn5+uHz5MlxdXSufeA1g7jWjNuYuIsjJyYGvr28VZkdEVD6rLWI8PDxga2tr1OuSkZFh1DsDAA4ODnBwcDAYVq9evUrP39XVVXV/kPSYe82obbmzB4aIaprVnthrb2+PkJAQxMfHGwyPj49Hx44daygrIiIishZW2xMDAJMmTcKwYcPQpk0bhIaGYvXq1bh06RLeeeedmk6NiIiIaphVFzGDBw9GZmYmPvzwQ6SlpSE4OBi7d++Gv79/lc3TwcEBkZGRRoem1IC51wzmTkRUMzTCaySJiIhIhaz2nBgiIiKi8rCIISIiIlViEUNERESqxCKGiIiIVIlFDBEREalSrS9iAgICoNFoDF6lHzpZmoggKioKvr6+cHJyQrdu3XD69Olqyvh/Lly4gJEjRyIwMBBOTk4ICgpCZGQk8vPzyx1vxIgRRsvcoUOHKs93+fLlCAwMhKOjI0JCQvDDDz+UG5+UlISQkBA4Ojri8ccfx8qVK6s8x9Kio6PRtm1buLi4wNPTEwMGDMDZs2fLHScxMdFo/Wo0Gvz888/VlPV9UVFRRjl4e3uXO441rHMiooqq9UUMAOU+NPrXX/7yl3Lj58+fj0WLFmHZsmU4evQovL290bt3b+XJvtXl559/RnFxMVatWoXTp09j8eLFWLlyJWbMmPHAcfv27WuwzLt3767SXDdv3oyIiAjMnDkTx48fR5cuXdCvXz9cunTJZHxqaiqee+45dOnSBcePH8eMGTPw7rvvYuvWrVWaZ2lJSUkYN24cDh06hPj4eBQWFiIsLAx37z746eJnz541WMeNGzeuhowNNW/e3CCHkydPlhlrLeuciKjCpJbz9/eXxYsXVzi+uLhYvL29Ze7cucqwP/74Q3Q6naxcubIKMjTP/PnzJTAwsNyY4cOHy4svvlg9Cf1/7dq1k3feecdgWNOmTWXatGkm46dOnSpNmzY1GDZ69Gjp0KFDleVYERkZGQJAkpKSyoxJSEgQAHL79u3qS8yEyMhIadWqVYXjrXWdExGVhT0xAObNmwd3d3c8/fTT+Oijj8o9HJOamor09HSEhYUpwxwcHNC1a1ckJydXR7rlysrKgpub2wPjEhMT4enpiSeffBKjRo1CRkZGleWUn5+PlJQUg3UGAGFhYWWus4MHDxrF9+nTB8eOHUNBQUGV5fogWVlZAFChddy6dWv4+PigZ8+eSEhIqOrUTDp37hx8fX0RGBiIIUOG4Pz582XGWus6JyIqS60vYiZOnIhNmzYhISEB48ePx5IlSzB27Ngy4/VP1S79JG0vLy+jJ25Xt99++w1Lly594LOl+vXrhw0bNmDfvn1YuHAhjh49ih49eiAvL69K8rp58yaKiorMWmfp6ekm4wsLC3Hz5s0qyfNBRASTJk1C586dERwcXGacj48PVq9eja1bt2Lbtm1o0qQJevbsif3791djtkD79u3xxRdfYM+ePVizZg3S09PRsWNHZGZmmoy3xnVORFQeq352UmVFRUVh1qxZ5cYcPXoUbdq0wXvvvacMa9myJerXr49BgwYpvTNl0Wg0Bu9FxGhYZZmTv961a9fQt29fvPLKK3jrrbfKHXfw4MHK/4ODg9GmTRv4+/tj165dGDhw4MMlXw5z15mpeFPDq8v48ePx448/4sCBA+XGNWnSBE2aNFHeh4aG4vLly/j444/x7LPPVnWain79+in/b9GiBUJDQxEUFIR169Zh0qRJJsextnVORFSeR7KIGT9+PIYMGVJuTEBAgMnh+qt0fv31V5NFjP7qjvT0dPj4+CjDMzIyjPZiK8vc/K9du4bu3bsrT/o2l4+PD/z9/XHu3Dmzx60IDw8P2NraGvW6lLfOvL29Tcbb2dmVW1xWlQkTJmDHjh3Yv38/GjZsaPb4HTp0wPr166sgs4qrU6cOWrRoUeb3bG3rnIjoQR7JIsbDwwMeHh6VGvf48eMAYFCglBQYGAhvb2/Ex8ejdevWAO6f85GUlIR58+ZVLuFSzMn/6tWr6N69O0JCQhAbGwsbG/OPEGZmZuLy5ctlLvPDsre3R0hICOLj4/HSSy8pw+Pj4/Hiiy+aHCc0NBQ7d+40GLZ37160adMGWq22SvI0RUQwYcIEbN++HYmJiQgMDKzUdI4fP15l67ei8vLy8NNPP6FLly4mP7eWdU5EVGE1eVZxTUtOTpZFixbJ8ePH5fz587J582bx9fWV/v37G8Q1adJEtm3bpryfO3eu6HQ62bZtm5w8eVJee+018fHxkezs7GrN/+rVq/LEE09Ijx495MqVK5KWlqa8yso/JydHJk+eLMnJyZKamioJCQkSGhoqjz32WJXmv2nTJtFqtRITEyNnzpyRiIgIqVOnjly4cEFERKZNmybDhg1T4s+fPy/Ozs7y3nvvyZkzZyQmJka0Wq1s2bKlynI0ZcyYMaLT6SQxMdFg/ebm5ioxpXNfvHixbN++XX755Rc5deqUTJs2TQDI1q1bqzX3yZMnS2Jiopw/f14OHTok4eHh4uLiYvXrnIioomp1EZOSkiLt27cXnU4njo6O0qRJE4mMjJS7d+8axAGQ2NhY5X1xcbFERkaKt7e3ODg4yLPPPisnT56s5uxFYmNjBYDJV0kl88/NzZWwsDBp0KCBaLVaadSokQwfPlwuXbpU5fl++umn4u/vL/b29vLMM88YXKY8fPhw6dq1q0F8YmKitG7dWuzt7SUgIEBWrFhR5TmWVtb6Lbk9lM593rx5EhQUJI6OjlK/fn3p3Lmz7Nq1q9pzHzx4sPj4+IhWqxVfX18ZOHCgnD59usy8RaxjnRMRVZRG5P+fuUdERESkIrX+EmsiIiJSJxYxREREpEosYoiIiEiVWMQQERGRKrGIISIiIlViEUNERESqxCKGiIiIVIlFDBEREakSixgiIiJSJRYxREREpEosYoiIiEiV/h9r7cSd0no9KgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train[continuous].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encodign of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing with the continuous variables, we will apply hot encoding to our categorical variables, which are:\n",
    "- Q_year\n",
    "- Fortnight\n",
    "- Week_info\n",
    "- Ac\n",
    "- Gr_o\n",
    "- Gr_d\n",
    "- ArrivalDayNight\n",
    "- DepartureDayNight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for categorical variables\n",
    "categorical = ['Q_YEAR', 'FORTNIGHT', 'WEEK_INFO', 'AC', 'GR_O','GR_D', 'ArrivalDayNight', 'DepartureDayNight']\n",
    "\n",
    "flights= pd.get_dummies(flights, columns=categorical)\n",
    "X_train = pd.get_dummies(X_train, columns = categorical)\n",
    "X_test = pd.get_dummies(X_test, columns = categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will start testing some initial models and check what accuracy are we getting. First of all we need to split our variables into features (i.e.: X) and target variable (i.e.: y). However recall that we already split before in order to infer from X_train which transformations may be necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data, we can initialize the model we will be using to predict flight delays. Then, using the fraction of the data we have selected for this purpouse, we will fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "GkrBH5jr6jDJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained, we are abla to make predictions based on the testing data. This will produce a one-dimentional vector containing 1's or 0's depending on if the model believes that flight is going to be delayed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.83698761 0.83984747 0.83460439 0.82268827 0.81696854]\n",
      "Mean Accuracy: 0.8302192564346997\n",
      "Standard Deviation: 0.008832209587721969\n",
      "Cross-Validation F1 Scores: [0.8225727  0.82526929 0.82127256 0.80499228 0.80238153]\n",
      "Mean F1 Score: 0.8152976721552498\n",
      "Standard Deviation: 0.009602981341004659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Assuming you have already initialized and fitted your logistic regression model\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_accuracy = cross_val_score(logreg, X_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_scores_f1 = cross_val_score(logreg, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores_accuracy)\n",
    "print(\"Mean Accuracy:\", cv_scores_accuracy.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_accuracy.std())\n",
    "\n",
    "print(\"Cross-Validation F1 Scores:\", cv_scores_f1)\n",
    "print(\"Mean F1 Score:\", cv_scores_f1.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_f1.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an 83% of accuracy of mean, which is not bad at all specially when taking into account that only a small fraction of the available data has been used troughout the training and testing of the model.\n",
    "\n",
    "Nextly we will check if getting a smaller subset of variables we can get better results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME',\n",
      "       'DISTANCE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the feature selection model\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(random_state=42), threshold='mean')\n",
    "\n",
    "# Fit the feature selector on the training data\n",
    "feature_selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training and test sets to include only the selected features\n",
    "X_train_selected = feature_selector.transform(X_train)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "\n",
    "# Get the original feature names for the selected features\n",
    "selected_feature_names = X.columns[selected_feature_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf = X_train[selected_feature_names]\n",
    "X_test_sf = X_test[selected_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train_sf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.83317445 0.83651096 0.8250715  0.82173499 0.81172545]\n",
      "Mean Accuracy: 0.8256434699714014\n",
      "Standard Deviation: 0.008765074939159272\n",
      "Cross-Validation F1 Scores: [0.81831471 0.82114188 0.81054187 0.80394385 0.79711529]\n",
      "Mean F1 Score: 0.810211518807203\n",
      "Standard Deviation: 0.008899871777217218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Assuming you have already initialized and fitted your logistic regression model\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_accuracy = cross_val_score(logreg, X_train_sf, y_train, cv=5, scoring='accuracy')\n",
    "cv_scores_f1 = cross_val_score(logreg, X_train_sf, y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores_accuracy)\n",
    "print(\"Mean Accuracy:\", cv_scores_accuracy.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_accuracy.std())\n",
    "\n",
    "print(\"Cross-Validation F1 Scores:\", cv_scores_f1)\n",
    "print(\"Mean F1 Score:\", cv_scores_f1.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_f1.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model with all the features gives better predictions, but they are kind of similar, so we will keep both subset of variables since they both work good and this new subset is much smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric for choosing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we are going to decide which of the metrics to use in order to compare our models and decide which of them is the best. We firstly observe the balance between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Balance:\n",
      "0    62.411348\n",
      "1    37.588652\n",
      "Name: DELAYED, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your target variable is stored in a DataFrame or Series named \"y\"\n",
    "class_counts = y.value_counts()\n",
    "class_balance = class_counts / len(y) * 100\n",
    "\n",
    "print(\"Class Balance:\")\n",
    "print(class_balance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With class 0 at 62.41% and class 1 at 37.59%, there is a noticeable difference in class frequencies, but it does not indicate severe class imbalance. However, it is still worth considering the implications of the imbalance and how it might affect our modeling approach.\n",
    "\n",
    "When dealing with imbalanced data, it is important to be aware that standard evaluation metrics like accuracy may not provide an accurate representation of model performance. Instead, we need to focus on metrics that are more robust to class imbalance, such as precision, recall, F1 score, or area under the ROC curve (AUC-ROC).\n",
    "\n",
    "Nevertheless, since the imbalance is not that big, we will also consider the accuracy score in order to choose our models. We mainly will take into account the F1-score (taking into account then Recall and precision that will take into account the slightly unbalanced classes that we have). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "To start with: We remember some of the options we have to optimize the learning of our models\n",
    "\n",
    "**1. Hyperparameter Tuning**: Fine-tune the hyperparameters of your model to find the optimal combination for your specific problem. Use techniques like grid search, random search, or Bayesian optimization to systematically explore the hyperparameter space and identify the best configuration.\n",
    "\n",
    "\n",
    "**2. Cross-Validation**: Utilize cross-validation techniques, such as k-fold cross-validation, to evaluate your model's performance more reliably. This helps to assess how well your model generalizes to unseen data and reduces the risk of overfitting.\n",
    "\n",
    "**3. Regularization**: Apply regularization techniques like L1 or L2 regularization (e.g., LASSO, Ridge) to prevent overfitting. Regularization helps in reducing the complexity of the model by adding a penalty term to the loss function, promoting simpler models that generalize better.\n",
    "\n",
    "\n",
    "**4. Ensemble Methods**: Explore ensemble methods like bagging, boosting, or stacking to combine multiple models and improve overall performance. Ensemble techniques can help capture different patterns in the data and reduce model variance.\n",
    "\n",
    "\n",
    "**5. Data Augmentation**: If you have limited data, consider data augmentation techniques to artificially increase the size of your training set. This can involve techniques like rotation, flipping, zooming, or adding noise to your existing data.\n",
    "\n",
    "\n",
    "**6. Early Stopping**: Implement early stopping during model training to prevent overfitting and find the optimal number of training epochs. Early stopping stops training when the model's performance on a validation set starts to deteriorate.\n",
    "\n",
    "\n",
    "**7. Batch Normalization**: Apply batch normalization techniques to normalize the activations of the previous layer in \n",
    "\n",
    "**8. Model Architecture**: Experiment with different model architectures or network architectures, such as adding or removing layers, adjusting layer sizes, or trying different activation functions. It can help to explore pre-trained models or architectures specifically designed for your problem domain.\n",
    "\n",
    "\n",
    "**9. Monitoring and Debugging**: Monitor your model during training, track performance metrics, and analyze learning curves to identify issues like underfitting, overfitting, or convergence problems. Debug any potential errors, explore misclassified samples, and consider adjusting your model or data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data-frame to store results: \n",
    "results_df = pd.DataFrame(index=[], columns= ['Accuracy', 'F1 Macro', 'Precision Macro', 'Recall Macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVC)\n",
    "First of all we recall the hyperparameters we are going to tune:\n",
    "\n",
    "1.C: Regularization parameter. It controls the trade-off between allowing training errors and maximizing the margin. Larger values of C penalize errors more, leading to a potentially narrower margin.\n",
    "\n",
    "2.kernel: Specifies the type of kernel function used for mapping the input data to a higher-dimensional feature space. Common options include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "3.gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels. It defines the influence of training samples on the decision boundary.\n",
    "\n",
    "We fit the SVC using the whole subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT  0.833937  0.817714        0.829882     0.810271"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only using the best features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT     0.833937  0.817714        0.829882     0.810271\n",
       "SVC subset features Before HT  0.832793  0.816053        0.829476     0.808078"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have very similar results, let us compute the hyperparameter tuning with the SVC using only the small part of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.8430095899394434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize the SVC model\n",
    "clf = SVC()\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit SVC using the hyperparameters that we found and using both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT     0.833937  0.817714        0.829882     0.810271\n",
       "SVC subset features Before HT  0.832793  0.816053        0.829476     0.808078\n",
       "SVC all features After HT      0.858055  0.842509        0.862018     0.831795"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='rbf', gamma = 'auto', C=10)\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  F1 Macro Precision Macro Recall Macro\n",
       "SVC all features Before HT     0.833937  0.817714        0.829882     0.810271\n",
       "SVC subset features Before HT  0.832793  0.816053        0.829476     0.808078\n",
       "SVC all features After HT      0.858055  0.842509        0.862018     0.831795\n",
       "SVC subset features after HT   0.860153   0.84301        0.870671     0.829687"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(kernel='rbf', gamma = 'auto', C=10)\n",
    "clf.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(clf , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['SVC subset features after HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now observing that the subset of features is now doing better job than that of all the features surprisingly. The best combination is the last one we fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "The parameters GaussianNB are the following: Priors: Prior probabilities of the classes. If specified, the priors are not adjusted according to the data. Var_smoothing:Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "\n",
    "However we are only going to tune var_smoothing:\n",
    "\n",
    "To proceed, we are going to fit the Naive Bayes model for both subsets of the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                   0.833937  0.817714   \n",
       "SVC subset features Before HT                0.832793  0.816053   \n",
       "SVC all features After HT                    0.858055  0.842509   \n",
       "SVC subset features after HT                 0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT               0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT  0.767016  0.747318   \n",
       "\n",
       "                                            Precision Macro Recall Macro  \n",
       "SVC all features Before HT                         0.829882     0.810271  \n",
       "SVC subset features Before HT                      0.829476     0.808078  \n",
       "SVC all features After HT                          0.862018     0.831795  \n",
       "SVC subset features after HT                       0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                     0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT        0.752331      0.74385  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB()\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB()\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute hyperparameter tuning using the smallest subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__var_smoothing': 0.1}\n",
      "Best Score:  0.8328884652049571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.25 ,0.3 , 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] \n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We now fit the model using the best parameter we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB(var_smoothing = 0.1)\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,  KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "naive = GaussianNB(var_smoothing = 0.1)\n",
    "naive.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(naive , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gaussian Naive Bayes subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the best model is by far the one using the subset of features (the other one is probably overfitting) and in this case the hyperparamter tuning hasn't been helpful since the F1 score has decreased and the accuracy has almost remained the same. Therefore we belief that the best model is that of Gaussian Naive Bayes subset features After HT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "n_components: This hyperparameter specifies the number of components (dimensions) to retain after performing dimensionality reduction with LDA. By default, n_components is set to None, which means it will retain (n_classes - 1) components, where n_classes is the number of distinct classes in the data. In the param_grid, values of [None, 1, 2, 3] are provided to try different numbers of retained components. By setting n_components to a specific value, you can explicitly control the dimensionality of the reduced feature space.\n",
    "\n",
    "shrinkage: Shrinkage is a regularization technique used to improve the estimation of the covariance matrix in LDA, especially when the number of samples is small or the covariance matrix is ill-conditioned. The shrinkage hyperparameter controls the degree of shrinkage applied. \n",
    "In the param_grid, the values [None, 'auto', 0.1, 0.5] are provided to try different levels of shrinkage:\n",
    "None: No shrinkage is applied.\n",
    "'auto': Shrinkage is estimated using the Ledoit-Wolf lemma, which automatically determines the amount of shrinkage based on the data.\n",
    "0.1, 0.5: Specific values between 0 and 1 can be provided to manually set the shrinkage intensity.\n",
    "\n",
    "solver: This hyperparameter specifies the solver used for LDA computation. LDA can be solved using different algorithms, and the solver parameter determines the specific algorithm to use. \n",
    "In the param_grid, the values ['svd', 'lsqr', 'eigen'] are provided to try different solver algorithms:\n",
    "'svd': Singular Value Decomposition (SVD) solver, which computes the exact solution but can be slower for large datasets.\n",
    "'lsqr': Least Squares solver, which can handle both shrinkage and regularized covariance matrix.\n",
    "'eigen': Eigenvalue Decomposition solver, which computes the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "We now fit the Linear Discriminant Analysis classifier for both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.831459  0.816218   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.825102     0.810283  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.831459  0.816218   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.825102     0.810283  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do hyperparameter tuning with the small subset of features dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'n_components': None, 'shrinkage': None, 'solver': 'svd', 'tol': 1e-06}\n",
      "Best Score:  0.8314585319351764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "800 fits failed out of a total of 1200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 8 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 10 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 631, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 463, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"/Users/marcamps/.local/lib/python3.8/site-packages/scipy/linalg/_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 15 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 615, in fit\n",
      "    raise NotImplementedError(\"shrinkage not supported with 'svd' solver.\")\n",
      "NotImplementedError: shrinkage not supported with 'svd' solver.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/marcamps/env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 608, in fit\n",
      "    raise ValueError(\n",
      "ValueError: n_components cannot be larger than min(n_features, n_classes - 1).\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.83145853 0.83145853 0.83145853 0.83145853 0.83145853 0.83145853\n",
      " 0.83145853 0.83145853 0.83145853 0.83145853        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82945663 0.82945663 0.82945663 0.82945663\n",
      " 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663\n",
      "        nan        nan        nan        nan        nan 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676        nan        nan        nan\n",
      "        nan        nan 0.81982841 0.81982841 0.81982841 0.81982841\n",
      " 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841\n",
      " 0.83145853 0.83145853 0.83145853 0.83145853 0.83145853 0.83145853\n",
      " 0.83145853 0.83145853 0.83145853 0.83145853        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82945663 0.82945663 0.82945663 0.82945663\n",
      " 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663 0.82945663\n",
      "        nan        nan        nan        nan        nan 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676 0.82659676\n",
      " 0.82659676 0.82659676 0.82659676        nan        nan        nan\n",
      "        nan        nan 0.81982841 0.81982841 0.81982841 0.81982841\n",
      " 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841 0.81982841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create an LDA classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_components': [None, 1, 2, 3],    # Values to try for n_components\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5],    # Values to try for shrinkage\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],    # Values to try for solver\n",
    "    'tol': [1e-6,1e-5,1e-4, 1e-3, 1e-2]    # Values to try for tol\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=lda, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning we fit the LDA with the hyperparameters we found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis(n_components = None,shrinkage = None,  solver = 'svd', tol = 1e-06)\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(lda , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['LDA subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "\n",
    "We now fit the normal model for the perceptron:\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "penalty: Specifies the penalty term used in the update rule to handle misclassifications. It can be set to 'l1', 'l2', or 'elasticnet'. The default value is 'l2'.\n",
    "\n",
    "alpha: The constant that multiplies the penalty term if regularization is applied. It controls the strength of the regularization. The default value is 0.0001.\n",
    "\n",
    "fit_intercept: Indicates whether an intercept term should be included in the model. If set to True, the perceptron learns an intercept term. The default value is True.\n",
    "\n",
    "max_iter: The maximum number of passes over the training data (epochs) for training the perceptron. The default value is 1000.\n",
    "\n",
    "tol: The tolerance for the stopping criterion. It specifies the minimum change in the average loss for training to continue. The default value is 1e-3.\n",
    "\n",
    "shuffle: Determines whether to shuffle the training data before each epoch during training. The default value is True.\n",
    "\n",
    "eta0: The initial learning rate. It controls the step size at each update during training. The default value is 1.0.\n",
    "\n",
    "early_stopping: If set to True, training will stop when validation loss does not improve anymore. The default value is False.\n",
    "\n",
    "validation_fraction: The proportion of training data to use for early stopping validation. The default value is 0.1.\n",
    "\n",
    "n_iter_no_change: The maximum number of epochs to wait for the validation loss to improve when early_stopping is enabled. The default value is 5.\n",
    "\n",
    "We now fit the models for both subsets of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the hyper parameters using the smallest subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'alpha': 0.001, 'eta0': 0.1, 'max_iter': 1000, 'penalty': 'elasticnet', 'tol': 10}\n",
      "Best Score:  0.8057197330791229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization penalty type\n",
    "    'alpha': [0.0001, 0.001, 0.01],         # Regularization parameter\n",
    "    'max_iter': [1000, 2000, 3000],         # Maximum number of iterations\n",
    "    'eta0': [0.1, 0.01, 0.001],             # Initial learning rate\n",
    "    'tol': [10,1,1e-1,1e-2,1e-3]               # Tolerance for stopping criterion\n",
    "}\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=perceptron, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit with the found hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron(alpha = 0.001, eta0=0.1, max_iter=1000, penalty='elasticnet', tol=10)\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create a Perceptron classifier\n",
    "perceptron = Perceptron(alpha = 0.001, eta0=0.1, max_iter=1000, penalty='elasticnet', tol=10)\n",
    "# Train the Perceptron classifier\n",
    "perceptron.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(perceptron , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Perceptron subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we observe that the best model is by far the last we fitted: Perceptron subset features After HT, since it has the best accuracy and f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boost \n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "n_estimators: The number of boosting stages to perform.\n",
    "\n",
    "learning_rate: The learning rate or shrinkage parameter, which controls the contribution of each tree.\n",
    "\n",
    "max_depth: The maximum depth of individual trees in the ensemble.\n",
    "\n",
    "subsample: The subsample ratio of the training instances.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node.\n",
    "\n",
    "We fit the model for both subset of features: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train_sf, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 4, 'min_samples_split': 4, 'subsample': 0.8}\n",
      "Best F1 Score: 0.8394708055874597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    #'n_estimators': [50, 100, 200],  # Number of boosting stages to perform\n",
    "    #'learning_rate': [0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [3, 4, 5],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [2, 3, 4]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tune the left hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best F1 Score: 0.8397310793851449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],  # Number of boosting stages to perform\n",
    "    'learning_rate': [1,0.1, 0.01, 0.001],  # Learning rate (shrinkage parameter)\n",
    "    'max_depth': [4],  # Maximum depth of individual trees\n",
    "    'subsample': [0.8],  # Subsample ratio of the training instances\n",
    "    'min_samples_split': [3]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding F1 score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=100, subsample=0.8)\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=3, n_estimators=100, subsample=0.8)\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "# Predict the target variable for the test set\n",
    "cross_val_results = pd.DataFrame(cross_validate(gb_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Gradient Boost subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain slightly better results with the Gradient Boost subset features After HT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Hyperparameters:\n",
    "\n",
    "n_estimators: The number of decision trees in the random forest. Increasing the number of estimators typically improves performance but increases computational complexity. It represents the ensemble size.\n",
    "\n",
    "max_depth: The maximum depth of each decision tree in the forest. Higher values increase model complexity and can lead to overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. Larger values prevent overfitting by requiring a certain number of samples in each split.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. Similar to min_samples_split, larger values help control overfitting by requiring a minimum number of samples in each leaf.\n",
    "\n",
    "max_features: The number of features to consider when looking for the best split. Reducing this number can help control overfitting. Values such as 'sqrt' or 'log2' can be used to consider a square root or logarithm of the total features, respectively.\n",
    "\n",
    "bootstrap: Determines whether bootstrap samples are used when building trees. Setting it to True enables bootstrap sampling, while False disables it. Bootstrap sampling introduces randomness into the training process and helps improve model diversity.\n",
    "\n",
    "criterion: The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We have an additional parameter (not to tune) : criterion: The function used to measure the quality of a split. For classification, 'gini' or 'entropy' are commonly used. For regression, 'mse' (mean squared error) or 'mae' (mean absolute error) can be used.\n",
    "\n",
    "We first fit the normal model with both subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features Before HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to do hyperparameter tuning with the small subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    #'n_estimators': [100, 200, 500],\n",
    "    #'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [42],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    #'n_estimators': [100, 200, 500],\n",
    "    #'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['log2'],\n",
    "    'bootstrap': [True],\n",
    "    'random_state': [42],\n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': ['log2'],\n",
    "    'bootstrap': [True],\n",
    "    'random_state': [42],\n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.849285</td>\n",
       "      <td>0.83157</td>\n",
       "      <td>0.855133</td>\n",
       "      <td>0.819829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "Random Forest all features After HT             0.849285   0.83157   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  \n",
       "Random Forest all features After HT                   0.855133     0.819829  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='gini', max_depth=None, max_features='log2', min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest all features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.849285</td>\n",
       "      <td>0.83157</td>\n",
       "      <td>0.855133</td>\n",
       "      <td>0.819829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.84776</td>\n",
       "      <td>0.830782</td>\n",
       "      <td>0.850959</td>\n",
       "      <td>0.820193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "Random Forest all features After HT             0.849285   0.83157   \n",
       "Random Forest subset features After HT           0.84776  0.830782   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  \n",
       "Random Forest all features After HT                   0.855133     0.819829  \n",
       "Random Forest subset features After HT                0.850959     0.820193  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(bootstrap = True, criterion='gini', max_depth=None, max_features='log2', min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "cross_val_results = pd.DataFrame(cross_validate(rf_classifier , X_train_sf, y_train, cv = 5, scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'] ))\n",
    "results_df.loc['Random Forest subset features After HT',:] = cross_val_results[['test_accuracy', 'test_f1_macro',\n",
    "       'test_precision_macro', 'test_recall_macro']].mean().values\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Nets\n",
    "Hyperparameters: \n",
    "\n",
    "Learning rate: The learning rate determines how much the weights are updated in response to the estimated error each time the model weights are updated. Choosing the right learning rate can be crucial as a value too small may result in a long training process that could get stuck, while a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "Optimizer: what type of optimizer you want to use\n",
    "\n",
    "Batch size: This is the number of samples to work through before updating the internal model parameters. A smaller batch size can lead to more updates and potentially faster convergence, but it also introduces more variance, which can lead to instability in the learning process.\n",
    "\n",
    "Activation function: Different activation functions can result in significant differences in the performance of a neural network. The Rectified Linear Unit (ReLU) and its variants (like Leaky ReLU, Parametric ReLU) are often a good starting point for many problems.\n",
    "\n",
    "We now do some neural nets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in /Users/marcamps/env/lib/python3.8/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: packaging>=0.21 in /Users/marcamps/env/lib/python3.8/site-packages (from scikeras) (23.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/marcamps/env/lib/python3.8/site-packages (from scikeras) (1.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/marcamps/.local/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.24.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/marcamps/env/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/marcamps/env/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/marcamps/.local/lib/python3.8/site-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\r\n"
     ]
    }
   ],
   "source": [
    "# install needed packages for following modeling\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network I \n",
    "Fit neural network with all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:33:55.609002: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4325 - accuracy: 0.8092\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3591 - accuracy: 0.8481\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8557\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8580\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8618\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8625\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3230 - accuracy: 0.8642\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8677\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8688\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3112 - accuracy: 0.8695\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:09.138132: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:09.421337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4357 - accuracy: 0.8076\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3599 - accuracy: 0.8477\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8550\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8584\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8618\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8612\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8621\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8648\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8673\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3139 - accuracy: 0.8684\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:22.784284: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:23.054705: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4374 - accuracy: 0.7961\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3621 - accuracy: 0.8464\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3471 - accuracy: 0.8546\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3398 - accuracy: 0.8565\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8606\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8620\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8646\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8668\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8663\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3137 - accuracy: 0.8669\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:36.534273: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:36.826091: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4358 - accuracy: 0.8023\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3540 - accuracy: 0.8497\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3399 - accuracy: 0.8615\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8608\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8637\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8699\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8699\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3118 - accuracy: 0.8695\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3090 - accuracy: 0.8687\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8749\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:34:50.862893: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:34:51.153262: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4334 - accuracy: 0.8043\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3599 - accuracy: 0.8494\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3446 - accuracy: 0.8563\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8589\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8622\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8643\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8668\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8668\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3130 - accuracy: 0.8677\n",
      "66/66 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:35:04.985248: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with the subset of variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:38:42.304313: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4341 - accuracy: 0.8130\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3600 - accuracy: 0.8502\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3481 - accuracy: 0.8565\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3438 - accuracy: 0.8561\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3403 - accuracy: 0.8594\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3388 - accuracy: 0.8583\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3379 - accuracy: 0.8582\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8602\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8602\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3357 - accuracy: 0.8609\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:38:56.124400: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:38:56.421478: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4257 - accuracy: 0.8159\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3595 - accuracy: 0.8483\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3495 - accuracy: 0.8555\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3438 - accuracy: 0.8566\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3435 - accuracy: 0.8577\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3415 - accuracy: 0.8576\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8597\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3400 - accuracy: 0.8584\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3400 - accuracy: 0.8588\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8614\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:10.068816: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:39:10.362915: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4403 - accuracy: 0.8051\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3623 - accuracy: 0.8468\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3504 - accuracy: 0.8519\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3471 - accuracy: 0.8556\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3431 - accuracy: 0.8575\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8550\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3399 - accuracy: 0.8574\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8571\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8564\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3378 - accuracy: 0.8557\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:24.293557: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  1/263 [..............................] - ETA: 1:19 - loss: 0.6959 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:24.574849: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4150 - accuracy: 0.8199\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3498 - accuracy: 0.8572\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8606\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8601\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8605\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8646\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8622\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8622\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8628\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:38.337303: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 20:39:38.976980: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 6ms/step - loss: 0.4232 - accuracy: 0.8160\n",
      "Epoch 2/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3547 - accuracy: 0.8521\n",
      "Epoch 3/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8577\n",
      "Epoch 4/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8572\n",
      "Epoch 5/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3375 - accuracy: 0.8580\n",
      "Epoch 6/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8599\n",
      "Epoch 7/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8601\n",
      "Epoch 8/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8614\n",
      "Epoch 9/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8608\n",
      "Epoch 10/10\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8606\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:39:52.696844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network I subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC all features Before HT</th>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.810271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features Before HT</th>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.829476</td>\n",
       "      <td>0.808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC all features After HT</th>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.862018</td>\n",
       "      <td>0.831795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC subset features after HT</th>\n",
       "      <td>0.860153</td>\n",
       "      <td>0.84301</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features Before HT</th>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.752331</td>\n",
       "      <td>0.74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features Before HT</th>\n",
       "      <td>0.832698</td>\n",
       "      <td>0.816219</td>\n",
       "      <td>0.828732</td>\n",
       "      <td>0.808563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes all features After HT</th>\n",
       "      <td>0.787703</td>\n",
       "      <td>0.764991</td>\n",
       "      <td>0.779569</td>\n",
       "      <td>0.757468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes subset features After HT</th>\n",
       "      <td>0.832888</td>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.833829</td>\n",
       "      <td>0.80391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features Before HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset featuresBefore HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA all features After HT</th>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.810283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA subset features After HT</th>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.812401</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features Before HT</th>\n",
       "      <td>0.759771</td>\n",
       "      <td>0.74195</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features Before HT</th>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.751187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron all features After HT</th>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.76102</td>\n",
       "      <td>0.758576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron subset features After HT</th>\n",
       "      <td>0.80572</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.779489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features Before HT</th>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.858118</td>\n",
       "      <td>0.825965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features Before HT</th>\n",
       "      <td>0.851096</td>\n",
       "      <td>0.83435</td>\n",
       "      <td>0.855089</td>\n",
       "      <td>0.82337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost all features After HT</th>\n",
       "      <td>0.854719</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.827442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boost subset features After HT</th>\n",
       "      <td>0.855291</td>\n",
       "      <td>0.839049</td>\n",
       "      <td>0.86005</td>\n",
       "      <td>0.828001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Before HT</th>\n",
       "      <td>0.847855</td>\n",
       "      <td>0.83044</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.819247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features Before HT</th>\n",
       "      <td>0.847283</td>\n",
       "      <td>0.829982</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.819047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features Before HT</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.828111</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.818319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest all features After HT</th>\n",
       "      <td>0.849285</td>\n",
       "      <td>0.83157</td>\n",
       "      <td>0.855133</td>\n",
       "      <td>0.819829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest subset features After HT</th>\n",
       "      <td>0.856339</td>\n",
       "      <td>0.841872</td>\n",
       "      <td>0.856631</td>\n",
       "      <td>0.833087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network all features before HT</th>\n",
       "      <td>0.856339</td>\n",
       "      <td>0.840602</td>\n",
       "      <td>0.860203</td>\n",
       "      <td>0.829915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network I subset features before HT</th>\n",
       "      <td>0.860248</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.833858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Accuracy  F1 Macro  \\\n",
       "SVC all features Before HT                      0.833937  0.817714   \n",
       "SVC subset features Before HT                   0.832793  0.816053   \n",
       "SVC all features After HT                       0.858055  0.842509   \n",
       "SVC subset features after HT                    0.860153   0.84301   \n",
       "Gaussian Naive Bayes Before HT                  0.767016  0.747318   \n",
       "Gaussian Naive Bayes all features Before HT     0.767016  0.747318   \n",
       "Gaussian Naive Bayes subset features Before HT  0.832698  0.816219   \n",
       "Gaussian Naive Bayes all features After HT      0.787703  0.764991   \n",
       "Gaussian Naive Bayes subset features After HT   0.832888  0.814109   \n",
       "LDA all features Before HT                      0.828122  0.812401   \n",
       "LDA subset featuresBefore HT                    0.828122  0.812401   \n",
       "LDA After HT                                    0.831459  0.816218   \n",
       "LDA all features After HT                       0.831459  0.816218   \n",
       "LDA subset features After HT                    0.828122  0.812401   \n",
       "Perceptron all features Before HT               0.759771   0.74195   \n",
       "Perceptron subset features Before HT            0.761487  0.745887   \n",
       "Perceptron all features After HT                0.771687  0.756834   \n",
       "Perceptron subset features After HT              0.80572  0.786355   \n",
       "Gradient Boost Before HT                        0.853575  0.837095   \n",
       "Gradient Boost all features Before HT           0.853575  0.837095   \n",
       "Gradient Boost subset features Before HT        0.851096   0.83435   \n",
       "Gradient Boost all features After HT            0.854719  0.838471   \n",
       "Gradient Boost subset features After HT         0.855291  0.839049   \n",
       "Random Forest Before HT                         0.847855   0.83044   \n",
       "Random Forest all features Before HT            0.847283  0.829982   \n",
       "Random Forest subset features Before HT           0.8449  0.828111   \n",
       "Random Forest all features After HT             0.849285   0.83157   \n",
       "Random Forest subset features After HT          0.856339  0.841872   \n",
       "Neural Network all features before HT           0.856339  0.840602   \n",
       "Neural Network I subset features before HT      0.860248  0.844776   \n",
       "\n",
       "                                               Precision Macro Recall Macro  \n",
       "SVC all features Before HT                            0.829882     0.810271  \n",
       "SVC subset features Before HT                         0.829476     0.808078  \n",
       "SVC all features After HT                             0.862018     0.831795  \n",
       "SVC subset features after HT                          0.870671     0.829687  \n",
       "Gaussian Naive Bayes Before HT                        0.752331      0.74385  \n",
       "Gaussian Naive Bayes all features Before HT           0.752331      0.74385  \n",
       "Gaussian Naive Bayes subset features Before HT        0.828732     0.808563  \n",
       "Gaussian Naive Bayes all features After HT            0.779569     0.757468  \n",
       "Gaussian Naive Bayes subset features After HT         0.833829      0.80391  \n",
       "LDA all features Before HT                            0.821662     0.806338  \n",
       "LDA subset featuresBefore HT                          0.821662     0.806338  \n",
       "LDA After HT                                          0.825102     0.810283  \n",
       "LDA all features After HT                             0.825102     0.810283  \n",
       "LDA subset features After HT                          0.821662     0.806338  \n",
       "Perceptron all features Before HT                     0.752749     0.742565  \n",
       "Perceptron subset features Before HT                  0.762428     0.751187  \n",
       "Perceptron all features After HT                       0.76102     0.758576  \n",
       "Perceptron subset features After HT                   0.798826     0.779489  \n",
       "Gradient Boost Before HT                              0.858118     0.825965  \n",
       "Gradient Boost all features Before HT                 0.858118     0.825965  \n",
       "Gradient Boost subset features Before HT              0.855089      0.82337  \n",
       "Gradient Boost all features After HT                  0.859126     0.827442  \n",
       "Gradient Boost subset features After HT                0.86005     0.828001  \n",
       "Random Forest Before HT                               0.852204     0.819247  \n",
       "Random Forest all features Before HT                  0.850998     0.819047  \n",
       "Random Forest subset features Before HT               0.846167     0.818319  \n",
       "Random Forest all features After HT                   0.855133     0.819829  \n",
       "Random Forest subset features After HT                0.856631     0.833087  \n",
       "Neural Network all features before HT                 0.860203     0.829915  \n",
       "Neural Network I subset features before HT            0.865241     0.833858  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we observe that the subset of features works better, we will do hyperparameter tuning using a the subset of the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit the model that has the best hyperparameters we found: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network II\n",
    "We firstly fit the neural network with both subsests of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning with all features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:06:17.974835: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.4614 - accuracy: 0.7908\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3635 - accuracy: 0.8470\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3473 - accuracy: 0.8568\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3398 - accuracy: 0.8620\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8644\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8651\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8665\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8659\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8683\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3148 - accuracy: 0.8714\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3119 - accuracy: 0.8727\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3117 - accuracy: 0.8733\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3068 - accuracy: 0.8732\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8725\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8749\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3010 - accuracy: 0.8754\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8763\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2986 - accuracy: 0.8754\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2962 - accuracy: 0.8794\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8782\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2931 - accuracy: 0.8787\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2908 - accuracy: 0.8775\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2895 - accuracy: 0.8782\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8795\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8814\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8818\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8816\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2797 - accuracy: 0.8843\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2797 - accuracy: 0.8825\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8850\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.8855\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8858\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8829\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8864\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2701 - accuracy: 0.8881\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2668 - accuracy: 0.8904\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2670 - accuracy: 0.8888\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8901\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8878\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8914\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8932\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8939\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8918\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8912\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2539 - accuracy: 0.8943\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8931\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.8947\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2517 - accuracy: 0.8945\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2490 - accuracy: 0.8945\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:07:22.170838: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:07:22.452516: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3930 - accuracy: 0.8295\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3492 - accuracy: 0.8509\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3395 - accuracy: 0.8576\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8596\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8614\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8646\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8639\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3180 - accuracy: 0.8659\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3138 - accuracy: 0.8696\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3133 - accuracy: 0.8702\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3090 - accuracy: 0.8699\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8708\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3055 - accuracy: 0.8709\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3015 - accuracy: 0.8723\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8749\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8754\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8767\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8791\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2888 - accuracy: 0.8774\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8813\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2846 - accuracy: 0.8806\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8808\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2801 - accuracy: 0.8817\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.2782 - accuracy: 0.8824\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8818\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.2732 - accuracy: 0.8852\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2723 - accuracy: 0.8838\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2718 - accuracy: 0.8830\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8899\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2657 - accuracy: 0.8873\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8893\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8870\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8913\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8910\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2593 - accuracy: 0.8913\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8949\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2520 - accuracy: 0.8939\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8944\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2497 - accuracy: 0.8963\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2483 - accuracy: 0.8962\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2465 - accuracy: 0.8972\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2431 - accuracy: 0.8992\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2418 - accuracy: 0.9000\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9007\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2374 - accuracy: 0.9028\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2398 - accuracy: 0.9007\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2350 - accuracy: 0.9044\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2343 - accuracy: 0.9040\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2306 - accuracy: 0.9035\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2299 - accuracy: 0.9037\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:08:28.510396: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:08:28.828648: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3891 - accuracy: 0.8314\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8520\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3388 - accuracy: 0.8565\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8588\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8636\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8656\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8656\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3189 - accuracy: 0.8687\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3159 - accuracy: 0.8679\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3133 - accuracy: 0.8680\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8725\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3097 - accuracy: 0.8686\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3050 - accuracy: 0.8744\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3030 - accuracy: 0.8755\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3025 - accuracy: 0.8738\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2980 - accuracy: 0.8776\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2973 - accuracy: 0.8774\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2945 - accuracy: 0.8800\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2928 - accuracy: 0.8787\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2897 - accuracy: 0.8810\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8813\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8808\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8827\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2824 - accuracy: 0.8851\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2813 - accuracy: 0.8839\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8832\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8894\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8874\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8869\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8849\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2701 - accuracy: 0.8880\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2675 - accuracy: 0.8881\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8906\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8904\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8912\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8919\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8944\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8916\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8955\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.8966\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8948\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2515 - accuracy: 0.8945\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2488 - accuracy: 0.8970\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2471 - accuracy: 0.8966\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2465 - accuracy: 0.8985\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2438 - accuracy: 0.8992\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2431 - accuracy: 0.8972\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2388 - accuracy: 0.9024\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2378 - accuracy: 0.9018\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.9046\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:09:35.331520: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:09:35.615800: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3796 - accuracy: 0.8344\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8595\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8632\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8669\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3201 - accuracy: 0.8659\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3150 - accuracy: 0.8692\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3126 - accuracy: 0.8713\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3091 - accuracy: 0.8733\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8746\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8735\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3003 - accuracy: 0.8760\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8757\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2945 - accuracy: 0.8804\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2939 - accuracy: 0.8802\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2915 - accuracy: 0.8791\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8810\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.8801\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8841\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8842\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2815 - accuracy: 0.8825\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8861\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8848\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2751 - accuracy: 0.8882\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8880\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2727 - accuracy: 0.8869\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8901\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8906\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8914\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8913\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8907\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2619 - accuracy: 0.8920\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.8928\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8920\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8963\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8963\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8963\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.8963\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.8962\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2483 - accuracy: 0.8992\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2451 - accuracy: 0.9005\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.8987\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.8994\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2409 - accuracy: 0.9020\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2389 - accuracy: 0.9018\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2353 - accuracy: 0.9037\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2353 - accuracy: 0.9032\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2338 - accuracy: 0.9043\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2332 - accuracy: 0.9032\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2324 - accuracy: 0.9032\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2286 - accuracy: 0.9067\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:10:43.200337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:10:43.522209: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3853 - accuracy: 0.8350\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3460 - accuracy: 0.8539\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8615\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8633\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8652\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8676\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3189 - accuracy: 0.8688\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3157 - accuracy: 0.8694\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3150 - accuracy: 0.8700\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3102 - accuracy: 0.8729\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3074 - accuracy: 0.8720\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8755\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3027 - accuracy: 0.8752\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3005 - accuracy: 0.8733\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2979 - accuracy: 0.8768\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8767\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2952 - accuracy: 0.8771\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.8806\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8799\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2884 - accuracy: 0.8808\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2856 - accuracy: 0.8795\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8820\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8837\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2803 - accuracy: 0.8845\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.2801 - accuracy: 0.8839\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8873\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.8852\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2745 - accuracy: 0.8861\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.8888\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8893\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8891\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8907\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8929\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8925\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8947\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2604 - accuracy: 0.8937\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.8957\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8976\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8950\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2532 - accuracy: 0.8976\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2525 - accuracy: 0.8962\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2511 - accuracy: 0.8991\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2503 - accuracy: 0.9013\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2455 - accuracy: 0.8997\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2473 - accuracy: 0.8998\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.9000\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9038\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9022\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2416 - accuracy: 0.9023\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2397 - accuracy: 0.9057\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:11:52.404724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:11:52.734718: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3971 - accuracy: 0.8282\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8551\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8571\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8601\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8601\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8634\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8679\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8679\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3157 - accuracy: 0.8677\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3120 - accuracy: 0.8699\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3112 - accuracy: 0.8707\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3078 - accuracy: 0.8718\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8729\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3035 - accuracy: 0.8736\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3014 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2995 - accuracy: 0.8774\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2978 - accuracy: 0.8755\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8788\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2923 - accuracy: 0.8787\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2905 - accuracy: 0.8789\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2881 - accuracy: 0.8796\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.8823\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8844\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2836 - accuracy: 0.8819\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8856\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8863\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2775 - accuracy: 0.8823\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8868\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8886\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2705 - accuracy: 0.8879\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8880\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.8895\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8924\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8912\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8917\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8926\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2590 - accuracy: 0.8938\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8951\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2538 - accuracy: 0.8976\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2522 - accuracy: 0.8980\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8964\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2478 - accuracy: 0.8987\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2496 - accuracy: 0.9011\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.9025\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.9011\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9015\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.9007\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2400 - accuracy: 0.9035\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2386 - accuracy: 0.9054\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2346 - accuracy: 0.9049\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:13:01.224672: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:13:01.554819: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3954 - accuracy: 0.8254\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3493 - accuracy: 0.8516\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3410 - accuracy: 0.8555\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8612\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8679\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8674\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8683\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3130 - accuracy: 0.8699\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8714\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3067 - accuracy: 0.8725\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3040 - accuracy: 0.8735\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8770\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2969 - accuracy: 0.8761\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2938 - accuracy: 0.8801\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8805\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8776\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.8804\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2833 - accuracy: 0.8804\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8831\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2792 - accuracy: 0.8832\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2763 - accuracy: 0.8838\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8842\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2720 - accuracy: 0.8864\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8881\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2670 - accuracy: 0.8882\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8911\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.8900\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8931\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8932\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8944\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8925\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8966\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2513 - accuracy: 0.8962\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.8956\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2471 - accuracy: 0.8979\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2478 - accuracy: 0.8975\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2440 - accuracy: 0.9011\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2415 - accuracy: 0.9007\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2391 - accuracy: 0.9009\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2382 - accuracy: 0.9016\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2366 - accuracy: 0.9024\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2333 - accuracy: 0.9046\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2346 - accuracy: 0.9042\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2318 - accuracy: 0.9041\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2279 - accuracy: 0.9080\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2282 - accuracy: 0.9074\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2252 - accuracy: 0.9069\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2231 - accuracy: 0.9085\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2251 - accuracy: 0.9075\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:14:08.547124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:14:08.842268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3973 - accuracy: 0.8211\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3523 - accuracy: 0.8506\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3427 - accuracy: 0.8543\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3368 - accuracy: 0.8570\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8602\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8633\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8648\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8676\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8680\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3156 - accuracy: 0.8687\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3123 - accuracy: 0.8708\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3120 - accuracy: 0.8695\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3073 - accuracy: 0.8726\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3049 - accuracy: 0.8738\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8751\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8756\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8751\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2960 - accuracy: 0.8769\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8793\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8789\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2916 - accuracy: 0.8780\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8818\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8806\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8816\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2833 - accuracy: 0.8810\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8830\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8844\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8839\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8863\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8856\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8860\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8885\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8882\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8895\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8920\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8910\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8936\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2603 - accuracy: 0.8920\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8948\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.8943\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2559 - accuracy: 0.8964\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8968\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2517 - accuracy: 0.8957\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.8970\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8969\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.8982\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.8959\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2451 - accuracy: 0.8998\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.8973\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2454 - accuracy: 0.8988\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:15:16.055401: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:15:16.344243: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3827 - accuracy: 0.8346\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3392 - accuracy: 0.8570\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8631\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8669\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8680\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8679\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8692\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8699\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3055 - accuracy: 0.8726\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3031 - accuracy: 0.8755\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3003 - accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2988 - accuracy: 0.8768\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2965 - accuracy: 0.8793\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2933 - accuracy: 0.8795\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2917 - accuracy: 0.8789\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8812\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8814\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8819\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8832\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8836\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8830\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8848\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2769 - accuracy: 0.8849\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8862\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8857\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2723 - accuracy: 0.8864\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8870\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2705 - accuracy: 0.8869\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8879\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8892\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8880\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8900\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8895\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8924\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2572 - accuracy: 0.8922\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8941\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8938\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2510 - accuracy: 0.8966\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2503 - accuracy: 0.8942\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2490 - accuracy: 0.8978\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.8968\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8973\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.8972\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2437 - accuracy: 0.8976\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.8978\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9005\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2387 - accuracy: 0.9010\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2384 - accuracy: 0.9009\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2363 - accuracy: 0.9026\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:16:23.337701: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:16:23.653265: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3931 - accuracy: 0.8246\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3437 - accuracy: 0.8568\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8597\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8611\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3239 - accuracy: 0.8650\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8670\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8661\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3130 - accuracy: 0.8695\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8705\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3082 - accuracy: 0.8732\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3049 - accuracy: 0.8746\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3043 - accuracy: 0.8750\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8781\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2975 - accuracy: 0.8786\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2964 - accuracy: 0.8771\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8786\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2922 - accuracy: 0.8791\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2902 - accuracy: 0.8831\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8802\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8811\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8818\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2807 - accuracy: 0.8835\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8850\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2765 - accuracy: 0.8857\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.8875\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2729 - accuracy: 0.8862\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2717 - accuracy: 0.8883\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8882\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8924\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8923\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8891\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8919\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8932\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.8919\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2573 - accuracy: 0.8944\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8947\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8950\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2489 - accuracy: 0.8974\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2492 - accuracy: 0.8962\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8981\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2443 - accuracy: 0.9015\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2445 - accuracy: 0.8990\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2423 - accuracy: 0.8992\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2389 - accuracy: 0.9000\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2385 - accuracy: 0.9023\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2366 - accuracy: 0.9028\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2345 - accuracy: 0.9040\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2359 - accuracy: 0.9015\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2314 - accuracy: 0.9055\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.2297 - accuracy: 0.9074\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:17:30.393477: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:17:30.701149: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.4041 - accuracy: 0.8234\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3520 - accuracy: 0.8520\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8592\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8630\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8656\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8664\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3174 - accuracy: 0.8692\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8694\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3134 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3095 - accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8725\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3063 - accuracy: 0.8743\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3044 - accuracy: 0.8735\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3013 - accuracy: 0.8738\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3022 - accuracy: 0.8761\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8783\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2969 - accuracy: 0.8785\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2954 - accuracy: 0.8799\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2941 - accuracy: 0.8794\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2926 - accuracy: 0.8783\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2901 - accuracy: 0.8783\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2894 - accuracy: 0.8826\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8789\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2865 - accuracy: 0.8813\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2836 - accuracy: 0.8808\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8806\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2810 - accuracy: 0.8843\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2805 - accuracy: 0.8826\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8867\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.8848\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8874\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2756 - accuracy: 0.8847\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2727 - accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8897\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8895\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8867\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2677 - accuracy: 0.8891\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8910\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2638 - accuracy: 0.8901\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8913\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8907\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8905\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2601 - accuracy: 0.8910\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8928\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.8945\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2563 - accuracy: 0.8955\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8939\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2551 - accuracy: 0.8959\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2542 - accuracy: 0.8956\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:18:06.251641: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:18:06.505568: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4085 - accuracy: 0.8217\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3534 - accuracy: 0.8494\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3425 - accuracy: 0.8534\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8583\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8617\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8611\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8626\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3168 - accuracy: 0.8639\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3144 - accuracy: 0.8668\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3094 - accuracy: 0.8683\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8687\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3060 - accuracy: 0.8692\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3024 - accuracy: 0.8726\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3010 - accuracy: 0.8727\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3006 - accuracy: 0.8724\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2972 - accuracy: 0.8731\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2955 - accuracy: 0.8738\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2935 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2961 - accuracy: 0.8739\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8769\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8751\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.8752\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2832 - accuracy: 0.8799\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8780\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8826\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8817\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8806\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8798\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2761 - accuracy: 0.8812\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2753 - accuracy: 0.8829\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8826\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8814\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8848\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8857\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2685 - accuracy: 0.8860\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8858\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8861\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8874\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8897\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8892\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2588 - accuracy: 0.8895\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2586 - accuracy: 0.8901\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2579 - accuracy: 0.8908\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8885\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8918\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8919\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2515 - accuracy: 0.8931\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2501 - accuracy: 0.8947\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8939\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.8956\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:18:41.367949: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:18:41.622265: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.3907 - accuracy: 0.8310\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3500 - accuracy: 0.8478\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3386 - accuracy: 0.8571\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8606\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3286 - accuracy: 0.8593\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3260 - accuracy: 0.8627\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3215 - accuracy: 0.8656\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3176 - accuracy: 0.8656\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3145 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3123 - accuracy: 0.8677\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3093 - accuracy: 0.8689\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3080 - accuracy: 0.8700\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3053 - accuracy: 0.8698\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3032 - accuracy: 0.8737\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3013 - accuracy: 0.8743\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2999 - accuracy: 0.8754\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2979 - accuracy: 0.8765\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8774\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2963 - accuracy: 0.8768\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2914 - accuracy: 0.8780\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8807\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8799\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8819\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2862 - accuracy: 0.8826\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2823 - accuracy: 0.8854\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2835 - accuracy: 0.8831\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2808 - accuracy: 0.8829\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8841\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2772 - accuracy: 0.8864\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8860\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8881\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2740 - accuracy: 0.8863\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.8873\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8882\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8891\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8913\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.8908\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8906\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2657 - accuracy: 0.8912\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8931\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8934\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8954\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8918\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8941\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8925\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8968\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8967\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8964\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8939\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2515 - accuracy: 0.8969\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:19:16.173297: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:19:16.443386: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.4035 - accuracy: 0.8183\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3443 - accuracy: 0.8571\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8618\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8645\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8655\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8667\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3134 - accuracy: 0.8701\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3105 - accuracy: 0.8724\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8713\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3049 - accuracy: 0.8735\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3002 - accuracy: 0.8760\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8777\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2955 - accuracy: 0.8774\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2928 - accuracy: 0.8787\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8796\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8789\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2880 - accuracy: 0.8791\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8789\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2847 - accuracy: 0.8830\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8811\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8831\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2808 - accuracy: 0.8827\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8844\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8851\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8845\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8837\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2724 - accuracy: 0.8856\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2706 - accuracy: 0.8860\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.8848\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2680 - accuracy: 0.8874\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2674 - accuracy: 0.8873\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2641 - accuracy: 0.8895\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8887\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8899\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2609 - accuracy: 0.8903\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2594 - accuracy: 0.8917\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2572 - accuracy: 0.8929\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2578 - accuracy: 0.8951\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2559 - accuracy: 0.8941\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8944\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2543 - accuracy: 0.8953\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2535 - accuracy: 0.8938\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2510 - accuracy: 0.8938\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2489 - accuracy: 0.8969\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2468 - accuracy: 0.9000\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2460 - accuracy: 0.8976\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2446 - accuracy: 0.8973\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2441 - accuracy: 0.8988\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2417 - accuracy: 0.8994\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:19:51.547268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:19:52.478572: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 3s 11ms/step - loss: 0.4097 - accuracy: 0.8189\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3498 - accuracy: 0.8522\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3381 - accuracy: 0.8614\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8652\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8658\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8649\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8700\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8699\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3141 - accuracy: 0.8683\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8711\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3095 - accuracy: 0.8731\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3069 - accuracy: 0.8760\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3048 - accuracy: 0.8740\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3028 - accuracy: 0.8762\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3038 - accuracy: 0.8723\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2992 - accuracy: 0.8770\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2973 - accuracy: 0.8791\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2966 - accuracy: 0.8773\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2947 - accuracy: 0.8782\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8791\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2915 - accuracy: 0.8787\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8792\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8783\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8793\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8823\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2844 - accuracy: 0.8849\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2835 - accuracy: 0.8827\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8831\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2813 - accuracy: 0.8844\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8836\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8830\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8837\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8854\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8850\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2750 - accuracy: 0.8873\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2725 - accuracy: 0.8863\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2708 - accuracy: 0.8880\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8885\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8900\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8868\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2663 - accuracy: 0.8894\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2636 - accuracy: 0.8891\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2636 - accuracy: 0.8914\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8934\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8910\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.8938\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.8943\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8953\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2565 - accuracy: 0.8930\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8941\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:20:28.188323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:20:28.473468: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4022 - accuracy: 0.8253\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3491 - accuracy: 0.8556\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8564\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8603\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8637\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8659\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3199 - accuracy: 0.8656\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3143 - accuracy: 0.8680\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3129 - accuracy: 0.8683\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8718\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3091 - accuracy: 0.8702\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8717\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3050 - accuracy: 0.8724\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3016 - accuracy: 0.8751\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8745\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2976 - accuracy: 0.8770\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2958 - accuracy: 0.8780\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8782\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8810\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8801\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2893 - accuracy: 0.8785\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8782\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8818\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2853 - accuracy: 0.8821\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8817\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2813 - accuracy: 0.8860\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8820\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8832\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2768 - accuracy: 0.8860\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8825\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2747 - accuracy: 0.8873\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8855\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8869\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2725 - accuracy: 0.8874\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8885\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8913\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8906\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8879\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8911\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8925\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8918\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8922\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8925\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8934\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8934\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8955\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2562 - accuracy: 0.8955\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8945\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8972\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:21:03.519908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:21:03.799901: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4151 - accuracy: 0.8082\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3557 - accuracy: 0.8495\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3432 - accuracy: 0.8549\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8566\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8596\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8595\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8648\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8644\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8682\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3144 - accuracy: 0.8682\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3117 - accuracy: 0.8696\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3102 - accuracy: 0.8705\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3067 - accuracy: 0.8725\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8723\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3028 - accuracy: 0.8749\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3010 - accuracy: 0.8742\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2963 - accuracy: 0.8776\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2940 - accuracy: 0.8792\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2922 - accuracy: 0.8758\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2922 - accuracy: 0.8792\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8818\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8796\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2853 - accuracy: 0.8820\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8817\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2812 - accuracy: 0.8826\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2805 - accuracy: 0.8824\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2779 - accuracy: 0.8868\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8850\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8839\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2724 - accuracy: 0.8869\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8852\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2719 - accuracy: 0.8855\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8863\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8867\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8880\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8888\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8882\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8911\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8912\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2598 - accuracy: 0.8905\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8923\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8918\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8937\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8949\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8962\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8935\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2505 - accuracy: 0.8948\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2497 - accuracy: 0.8956\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8976\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2483 - accuracy: 0.8993\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:21:38.951531: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:21:39.236395: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4105 - accuracy: 0.8209\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3552 - accuracy: 0.8488\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3488 - accuracy: 0.8519\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3376 - accuracy: 0.8600\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3331 - accuracy: 0.8594\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3315 - accuracy: 0.8607\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3269 - accuracy: 0.8645\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8652\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3221 - accuracy: 0.8643\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8663\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8676\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3148 - accuracy: 0.8704\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3110 - accuracy: 0.8695\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3116 - accuracy: 0.8702\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8708\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8711\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8737\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3040 - accuracy: 0.8743\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3008 - accuracy: 0.8730\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3006 - accuracy: 0.8762\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2997 - accuracy: 0.8745\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2974 - accuracy: 0.8754\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8765\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2946 - accuracy: 0.8763\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2935 - accuracy: 0.8762\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2920 - accuracy: 0.8782\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2907 - accuracy: 0.8776\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2892 - accuracy: 0.8794\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8783\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8792\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8795\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8802\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8825\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2788 - accuracy: 0.8835\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8819\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8837\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8841\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8862\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8848\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8839\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8845\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.8843\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.8875\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2676 - accuracy: 0.8875\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8885\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2655 - accuracy: 0.8893\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8898\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2654 - accuracy: 0.8897\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8905\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8900\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:22:14.691329: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:22:14.973475: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3918 - accuracy: 0.8286\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3420 - accuracy: 0.8565\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3313 - accuracy: 0.8633\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3237 - accuracy: 0.8652\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3199 - accuracy: 0.8665\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3148 - accuracy: 0.8683\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3106 - accuracy: 0.8676\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3098 - accuracy: 0.8717\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3073 - accuracy: 0.8719\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3029 - accuracy: 0.8738\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8756\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2976 - accuracy: 0.8769\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8757\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2926 - accuracy: 0.8771\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2918 - accuracy: 0.8795\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2894 - accuracy: 0.8791\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8817\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8789\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8811\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8830\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8848\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2798 - accuracy: 0.8847\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8844\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8812\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8864\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8863\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2715 - accuracy: 0.8875\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8878\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2696 - accuracy: 0.8905\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8900\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2662 - accuracy: 0.8903\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8900\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2630 - accuracy: 0.8942\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2616 - accuracy: 0.8910\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8939\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2579 - accuracy: 0.8939\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8959\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2543 - accuracy: 0.8945\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8969\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8954\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2511 - accuracy: 0.8961\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2512 - accuracy: 0.8976\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2495 - accuracy: 0.8975\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.8990\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2476 - accuracy: 0.8993\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2449 - accuracy: 0.8987\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2453 - accuracy: 0.9013\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2419 - accuracy: 0.9000\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.9013\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2426 - accuracy: 0.9020\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:22:50.674443: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:22:50.944579: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.3910 - accuracy: 0.8316\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3403 - accuracy: 0.8555\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3314 - accuracy: 0.8607\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3231 - accuracy: 0.8655\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3196 - accuracy: 0.8702\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3187 - accuracy: 0.8695\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8705\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3123 - accuracy: 0.8696\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3107 - accuracy: 0.8686\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3088 - accuracy: 0.8711\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8748\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3046 - accuracy: 0.8760\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3022 - accuracy: 0.8730\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3019 - accuracy: 0.8727\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8748\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2989 - accuracy: 0.8760\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8763\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8755\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2940 - accuracy: 0.8767\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8764\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8767\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8780\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2911 - accuracy: 0.8796\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8785\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8807\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8796\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2820 - accuracy: 0.8810\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8792\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2795 - accuracy: 0.8823\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8813\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2772 - accuracy: 0.8833\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8817\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.8833\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8838\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2734 - accuracy: 0.8842\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2727 - accuracy: 0.8849\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8824\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8841\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8857\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8870\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2652 - accuracy: 0.8868\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8882\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2642 - accuracy: 0.8886\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8893\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8888\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2598 - accuracy: 0.8892\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8905\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.8920\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2566 - accuracy: 0.8894\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:23:27.129832: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:23:27.406273: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4087 - accuracy: 0.8194\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3514 - accuracy: 0.8533\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.8587\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3340 - accuracy: 0.8595\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8633\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8643\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8649\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.8680\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8695\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8689\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3106 - accuracy: 0.8720\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3075 - accuracy: 0.8745\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3074 - accuracy: 0.8738\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8740\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3036 - accuracy: 0.8730\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2998 - accuracy: 0.8765\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2995 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2983 - accuracy: 0.8755\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2961 - accuracy: 0.8782\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8779\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2946 - accuracy: 0.8781\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2912 - accuracy: 0.8762\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2888 - accuracy: 0.8796\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2896 - accuracy: 0.8795\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2867 - accuracy: 0.8800\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2854 - accuracy: 0.8837\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2847 - accuracy: 0.8794\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2822 - accuracy: 0.8804\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2809 - accuracy: 0.8825\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.8833\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2797 - accuracy: 0.8806\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2774 - accuracy: 0.8844\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2752 - accuracy: 0.8847\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2760 - accuracy: 0.8830\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2725 - accuracy: 0.8864\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2732 - accuracy: 0.8847\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2699 - accuracy: 0.8849\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2707 - accuracy: 0.8866\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8874\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.8881\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.8900\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.8905\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2620 - accuracy: 0.8908\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.8917\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2589 - accuracy: 0.8916\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2580 - accuracy: 0.8931\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2563 - accuracy: 0.8925\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2557 - accuracy: 0.8924\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2559 - accuracy: 0.8926\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:23:51.140217: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:23:51.397566: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4234 - accuracy: 0.8061\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3554 - accuracy: 0.8471\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3415 - accuracy: 0.8552\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3384 - accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3301 - accuracy: 0.8638\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8639\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8655\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.8663\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8670\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8708\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3115 - accuracy: 0.8687\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.8730\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3058 - accuracy: 0.8731\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3051 - accuracy: 0.8711\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3024 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3005 - accuracy: 0.8761\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2984 - accuracy: 0.8755\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2969 - accuracy: 0.8761\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2943 - accuracy: 0.8777\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.8769\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2913 - accuracy: 0.8793\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.8785\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2878 - accuracy: 0.8807\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2872 - accuracy: 0.8800\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.8805\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2828 - accuracy: 0.8829\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2813 - accuracy: 0.8844\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2817 - accuracy: 0.8829\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2785 - accuracy: 0.8835\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2768 - accuracy: 0.8836\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.8855\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.8862\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2721 - accuracy: 0.8863\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2728 - accuracy: 0.8845\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2692 - accuracy: 0.8875\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2680 - accuracy: 0.8885\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8892\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.8883\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.8882\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2663 - accuracy: 0.8885\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8903\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.8922\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2609 - accuracy: 0.8903\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8917\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2588 - accuracy: 0.8917\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2558 - accuracy: 0.8943\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2556 - accuracy: 0.8948\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2536 - accuracy: 0.8937\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2525 - accuracy: 0.8954\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2535 - accuracy: 0.8919\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:24:15.019385: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:24:15.259587: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4227 - accuracy: 0.8105\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3544 - accuracy: 0.8474\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3466 - accuracy: 0.8528\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3404 - accuracy: 0.8564\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8595\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3329 - accuracy: 0.8597\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8602\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.8646\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8606\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3216 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8670\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8650\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8686\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8700\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8717\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3122 - accuracy: 0.8698\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3117 - accuracy: 0.8714\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3084 - accuracy: 0.8735\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3067 - accuracy: 0.8718\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3062 - accuracy: 0.8750\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3053 - accuracy: 0.8742\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3029 - accuracy: 0.8758\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3015 - accuracy: 0.8742\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2990 - accuracy: 0.8791\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2997 - accuracy: 0.8748\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2970 - accuracy: 0.8757\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8779\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2957 - accuracy: 0.8774\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.8782\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2932 - accuracy: 0.8786\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2922 - accuracy: 0.8783\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2908 - accuracy: 0.8789\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2883 - accuracy: 0.8820\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.8791\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2862 - accuracy: 0.8825\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2858 - accuracy: 0.8825\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2839 - accuracy: 0.8833\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2832 - accuracy: 0.8833\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2824 - accuracy: 0.8848\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2808 - accuracy: 0.8847\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2800 - accuracy: 0.8841\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.8844\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2766 - accuracy: 0.8861\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.8857\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2736 - accuracy: 0.8878\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2747 - accuracy: 0.8855\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2710 - accuracy: 0.8898\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2721 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2697 - accuracy: 0.8903\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.8893\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:24:39.127950: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:24:39.389986: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4062 - accuracy: 0.8234\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3468 - accuracy: 0.8505\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3349 - accuracy: 0.8607\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.8662\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8665\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.8689\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8713\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8698\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3083 - accuracy: 0.8740\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.8730\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3033 - accuracy: 0.8731\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3009 - accuracy: 0.8758\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3000 - accuracy: 0.8743\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2993 - accuracy: 0.8756\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2960 - accuracy: 0.8776\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2948 - accuracy: 0.8795\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2930 - accuracy: 0.8799\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2915 - accuracy: 0.8808\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2888 - accuracy: 0.8819\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2868 - accuracy: 0.8841\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2883 - accuracy: 0.8832\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.8836\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.8850\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2822 - accuracy: 0.8854\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2815 - accuracy: 0.8841\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2811 - accuracy: 0.8844\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2772 - accuracy: 0.8869\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.8894\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2743 - accuracy: 0.8878\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.8854\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.8882\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2720 - accuracy: 0.8864\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2713 - accuracy: 0.8898\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2701 - accuracy: 0.8899\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2691 - accuracy: 0.8913\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2669 - accuracy: 0.8919\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2668 - accuracy: 0.8917\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.8917\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.8929\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2629 - accuracy: 0.8926\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.8924\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2622 - accuracy: 0.8930\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2594 - accuracy: 0.8948\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2589 - accuracy: 0.8961\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2598 - accuracy: 0.8937\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2570 - accuracy: 0.8961\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2559 - accuracy: 0.8966\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2583 - accuracy: 0.8941\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2554 - accuracy: 0.8975\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2531 - accuracy: 0.8960\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:25:03.354151: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:25:03.610167: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4157 - accuracy: 0.8108\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 0.3517 - accuracy: 0.8516\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3379 - accuracy: 0.8599\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3287 - accuracy: 0.8617\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.8673\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8673\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8677\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8713\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3119 - accuracy: 0.8727\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3101 - accuracy: 0.8717\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.8740\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3047 - accuracy: 0.8737\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3028 - accuracy: 0.8736\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3015 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2989 - accuracy: 0.8756\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2983 - accuracy: 0.8764\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2956 - accuracy: 0.8763\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2955 - accuracy: 0.8775\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8791\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2919 - accuracy: 0.8795\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2893 - accuracy: 0.8783\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.8787\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2871 - accuracy: 0.8792\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.8800\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.8814\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2821 - accuracy: 0.8812\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8841\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2809 - accuracy: 0.8796\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.8817\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.8826\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2750 - accuracy: 0.8825\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.8851\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.8852\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2699 - accuracy: 0.8868\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2706 - accuracy: 0.8866\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2690 - accuracy: 0.8878\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8889\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2675 - accuracy: 0.8867\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8910\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2629 - accuracy: 0.8892\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2631 - accuracy: 0.8895\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8912\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2623 - accuracy: 0.8894\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2600 - accuracy: 0.8924\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2585 - accuracy: 0.8922\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2578 - accuracy: 0.8938\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2558 - accuracy: 0.8949\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2542 - accuracy: 0.8944\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2561 - accuracy: 0.8917\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:25:28.342231: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:25:28.626464: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4193 - accuracy: 0.8133\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3489 - accuracy: 0.8537\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3411 - accuracy: 0.8561\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8608\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8624\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3260 - accuracy: 0.8633\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.8673\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8661\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8690\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3123 - accuracy: 0.8727\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3101 - accuracy: 0.8717\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3102 - accuracy: 0.8724\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3083 - accuracy: 0.8735\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8744\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8751\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.8750\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3005 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2987 - accuracy: 0.8771\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2976 - accuracy: 0.8765\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2961 - accuracy: 0.8812\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2950 - accuracy: 0.8783\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.8792\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2925 - accuracy: 0.8780\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2910 - accuracy: 0.8802\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2886 - accuracy: 0.8796\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2877 - accuracy: 0.8817\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2859 - accuracy: 0.8819\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.8829\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2840 - accuracy: 0.8813\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2827 - accuracy: 0.8832\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2821 - accuracy: 0.8844\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2802 - accuracy: 0.8841\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.8858\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.8827\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.8831\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.8863\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2735 - accuracy: 0.8850\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2727 - accuracy: 0.8869\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2707 - accuracy: 0.8882\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.8880\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2701 - accuracy: 0.8880\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2653 - accuracy: 0.8906\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2675 - accuracy: 0.8897\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8900\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2632 - accuracy: 0.8905\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.8901\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2614 - accuracy: 0.8892\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8920\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2582 - accuracy: 0.8941\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:25:52.527268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:25:52.781885: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4107 - accuracy: 0.8233\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3542 - accuracy: 0.8493\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3430 - accuracy: 0.8580\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3378 - accuracy: 0.8595\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3332 - accuracy: 0.8592\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8631\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.8662\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3214 - accuracy: 0.8651\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.8658\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3160 - accuracy: 0.8681\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8663\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3119 - accuracy: 0.8724\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.8696\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3082 - accuracy: 0.8709\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3080 - accuracy: 0.8706\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3039 - accuracy: 0.8718\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.8735\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3017 - accuracy: 0.8750\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2996 - accuracy: 0.8752\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2986 - accuracy: 0.8757\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2955 - accuracy: 0.8757\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2938 - accuracy: 0.8755\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2963 - accuracy: 0.8733\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.8754\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2894 - accuracy: 0.8793\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.8777\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2875 - accuracy: 0.8800\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2867 - accuracy: 0.8807\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.8798\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2845 - accuracy: 0.8806\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2814 - accuracy: 0.8808\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2816 - accuracy: 0.8776\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2797 - accuracy: 0.8826\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.8820\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.8825\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2780 - accuracy: 0.8844\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.8826\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2747 - accuracy: 0.8825\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2749 - accuracy: 0.8830\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2738 - accuracy: 0.8851\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2713 - accuracy: 0.8858\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.8847\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.8851\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8855\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.8861\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2656 - accuracy: 0.8854\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2664 - accuracy: 0.8879\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2635 - accuracy: 0.8878\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8883\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8878\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:26:16.843081: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:26:17.098243: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4164 - accuracy: 0.8142\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3531 - accuracy: 0.8509\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3428 - accuracy: 0.8553\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3363 - accuracy: 0.8584\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8602\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3277 - accuracy: 0.8628\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.8639\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8640\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3165 - accuracy: 0.8688\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8676\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3120 - accuracy: 0.8700\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3098 - accuracy: 0.8690\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3091 - accuracy: 0.8705\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8727\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8732\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.8740\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3001 - accuracy: 0.8754\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2994 - accuracy: 0.8754\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2964 - accuracy: 0.8780\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8768\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8783\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.8770\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2924 - accuracy: 0.8783\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.8791\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2895 - accuracy: 0.8819\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.8792\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2864 - accuracy: 0.8798\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2837 - accuracy: 0.8829\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8850\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2807 - accuracy: 0.8832\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2798 - accuracy: 0.8825\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2772 - accuracy: 0.8857\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.8845\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2762 - accuracy: 0.8854\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2743 - accuracy: 0.8869\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2732 - accuracy: 0.8882\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2715 - accuracy: 0.8892\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2692 - accuracy: 0.8895\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2691 - accuracy: 0.8897\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2674 - accuracy: 0.8900\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8903\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8898\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.8924\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2635 - accuracy: 0.8912\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2617 - accuracy: 0.8929\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2600 - accuracy: 0.8932\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.8901\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.8922\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2579 - accuracy: 0.8948\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2564 - accuracy: 0.8943\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:26:40.507529: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:26:40.776018: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4098 - accuracy: 0.8223\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3516 - accuracy: 0.8518\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8570\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8618\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3268 - accuracy: 0.8633\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3235 - accuracy: 0.8636\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8648\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3146 - accuracy: 0.8692\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3122 - accuracy: 0.8698\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3094 - accuracy: 0.8706\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3073 - accuracy: 0.8735\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3067 - accuracy: 0.8736\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3035 - accuracy: 0.8740\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3009 - accuracy: 0.8752\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.2997 - accuracy: 0.8750\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2989 - accuracy: 0.8762\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2952 - accuracy: 0.8748\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2943 - accuracy: 0.8775\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2923 - accuracy: 0.8789\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2906 - accuracy: 0.8781\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2913 - accuracy: 0.8781\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2874 - accuracy: 0.8805\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2871 - accuracy: 0.8798\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2850 - accuracy: 0.8821\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2841 - accuracy: 0.8824\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2818 - accuracy: 0.8824\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.8842\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2799 - accuracy: 0.8839\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2787 - accuracy: 0.8854\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2755 - accuracy: 0.8843\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.8844\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2744 - accuracy: 0.8847\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2726 - accuracy: 0.8863\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2734 - accuracy: 0.8866\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2703 - accuracy: 0.8880\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2710 - accuracy: 0.8866\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2682 - accuracy: 0.8895\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2667 - accuracy: 0.8883\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8872\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2666 - accuracy: 0.8900\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.8918\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.8901\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8919\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2603 - accuracy: 0.8919\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2595 - accuracy: 0.8935\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2583 - accuracy: 0.8924\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2567 - accuracy: 0.8929\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2565 - accuracy: 0.8937\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2566 - accuracy: 0.8938\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2536 - accuracy: 0.8959\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:27:04.663345: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:27:04.929428: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4079 - accuracy: 0.8158\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8544\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.8597\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3345 - accuracy: 0.8620\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8646\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.8653\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8676\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8705\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3172 - accuracy: 0.8681\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8698\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3109 - accuracy: 0.8709\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.8719\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3047 - accuracy: 0.8745\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3046 - accuracy: 0.8743\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3040 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3011 - accuracy: 0.8763\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2990 - accuracy: 0.8760\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2979 - accuracy: 0.8744\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2967 - accuracy: 0.8762\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2940 - accuracy: 0.8773\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2923 - accuracy: 0.8786\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2909 - accuracy: 0.8807\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.2897 - accuracy: 0.8795\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2900 - accuracy: 0.8787\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2854 - accuracy: 0.8794\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.8806\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.8816\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2823 - accuracy: 0.8817\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8819\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2785 - accuracy: 0.8841\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.8829\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.8849\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2746 - accuracy: 0.8856\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2740 - accuracy: 0.8847\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2735 - accuracy: 0.8847\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2696 - accuracy: 0.8881\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2690 - accuracy: 0.8898\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2683 - accuracy: 0.8869\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2679 - accuracy: 0.8867\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2654 - accuracy: 0.8888\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2634 - accuracy: 0.8916\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2627 - accuracy: 0.8906\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2611 - accuracy: 0.8913\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2619 - accuracy: 0.8916\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2596 - accuracy: 0.8928\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2588 - accuracy: 0.8926\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2587 - accuracy: 0.8925\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2569 - accuracy: 0.8948\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2567 - accuracy: 0.8926\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.2556 - accuracy: 0.8924\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:27:29.007584: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:27:29.244683: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 9ms/step - loss: 0.4012 - accuracy: 0.8255\n",
      "Epoch 2/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3488 - accuracy: 0.8548\n",
      "Epoch 3/50\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.3365 - accuracy: 0.8601\n",
      "Epoch 4/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8601\n",
      "Epoch 5/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8636\n",
      "Epoch 6/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8653\n",
      "Epoch 8/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8655\n",
      "Epoch 9/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8687\n",
      "Epoch 10/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8684\n",
      "Epoch 11/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8684\n",
      "Epoch 12/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3118 - accuracy: 0.8708\n",
      "Epoch 13/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3099 - accuracy: 0.8693\n",
      "Epoch 14/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3080 - accuracy: 0.8717\n",
      "Epoch 15/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3056 - accuracy: 0.8730\n",
      "Epoch 16/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8718\n",
      "Epoch 17/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3018 - accuracy: 0.8753\n",
      "Epoch 18/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3015 - accuracy: 0.8752\n",
      "Epoch 19/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8749\n",
      "Epoch 20/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2985 - accuracy: 0.8758\n",
      "Epoch 21/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2960 - accuracy: 0.8764\n",
      "Epoch 22/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2936 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8780\n",
      "Epoch 24/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2919 - accuracy: 0.8788\n",
      "Epoch 25/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8804\n",
      "Epoch 26/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2888 - accuracy: 0.8794\n",
      "Epoch 27/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8790\n",
      "Epoch 28/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2859 - accuracy: 0.8806\n",
      "Epoch 29/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8817\n",
      "Epoch 30/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8831\n",
      "Epoch 31/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8805\n",
      "Epoch 32/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2807 - accuracy: 0.8845\n",
      "Epoch 33/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8845\n",
      "Epoch 34/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8848\n",
      "Epoch 35/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8848\n",
      "Epoch 36/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8832\n",
      "Epoch 37/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2745 - accuracy: 0.8869\n",
      "Epoch 38/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8846\n",
      "Epoch 39/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2716 - accuracy: 0.8864\n",
      "Epoch 40/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8890\n",
      "Epoch 41/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.8869\n",
      "Epoch 42/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8887\n",
      "Epoch 43/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2680 - accuracy: 0.8888\n",
      "Epoch 44/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8885\n",
      "Epoch 45/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8894\n",
      "Epoch 46/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8914\n",
      "Epoch 47/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.8882\n",
      "Epoch 48/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8906\n",
      "Epoch 49/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8931\n",
      "Epoch 50/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8925\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x42c558670&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3c820&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x42c558670&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3c820&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x42c558670&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x42c558670&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=<function create_model at 0x42c558670>, epochs=30),\n",
       "             param_grid={'batch_size': [32, 64, 96], 'epochs': [50],\n",
       "                         'optimizer': [<keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30>,\n",
       "                                       <keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3c820>]},\n",
       "             refit='f1_macro',\n",
       "             scoring=['accuracy', 'f1_macro', 'precision_macro',\n",
       "                      'recall_macro'])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 64, 'epochs': 50, 'optimizer': <keras.optimizers.optimizer_v2.adam.Adam object at 0x3c2c3cd30>}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with small subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:32:43.021296: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 9ms/step - loss: 0.4758 - accuracy: 0.7996\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3813 - accuracy: 0.8363\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3608 - accuracy: 0.8493\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3516 - accuracy: 0.8547\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8541\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3439 - accuracy: 0.8587\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3411 - accuracy: 0.8603\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3404 - accuracy: 0.8588\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8602\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3387 - accuracy: 0.8583\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8606\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8620\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8584\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8611\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3358 - accuracy: 0.8630\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8618\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8640\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8611\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8621\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8622\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8607\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8624\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8634\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8620\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8620\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8599\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8609\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8622\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8617\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8628\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8618\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8638\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8618\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8611\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8620\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8642\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8638\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8625\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8640\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8636\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8626\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8626\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8640\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8633\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8636\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8637\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8627\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8638\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:33:16.635692: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:33:16.925359: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4652 - accuracy: 0.7946\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3791 - accuracy: 0.8394\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3623 - accuracy: 0.8500\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3528 - accuracy: 0.8528\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3495 - accuracy: 0.8531\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8543\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3451 - accuracy: 0.8568\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8541\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3418 - accuracy: 0.8571\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3410 - accuracy: 0.8569\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8586\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3392 - accuracy: 0.8568\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3384 - accuracy: 0.8587\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8605\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8594\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8589\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3376 - accuracy: 0.8582\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3358 - accuracy: 0.8613\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8595\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8599\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8608\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8600\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8603\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8601\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3348 - accuracy: 0.8605\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8605\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8622\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8590\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8612\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8617\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8612\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8612\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8599\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8619\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8597\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8605\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8601\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8630\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8628\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8611\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8620\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8619\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8620\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8621\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8630\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8620\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8622\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8630\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:33:50.856342: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:33:51.102486: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4806 - accuracy: 0.7854\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3813 - accuracy: 0.8363\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3618 - accuracy: 0.8477\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3537 - accuracy: 0.8494\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3491 - accuracy: 0.8540\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3448 - accuracy: 0.8563\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8552\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8543\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8586\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3402 - accuracy: 0.8557\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8576\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8566\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8569\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8568\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3375 - accuracy: 0.8580\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8587\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8589\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3358 - accuracy: 0.8599\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8582\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8606\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8612\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8582\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8602\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8589\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8611\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8608\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8588\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8584\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8582\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8622\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8592\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8613\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8615\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8621\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8620\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8606\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8628\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8615\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8622\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8613\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8640\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8627\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8625\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8608\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8648\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8637\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8613\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8612\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8628\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:34:25.384690: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:34:25.664220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4875 - accuracy: 0.7890\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3720 - accuracy: 0.8425\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3505 - accuracy: 0.8566\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3406 - accuracy: 0.8617\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3361 - accuracy: 0.8620\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8601\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8624\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8640\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8630\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8621\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8628\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8651\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8659\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8655\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8658\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8649\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8656\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8657\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8642\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3223 - accuracy: 0.8656\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8675\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8662\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8661\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8663\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8650\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8668\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8675\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8645\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8668\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8663\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8673\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3193 - accuracy: 0.8661\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8674\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8680\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3201 - accuracy: 0.8674\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8695\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8676\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8675\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8681\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3191 - accuracy: 0.8670\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8649\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8682\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3180 - accuracy: 0.8665\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8683\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8670\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8671\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8702\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8683\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8688\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:35:00.629499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 22:35:00.886216: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4779 - accuracy: 0.7815\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3812 - accuracy: 0.8335\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3605 - accuracy: 0.8501\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3504 - accuracy: 0.8528\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8583\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3414 - accuracy: 0.8569\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8593\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8600\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8592\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8606\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8614\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8597\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8609\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8595\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8608\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8627\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8624\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8627\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8626\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8637\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8622\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8643\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8648\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8652\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8643\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8643\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8648\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8631\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8633\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8636\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8645\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8631\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8662\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8649\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8637\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8649\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8644\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8642\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8640\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8651\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8640\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8664\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8649\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8652\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8682\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8662\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8655\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3231 - accuracy: 0.8645\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Mean Accuracy: 0.8591992373689228\n",
      "Mean F1 Macro: 0.8424138259306495\n",
      "Mean Precision Macro: 0.8677992723881209\n",
      "Mean Recall Macro: 0.8298988923464122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:35:35.647436: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train_sf, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II subset features before HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning with subset of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:41:13.853214: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.4457 - accuracy: 0.8009\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3622 - accuracy: 0.8497\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3485 - accuracy: 0.8564\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8613\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8599\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3392 - accuracy: 0.8601\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3374 - accuracy: 0.8597\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3357 - accuracy: 0.8607\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8608\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8612\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8601\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8608\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8624\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8619\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8622\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8643\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8621\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3320 - accuracy: 0.8603\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8648\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8633\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8606\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8630\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8638\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8633\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8639\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8618\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8619\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8637\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8612\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8625\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8624\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8648\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8636\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8637\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8646\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8645\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8636\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8621\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8637\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8630\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8655\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8643\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8651\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8650\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8643\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8632\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8634\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8645\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3231 - accuracy: 0.8643\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:42:18.874961: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:42:19.193528: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3838 - accuracy: 0.8346\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3479 - accuracy: 0.8558\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3445 - accuracy: 0.8572\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3413 - accuracy: 0.8570\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8578\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3383 - accuracy: 0.8596\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3375 - accuracy: 0.8587\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3374 - accuracy: 0.8578\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8601\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8582\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8602\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8595\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8613\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8603\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8615\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3347 - accuracy: 0.8603\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8593\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8597\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8601\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3330 - accuracy: 0.8620\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8593\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8613\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8618\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8606\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8621\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8611\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8631\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8597\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8634\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8621\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8634\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8607\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8638\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8648\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8628\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8640\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3277 - accuracy: 0.8628\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8630\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8633\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8631\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8640\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8640\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8657\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8639\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8643\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8651\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8659\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:43:25.396228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:43:25.710255: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 2s 7ms/step - loss: 0.3821 - accuracy: 0.8354\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3504 - accuracy: 0.8510\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3453 - accuracy: 0.8559\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3417 - accuracy: 0.8545\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3406 - accuracy: 0.8556\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8578\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3389 - accuracy: 0.8592\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8599\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8589\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8588\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8593\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8617\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8590\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8596\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8584\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3347 - accuracy: 0.8600\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8597\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8584\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8615\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8605\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8606\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8611\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8639\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8612\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8625\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8625\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8615\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8619\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8614\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8607\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8611\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8619\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8618\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8630\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8646\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8634\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8644\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8631\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8621\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8633\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8631\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8624\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8637\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8627\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8639\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8664\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:44:33.090715: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:44:33.410237: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3767 - accuracy: 0.8384\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8606\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8607\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8625\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8640\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8646\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8645\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8639\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8679\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8653\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8652\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8651\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8636\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8648\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8657\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3230 - accuracy: 0.8680\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8682\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8673\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3217 - accuracy: 0.8674\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8668\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3210 - accuracy: 0.8677\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8663\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3204 - accuracy: 0.8686\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8683\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8658\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3203 - accuracy: 0.8668\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8662\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8681\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8668\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8671\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8681\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8687\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8687\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8690\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8692\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8676\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8683\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8679\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8687\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8687\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8693\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8675\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8679\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8680\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8675\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8701\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8680\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8684\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3160 - accuracy: 0.8706\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:45:41.431133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:45:42.419404: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3775 - accuracy: 0.8401\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3452 - accuracy: 0.8555\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8577\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8589\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3356 - accuracy: 0.8615\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8636\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8632\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8632\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8608\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8625\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8653\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8649\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 2s 6ms/step - loss: 0.3294 - accuracy: 0.8638\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8642\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8648\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8650\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8643\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8664\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8645\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8652\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8657\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8659\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8663\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8656\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8652\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8652\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8670\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8643\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8657\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8652\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8657\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8658\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8668\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8655\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8670\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8665\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8655\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8674\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8674\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8673\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8665\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8682\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8682\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3222 - accuracy: 0.8668\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8676\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8669\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8681\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8673\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8680\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:46:51.277610: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:46:51.595972: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3865 - accuracy: 0.8358\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3461 - accuracy: 0.8565\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3419 - accuracy: 0.8575\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8586\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3372 - accuracy: 0.8599\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8614\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8614\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8595\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8608\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8605\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8632\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8608\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8611\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8608\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8621\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8640\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8637\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8607\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8627\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8619\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8618\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8640\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8617\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8632\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8625\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8628\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8631\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8643\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8634\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8653\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8640\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8663\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8634\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8642\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8648\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8646\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8649\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8646\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8625\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8645\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8648\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8646\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8659\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8658\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8650\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8656\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3246 - accuracy: 0.8651\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:48:00.552107: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:48:00.884416: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3877 - accuracy: 0.8354\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3515 - accuracy: 0.8510\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3454 - accuracy: 0.8561\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3437 - accuracy: 0.8553\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3409 - accuracy: 0.8572\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8581\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8595\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8576\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8605\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8594\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8574\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8594\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8583\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8599\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8628\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8595\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8605\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8586\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8622\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8589\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8627\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8628\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8615\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8608\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8633\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8617\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8622\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8619\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8621\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8618\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8619\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8627\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8636\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8653\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8640\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8639\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8645\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8642\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8655\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8639\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8645\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8637\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8643\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3246 - accuracy: 0.8646\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8640\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8659\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8659\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8649\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:49:08.458991: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:49:08.768094: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3823 - accuracy: 0.8389\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3483 - accuracy: 0.8521\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3448 - accuracy: 0.8547\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3414 - accuracy: 0.8569\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8552\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8576\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8583\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3379 - accuracy: 0.8578\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8575\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3359 - accuracy: 0.8577\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8593\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8596\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8588\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8577\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8603\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8603\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8599\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8617\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8619\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8615\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8601\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8612\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8592\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8620\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8624\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8595\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8621\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8599\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8619\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8611\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8620\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8628\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8637\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3277 - accuracy: 0.8628\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8637\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8655\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8633\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8626\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8628\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8633\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8650\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8630\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8658\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8644\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8644\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3263 - accuracy: 0.8640\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8631\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:50:16.863695: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:50:17.187618: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3809 - accuracy: 0.8359\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3448 - accuracy: 0.8570\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8601\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8619\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8607\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8619\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8667\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8645\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8633\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8639\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8651\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8673\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8652\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8648\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8636\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8665\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8659\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8667\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8675\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8671\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8650\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8684\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3199 - accuracy: 0.8680\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8674\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3190 - accuracy: 0.8676\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8674\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8680\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8663\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8679\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8671\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8671\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8684\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3179 - accuracy: 0.8673\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8680\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8688\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3179 - accuracy: 0.8688\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8683\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8694\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8676\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3152 - accuracy: 0.8694\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8702\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8694\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3159 - accuracy: 0.8695\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3152 - accuracy: 0.8704\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3153 - accuracy: 0.8693\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3155 - accuracy: 0.8689\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8695\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3162 - accuracy: 0.8674\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3151 - accuracy: 0.8657\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3144 - accuracy: 0.8704\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:51:24.991091: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:51:25.308484: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 3s 8ms/step - loss: 0.3818 - accuracy: 0.8364\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3450 - accuracy: 0.8576\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3398 - accuracy: 0.8586\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8628\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8597\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3340 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8642\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8615\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8632\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8646\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8618\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3307 - accuracy: 0.8633\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8640\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8642\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3290 - accuracy: 0.8643\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8622\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 1s 6ms/step - loss: 0.3286 - accuracy: 0.8634\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8651\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8627\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8646\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8657\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8636\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8630\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8643\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8634\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8650\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8642\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8639\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8650\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8674\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8656\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3239 - accuracy: 0.8655\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8667\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8661\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8664\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8674\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8651\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8653\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8682\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8683\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3223 - accuracy: 0.8657\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8679\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8688\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8655\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8668\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8673\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8675\n",
      "66/66 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:52:34.752047: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:52:35.062575: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3990 - accuracy: 0.8258\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3497 - accuracy: 0.8561\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8576\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8589\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3386 - accuracy: 0.8575\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8609\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8603\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8614\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8606\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8617\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8621\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8625\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8633\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8614\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8617\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8619\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8614\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8632\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8620\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8630\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8625\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8626\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8638\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8620\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8653\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8631\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8631\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8626\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8622\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8636\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8639\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8642\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8648\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8646\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8633\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8643\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8648\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8636\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8642\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8653\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8643\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8669\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8648\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8644\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8650\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8648\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8648\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8631\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8668\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:53:10.294860: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:53:10.568545: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3978 - accuracy: 0.8290\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3503 - accuracy: 0.8531\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3438 - accuracy: 0.8545\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3418 - accuracy: 0.8559\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3405 - accuracy: 0.8553\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3389 - accuracy: 0.8580\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8599\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8593\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3363 - accuracy: 0.8583\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8581\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8615\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8586\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8618\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8589\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8602\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8603\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3348 - accuracy: 0.8613\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8624\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8603\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8618\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8602\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8618\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8617\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8600\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8614\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8621\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8613\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8617\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8603\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8613\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8614\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8606\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8611\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8618\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8605\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8624\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8612\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8625\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8622\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8611\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8615\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8621\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8639\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8605\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8622\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8628\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3266 - accuracy: 0.8622\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8620\n",
      "33/33 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:53:45.754807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:53:46.024670: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.3967 - accuracy: 0.8296\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3537 - accuracy: 0.8546\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3460 - accuracy: 0.8557\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3444 - accuracy: 0.8564\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3411 - accuracy: 0.8566\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8578\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3394 - accuracy: 0.8564\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3368 - accuracy: 0.8596\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8588\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8569\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3367 - accuracy: 0.8584\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8570\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8594\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3344 - accuracy: 0.8578\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8580\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3355 - accuracy: 0.8574\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8603\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3330 - accuracy: 0.8601\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8600\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8605\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8603\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3333 - accuracy: 0.8595\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8632\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8597\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8596\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8620\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8617\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8608\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8626\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8600\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8613\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8618\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8618\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8609\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8620\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8619\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8617\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8636\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8636\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8627\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8626\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8632\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8609\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8645\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8605\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8642\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8639\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8607\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8637\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:54:21.264850: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:54:21.537763: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.3959 - accuracy: 0.8275\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3426 - accuracy: 0.8581\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8618\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8631\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8652\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8657\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8649\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8659\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8649\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3246 - accuracy: 0.8642\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8693\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8676\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8646\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8651\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8664\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8680\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8659\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8659\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3222 - accuracy: 0.8690\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8677\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8661\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8667\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3210 - accuracy: 0.8675\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8677\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3203 - accuracy: 0.8693\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8682\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8667\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3190 - accuracy: 0.8680\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8684\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8694\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8671\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8686\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8679\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8687\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3181 - accuracy: 0.8688\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3175 - accuracy: 0.8681\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3180 - accuracy: 0.8670\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8681\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3175 - accuracy: 0.8674\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3168 - accuracy: 0.8674\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8693\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8689\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8694\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3161 - accuracy: 0.8693\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8675\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8700\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8680\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8698\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:54:56.672810: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:54:56.946130: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3966 - accuracy: 0.8285\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3456 - accuracy: 0.8570\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3396 - accuracy: 0.8571\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3359 - accuracy: 0.8580\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8615\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3334 - accuracy: 0.8607\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8622\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8631\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8621\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3306 - accuracy: 0.8640\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8627\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8619\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8628\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8632\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8628\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8624\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8631\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8643\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8631\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8639\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8650\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8642\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8662\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8643\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8651\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8651\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8664\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8665\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8646\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8640\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8658\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8640\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8653\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8667\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8653\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8663\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3239 - accuracy: 0.8657\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8653\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8638\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8669\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8669\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8675\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8671\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8656\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8643\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8650\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8653\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8677\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:55:32.381509: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:55:32.667291: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3967 - accuracy: 0.8279\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3497 - accuracy: 0.8557\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3453 - accuracy: 0.8575\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3412 - accuracy: 0.8564\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3389 - accuracy: 0.8608\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8590\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8607\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3359 - accuracy: 0.8603\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8611\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3342 - accuracy: 0.8603\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3347 - accuracy: 0.8615\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8605\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8619\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8622\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8619\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8607\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8603\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8619\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8612\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8617\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8633\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8612\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8639\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8628\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8634\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8628\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8633\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8617\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8630\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8638\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8643\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8634\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8646\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8632\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8624\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8628\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8657\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8642\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8636\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8638\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8632\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8626\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8657\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8636\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8628\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3264 - accuracy: 0.8640\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8640\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8657\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8640\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8646\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:56:07.859423: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:56:08.141355: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4064 - accuracy: 0.8246\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3547 - accuracy: 0.8508\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3469 - accuracy: 0.8546\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3437 - accuracy: 0.8572\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3411 - accuracy: 0.8597\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8593\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3384 - accuracy: 0.8587\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3373 - accuracy: 0.8602\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3365 - accuracy: 0.8608\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8605\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8596\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3370 - accuracy: 0.8589\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3361 - accuracy: 0.8607\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3352 - accuracy: 0.8607\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8596\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8601\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8614\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8611\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8612\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8637\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3325 - accuracy: 0.8630\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8609\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8614\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8588\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8631\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3315 - accuracy: 0.8619\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8602\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8619\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8621\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8617\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.8608\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8617\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8621\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8613\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8608\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8621\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8613\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8613\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8621\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8613\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8617\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8632\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8640\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8614\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8622\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8620\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8638\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:56:43.407428: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:56:43.691042: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4036 - accuracy: 0.8233\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3526 - accuracy: 0.8568\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3471 - accuracy: 0.8547\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3434 - accuracy: 0.8552\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3414 - accuracy: 0.8566\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3409 - accuracy: 0.8572\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3391 - accuracy: 0.8559\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3388 - accuracy: 0.8601\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8593\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8601\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3364 - accuracy: 0.8577\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3356 - accuracy: 0.8592\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.8578\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8602\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8586\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8602\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8600\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8592\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3332 - accuracy: 0.8602\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8596\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8606\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3328 - accuracy: 0.8606\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8619\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8607\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8599\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8626\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8625\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3322 - accuracy: 0.8599\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8611\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3306 - accuracy: 0.8599\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8612\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8614\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3302 - accuracy: 0.8614\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8614\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8634\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8617\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8609\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8626\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8620\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8625\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8628\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8640\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8636\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3277 - accuracy: 0.8626\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8613\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8645\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8637\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8638\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8645\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8625\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:57:19.158074: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:57:19.439552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3905 - accuracy: 0.8320\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3407 - accuracy: 0.8587\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3351 - accuracy: 0.8596\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8644\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3310 - accuracy: 0.8638\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8657\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3273 - accuracy: 0.8645\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8651\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3253 - accuracy: 0.8661\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8668\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8653\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8664\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8675\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8651\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8671\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3231 - accuracy: 0.8665\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8669\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8652\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8673\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3217 - accuracy: 0.8665\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8673\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8674\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3203 - accuracy: 0.8674\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3208 - accuracy: 0.8662\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3210 - accuracy: 0.8670\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8687\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8690\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3195 - accuracy: 0.8677\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8679\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3199 - accuracy: 0.8682\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8684\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3200 - accuracy: 0.8681\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3189 - accuracy: 0.8684\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3191 - accuracy: 0.8677\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3194 - accuracy: 0.8662\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8675\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3188 - accuracy: 0.8695\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8695\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3193 - accuracy: 0.8665\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3185 - accuracy: 0.8676\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3176 - accuracy: 0.8692\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8686\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8671\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8677\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8702\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8690\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8689\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8702\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3174 - accuracy: 0.8698\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3170 - accuracy: 0.8682\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:57:54.324747: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:57:54.606082: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.3945 - accuracy: 0.8348\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3481 - accuracy: 0.8538\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3422 - accuracy: 0.8583\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3384 - accuracy: 0.8605\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8602\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8626\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8607\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8624\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8621\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3316 - accuracy: 0.8619\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8638\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8608\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8637\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8626\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8636\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8644\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.8627\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8640\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8636\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8633\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8639\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8627\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8642\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8656\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3274 - accuracy: 0.8657\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3260 - accuracy: 0.8655\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8642\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8643\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8652\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8642\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8640\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3241 - accuracy: 0.8634\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8646\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8659\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3248 - accuracy: 0.8653\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8636\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3233 - accuracy: 0.8639\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3229 - accuracy: 0.8662\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.8659\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3235 - accuracy: 0.8653\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8665\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8664\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8665\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8662\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3221 - accuracy: 0.8656\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8674\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3224 - accuracy: 0.8664\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8676\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8653\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:58:29.885016: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:58:30.177292: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 14ms/step - loss: 0.4072 - accuracy: 0.8216\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3508 - accuracy: 0.8562\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3438 - accuracy: 0.8558\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3409 - accuracy: 0.8581\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3388 - accuracy: 0.8593\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3381 - accuracy: 0.8601\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.8601\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3355 - accuracy: 0.8582\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3351 - accuracy: 0.8619\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8620\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3333 - accuracy: 0.8603\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3344 - accuracy: 0.8611\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8625\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8628\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3319 - accuracy: 0.8615\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3314 - accuracy: 0.8634\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8634\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8622\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8630\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8619\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8630\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.8634\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8637\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8642\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8631\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8642\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8627\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8652\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8630\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8648\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8632\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8652\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8648\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8628\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8645\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8622\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8639\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8663\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8644\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3261 - accuracy: 0.8634\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.8648\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3260 - accuracy: 0.8640\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8657\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8640\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3253 - accuracy: 0.8638\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3253 - accuracy: 0.8658\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8650\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8649\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:58:54.127973: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:58:54.389995: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4160 - accuracy: 0.8177\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3541 - accuracy: 0.8508\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3456 - accuracy: 0.8546\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3422 - accuracy: 0.8587\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3425 - accuracy: 0.8580\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3386 - accuracy: 0.8587\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3379 - accuracy: 0.8593\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3379 - accuracy: 0.8584\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8577\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8588\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.8615\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8605\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.8606\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8594\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3349 - accuracy: 0.8605\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8600\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3330 - accuracy: 0.8607\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3341 - accuracy: 0.8589\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3333 - accuracy: 0.8609\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8611\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8609\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3329 - accuracy: 0.8613\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3325 - accuracy: 0.8606\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8599\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8608\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8600\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8595\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3307 - accuracy: 0.8618\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8597\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8627\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8621\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8617\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8597\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8607\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8617\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8637\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.8620\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.8620\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8624\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3277 - accuracy: 0.8636\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8620\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8626\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8630\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8627\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3275 - accuracy: 0.8639\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8619\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8630\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8614\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8632\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:59:18.230261: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:59:18.484651: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4011 - accuracy: 0.8290\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3532 - accuracy: 0.8524\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3452 - accuracy: 0.8534\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3436 - accuracy: 0.8570\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3412 - accuracy: 0.8570\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3389 - accuracy: 0.8572\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3383 - accuracy: 0.8582\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8587\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3379 - accuracy: 0.8547\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3362 - accuracy: 0.8588\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3359 - accuracy: 0.8581\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.8575\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8606\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8583\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3341 - accuracy: 0.8605\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8592\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3332 - accuracy: 0.8606\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8609\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3338 - accuracy: 0.8607\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.8614\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3328 - accuracy: 0.8600\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8596\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8621\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8612\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3319 - accuracy: 0.8594\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8607\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8615\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8588\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3316 - accuracy: 0.8601\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8620\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3306 - accuracy: 0.8624\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8621\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8611\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8617\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8632\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8613\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8609\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8618\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8626\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8619\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8615\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8627\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8613\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8626\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8628\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8614\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8607\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8631\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8624\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:59:42.679599: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 22:59:42.953386: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4128 - accuracy: 0.8171\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3490 - accuracy: 0.8559\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3414 - accuracy: 0.8584\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3360 - accuracy: 0.8595\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3341 - accuracy: 0.8632\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8617\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.8620\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3298 - accuracy: 0.8639\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8608\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8643\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8648\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8645\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8633\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8644\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8632\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8639\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.8628\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3240 - accuracy: 0.8636\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3241 - accuracy: 0.8640\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8657\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8643\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8657\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8673\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8661\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8676\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8658\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8664\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8665\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8670\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8680\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8658\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8687\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8667\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3193 - accuracy: 0.8682\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3194 - accuracy: 0.8670\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8680\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8674\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3192 - accuracy: 0.8684\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8689\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8681\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3191 - accuracy: 0.8679\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8706\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8687\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8687\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8704\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8676\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8686\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8676\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8682\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8686\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:00:06.851034: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:00:07.115632: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.3986 - accuracy: 0.8315\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3462 - accuracy: 0.8563\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8572\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3355 - accuracy: 0.8606\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3337 - accuracy: 0.8624\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8627\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8621\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8621\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8612\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8619\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3298 - accuracy: 0.8621\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8633\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8646\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8632\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8628\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8636\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8667\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8614\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8640\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8642\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8645\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8630\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8643\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3261 - accuracy: 0.8653\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8644\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8640\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8655\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3254 - accuracy: 0.8649\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8645\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8633\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8649\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8648\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8637\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.8650\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8650\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3236 - accuracy: 0.8665\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8648\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.8659\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8668\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8653\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8659\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8655\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8640\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3220 - accuracy: 0.8662\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8664\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8667\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8656\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8686\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8662\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:00:31.249654: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:00:31.526526: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4057 - accuracy: 0.8246\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3518 - accuracy: 0.8531\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3432 - accuracy: 0.8595\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3414 - accuracy: 0.8586\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3394 - accuracy: 0.8599\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.8599\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3384 - accuracy: 0.8581\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3355 - accuracy: 0.8624\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3351 - accuracy: 0.8602\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8614\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3350 - accuracy: 0.8594\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3332 - accuracy: 0.8625\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8611\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8617\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8622\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8617\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8621\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8607\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8639\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8614\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3321 - accuracy: 0.8625\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8611\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8617\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8634\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8645\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8627\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8637\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8644\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8646\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8642\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8637\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8638\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8630\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3287 - accuracy: 0.8646\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8644\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8673\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8638\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8649\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8643\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8634\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8646\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8646\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8642\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8651\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8658\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8640\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8661\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8655\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:00:56.138517: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:00:56.406552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4138 - accuracy: 0.8176\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3543 - accuracy: 0.8506\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3465 - accuracy: 0.8539\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3426 - accuracy: 0.8572\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3411 - accuracy: 0.8571\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3402 - accuracy: 0.8557\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3382 - accuracy: 0.8577\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8596\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8597\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3356 - accuracy: 0.8595\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3344 - accuracy: 0.8589\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3344 - accuracy: 0.8612\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8584\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8599\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3330 - accuracy: 0.8605\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8597\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8609\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3330 - accuracy: 0.8618\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8613\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8637\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.8614\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8627\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.8632\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3315 - accuracy: 0.8614\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8627\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8638\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8631\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8634\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.8622\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3298 - accuracy: 0.8624\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8618\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8625\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8624\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8617\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8632\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8638\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8645\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8615\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8624\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8619\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8627\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8626\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8638\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.8632\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8625\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8627\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8626\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8637\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3270 - accuracy: 0.8638\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:01:20.105507: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:01:20.372474: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 13ms/step - loss: 0.4063 - accuracy: 0.8272\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3541 - accuracy: 0.8522\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3479 - accuracy: 0.8550\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3445 - accuracy: 0.8558\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3417 - accuracy: 0.8557\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3411 - accuracy: 0.8550\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3403 - accuracy: 0.8558\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8561\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3376 - accuracy: 0.8586\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3378 - accuracy: 0.8581\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3367 - accuracy: 0.8606\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8566\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3364 - accuracy: 0.8589\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3357 - accuracy: 0.8601\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8607\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8597\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3360 - accuracy: 0.8559\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3337 - accuracy: 0.8587\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3332 - accuracy: 0.8587\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3340 - accuracy: 0.8601\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.8588\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3345 - accuracy: 0.8606\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8620\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.8613\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3324 - accuracy: 0.8613\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8596\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8597\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8612\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8618\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8619\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8609\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8617\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8614\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.8625\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8613\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8618\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8613\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8628\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8619\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8612\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8628\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8640\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8621\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8613\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8637\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3275 - accuracy: 0.8648\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8622\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8628\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8615\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8622\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:01:44.239922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:01:44.508278: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4060 - accuracy: 0.8226\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3420 - accuracy: 0.8592\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3371 - accuracy: 0.8618\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8619\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8618\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3308 - accuracy: 0.8628\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3286 - accuracy: 0.8643\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8634\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8648\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.8644\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8638\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8664\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8640\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8655\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3236 - accuracy: 0.8638\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.8658\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.8657\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.8655\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3227 - accuracy: 0.8662\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8663\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3227 - accuracy: 0.8645\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8673\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3221 - accuracy: 0.8669\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8664\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3219 - accuracy: 0.8670\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3218 - accuracy: 0.8665\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8661\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.8676\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3212 - accuracy: 0.8671\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8682\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8689\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.8655\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8671\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8658\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8668\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8658\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8670\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.8687\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3193 - accuracy: 0.8688\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8671\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8680\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8680\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.8681\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8677\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3190 - accuracy: 0.8669\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8677\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8682\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3185 - accuracy: 0.8696\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.8668\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8684\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:02:08.457993: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:02:08.726063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 12ms/step - loss: 0.4076 - accuracy: 0.8241\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 6ms/step - loss: 0.3490 - accuracy: 0.8538\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3420 - accuracy: 0.8594\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.8580\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3369 - accuracy: 0.8612\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3362 - accuracy: 0.8596\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3338 - accuracy: 0.8627\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 0.3321 - accuracy: 0.8624\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3332 - accuracy: 0.8619\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3324 - accuracy: 0.8609\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8630\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3313 - accuracy: 0.8650\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3307 - accuracy: 0.8625\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.8643\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8633\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8631\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8631\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8646\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8637\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8649\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8644\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8642\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.8651\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8665\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8643\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8658\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8649\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8638\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8638\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.8644\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.8636\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8664\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.8659\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3254 - accuracy: 0.8644\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8643\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8655\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3240 - accuracy: 0.8661\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8661\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8653\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.8645\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8643\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3236 - accuracy: 0.8671\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8661\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8650\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.8667\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.8661\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8669\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3229 - accuracy: 0.8662\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8676\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8657\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:02:32.760868: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/marcamps/env/lib/python3.8/site-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n",
      "2023-05-24 23:02:33.016371: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 9ms/step - loss: 0.3883 - accuracy: 0.8331\n",
      "Epoch 2/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3459 - accuracy: 0.8567\n",
      "Epoch 3/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8579\n",
      "Epoch 4/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3384 - accuracy: 0.8597\n",
      "Epoch 5/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3368 - accuracy: 0.8594\n",
      "Epoch 6/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3366 - accuracy: 0.8598\n",
      "Epoch 7/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8588\n",
      "Epoch 8/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3339 - accuracy: 0.8608\n",
      "Epoch 9/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8606\n",
      "Epoch 10/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8620\n",
      "Epoch 11/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3323 - accuracy: 0.8607\n",
      "Epoch 12/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8602\n",
      "Epoch 13/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3317 - accuracy: 0.8622\n",
      "Epoch 14/50\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.3313 - accuracy: 0.8609\n",
      "Epoch 15/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8625\n",
      "Epoch 16/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8620\n",
      "Epoch 17/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3301 - accuracy: 0.8633\n",
      "Epoch 18/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8624\n",
      "Epoch 19/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3288 - accuracy: 0.8622\n",
      "Epoch 21/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3295 - accuracy: 0.8643\n",
      "Epoch 22/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8630\n",
      "Epoch 23/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8623\n",
      "Epoch 24/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8633\n",
      "Epoch 25/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8632\n",
      "Epoch 26/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8659\n",
      "Epoch 27/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8640\n",
      "Epoch 28/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3275 - accuracy: 0.8636\n",
      "Epoch 29/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8641\n",
      "Epoch 30/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8640\n",
      "Epoch 31/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8642\n",
      "Epoch 32/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3266 - accuracy: 0.8640\n",
      "Epoch 33/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8650\n",
      "Epoch 34/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8654\n",
      "Epoch 35/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8647\n",
      "Epoch 36/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3267 - accuracy: 0.8643\n",
      "Epoch 37/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3259 - accuracy: 0.8643\n",
      "Epoch 38/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3276 - accuracy: 0.8622\n",
      "Epoch 39/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8637\n",
      "Epoch 40/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3257 - accuracy: 0.8651\n",
      "Epoch 41/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3256 - accuracy: 0.8633\n",
      "Epoch 42/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8641\n",
      "Epoch 43/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8632\n",
      "Epoch 44/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8651\n",
      "Epoch 45/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8647\n",
      "Epoch 46/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3249 - accuracy: 0.8658\n",
      "Epoch 47/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8645\n",
      "Epoch 48/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3242 - accuracy: 0.8663\n",
      "Epoch 49/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8647\n",
      "Epoch 50/50\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=<function create_model at 0x4f94c3af0>, epochs=30),\n",
       "             param_grid={'batch_size': [32, 64, 96], 'epochs': [50],\n",
       "                         'optimizer': [<keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50>,\n",
       "                                       <keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490>]},\n",
       "             refit='f1_macro',\n",
       "             scoring=['accuracy', 'f1_macro', 'precision_macro',\n",
       "                      'recall_macro'])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 64, 'epochs': 50, 'optimizer': <keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490>}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:18:08.425784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 12ms/step - loss: 0.4709 - accuracy: 0.7861\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3751 - accuracy: 0.8440\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3542 - accuracy: 0.8495\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3437 - accuracy: 0.8556\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3381 - accuracy: 0.8574\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8595\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8611\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3267 - accuracy: 0.8634\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3243 - accuracy: 0.8651\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8655\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8659\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8673\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3155 - accuracy: 0.8673\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8687\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3106 - accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3096 - accuracy: 0.8714\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3076 - accuracy: 0.8699\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8712\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8739\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3034 - accuracy: 0.8735\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8730\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8750\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3009 - accuracy: 0.8751\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3013 - accuracy: 0.8740\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2966 - accuracy: 0.8762\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8750\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8757\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8768\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8768\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2923 - accuracy: 0.8785\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2905 - accuracy: 0.8800\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8818\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2890 - accuracy: 0.8820\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2880 - accuracy: 0.8804\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8813\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8830\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2850 - accuracy: 0.8839\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8827\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8839\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2812 - accuracy: 0.8845\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2810 - accuracy: 0.8841\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8847\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8861\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2758 - accuracy: 0.8880\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2763 - accuracy: 0.8868\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8891\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8887\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8911\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:18:43.288388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:18:43.570372: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4784 - accuracy: 0.7791\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3854 - accuracy: 0.8347\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3628 - accuracy: 0.8453\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8541\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8543\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3383 - accuracy: 0.8608\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8602\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8632\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8637\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8645\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8683\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8688\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8699\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3151 - accuracy: 0.8662\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8714\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3102 - accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3101 - accuracy: 0.8680\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3068 - accuracy: 0.8715\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3052 - accuracy: 0.8702\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3036 - accuracy: 0.8735\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3041 - accuracy: 0.8740\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8740\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8742\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2968 - accuracy: 0.8750\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2942 - accuracy: 0.8795\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8760\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2915 - accuracy: 0.8788\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2911 - accuracy: 0.8771\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8787\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8787\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2885 - accuracy: 0.8769\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8799\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2815 - accuracy: 0.8806\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8799\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8816\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8826\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8829\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8854\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8837\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2734 - accuracy: 0.8826\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2720 - accuracy: 0.8858\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8857\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8866\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.8862\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8867\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8878\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8879\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8852\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8885\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8899\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:19:17.923543: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:19:18.214063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 11ms/step - loss: 0.4750 - accuracy: 0.7877\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3872 - accuracy: 0.8341\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3640 - accuracy: 0.8464\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3530 - accuracy: 0.8503\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3462 - accuracy: 0.8546\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3405 - accuracy: 0.8568\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3365 - accuracy: 0.8580\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8609\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8609\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8633\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8653\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8649\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8671\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3177 - accuracy: 0.8658\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8681\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3146 - accuracy: 0.8693\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8701\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3085 - accuracy: 0.8726\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8744\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8748\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8751\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8740\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8762\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2997 - accuracy: 0.8776\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2987 - accuracy: 0.8763\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8789\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8768\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2931 - accuracy: 0.8799\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2900 - accuracy: 0.8793\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2896 - accuracy: 0.8793\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2886 - accuracy: 0.8795\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2881 - accuracy: 0.8794\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8835\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8819\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2841 - accuracy: 0.8833\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8808\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2814 - accuracy: 0.8810\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8814\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2780 - accuracy: 0.8832\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8848\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8856\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8848\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2736 - accuracy: 0.8854\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8880\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8897\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8878\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8875\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8879\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8886\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8923\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:19:52.460622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:19:52.729206: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4850 - accuracy: 0.7750\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3803 - accuracy: 0.8388\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3575 - accuracy: 0.8497\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3429 - accuracy: 0.8551\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8587\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8609\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8665\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8662\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8675\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3153 - accuracy: 0.8671\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3131 - accuracy: 0.8705\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3115 - accuracy: 0.8695\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3099 - accuracy: 0.8724\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8700\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8744\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3020 - accuracy: 0.8745\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3008 - accuracy: 0.8745\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2975 - accuracy: 0.8767\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2982 - accuracy: 0.8749\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2943 - accuracy: 0.8783\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8774\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8795\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2886 - accuracy: 0.8796\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2883 - accuracy: 0.8796\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8805\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8820\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8826\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8825\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8850\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8854\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8858\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2739 - accuracy: 0.8870\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.8864\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8868\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8893\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8876\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8911\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8903\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8916\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8914\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8932\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8922\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8955\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8932\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8951\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.8961\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8961\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8943\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8988\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8984\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:20:27.099271: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-24 23:20:27.374418: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 2s 10ms/step - loss: 0.4757 - accuracy: 0.7844\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3759 - accuracy: 0.8409\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3578 - accuracy: 0.8499\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3460 - accuracy: 0.8555\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3403 - accuracy: 0.8603\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3363 - accuracy: 0.8619\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8614\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8628\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8651\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3219 - accuracy: 0.8689\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3204 - accuracy: 0.8652\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8679\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8694\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3150 - accuracy: 0.8689\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3122 - accuracy: 0.8704\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3111 - accuracy: 0.8712\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8720\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3078 - accuracy: 0.8715\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3064 - accuracy: 0.8733\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3028 - accuracy: 0.8761\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3038 - accuracy: 0.8738\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8727\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3004 - accuracy: 0.8743\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2990 - accuracy: 0.8724\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8752\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2971 - accuracy: 0.8754\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8781\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8779\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2943 - accuracy: 0.8767\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2904 - accuracy: 0.8763\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8776\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2891 - accuracy: 0.8835\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8814\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2876 - accuracy: 0.8804\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.8818\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8802\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2854 - accuracy: 0.8805\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8838\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2825 - accuracy: 0.8817\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8827\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2786 - accuracy: 0.8826\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8835\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2779 - accuracy: 0.8839\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8843\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8862\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.8856\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.8869\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8866\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8882\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Mean Accuracy: 0.8451858913250714\n",
      "Mean F1 Macro: 0.8309404592135102\n",
      "Mean Precision Macro: 0.8417203807511993\n",
      "Mean Recall Macro: 0.8249394258137215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:21:02.000896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cross_val_results = cross_validate(keras_classifier, X_train, y_train, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'])\n",
    "\n",
    "results_df.loc['Neural Network II all features after HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=&lt;function create_model at 0x4f94c3af0&gt;, epochs=30),\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 96], &#x27;epochs&#x27;: [50],\n",
       "                         &#x27;optimizer&#x27;: [&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50&gt;,\n",
       "                                       &lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490&gt;]},\n",
       "             refit=&#x27;f1_macro&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_macro&#x27;, &#x27;precision_macro&#x27;,\n",
       "                      &#x27;recall_macro&#x27;])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x4f94c3af0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=30\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(batch_size=32, build_fn=<function create_model at 0x4f94c3af0>, epochs=30),\n",
       "             param_grid={'batch_size': [32, 64, 96], 'epochs': [50],\n",
       "                         'optimizer': [<keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9ee50>,\n",
       "                                       <keras.optimizers.optimizer_v2.adam.Adam object at 0x52cf9e490>]},\n",
       "             refit='f1_macro',\n",
       "             scoring=['accuracy', 'f1_macro', 'precision_macro',\n",
       "                      'recall_macro'])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model(optimizer=Adam(learning_rate=0.001)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Define the hyperparameters and their respective values to tune\n",
    "param_grid = {\n",
    "    'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=5, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro'], refit='f1_macro')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train_sf, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_sf.shape[1]\n",
    "\n",
    "# Define a function to create your neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with your model function\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=64, optimizer=Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc['Neural Network II subset features after HT',:] = [cross_val_results['test_accuracy'].mean(), cross_val_results['test_f1_macro'].mean(),cross_val_results['test_precision_macro'].mean() , cross_val_results['test_recall_macro'].mean()]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
